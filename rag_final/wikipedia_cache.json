[
  {
    "title": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "content": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\n\n== Goals ==\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\n\n=== Reasoning and problem-solving ===\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\n\n\n=== Knowledge representation ===\n\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\n\n\n=== Planning and decision-making ===\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\n\n\n=== Learning ===\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\n\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\n\n\n=== Natural language processing ===\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\n\n\n=== Perception ===\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\nThe field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception.\n\n\n=== Social intelligence ===\n\nAffective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\nHowever, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.\n\n\n=== General intelligence ===\nA machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\n\n\n== Techniques ==\nAI research uses a wide variety of techniques to accomplish the goals above.\n\n\n=== Search and optimization ===\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\n\n\n==== State space search ====\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.\n\n\n==== Local search ====\n Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm.\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\n\n=== Logic ===\nFormal logic is used for reasoning and knowledge representation.\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.\n\n\n=== Probabilistic methods for uncertain reasoning ===\n\nMany problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\n\n\n=== Classifiers and statistical learning methods ===\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\nNeural networks are also used as classifiers.\n\n\n=== Artificial neural networks ===\n\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\nIn feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.\n\n\n=== Deep learning ===\n\nDeep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.\n\n\n=== GPT ===\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.\nCurrent models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.\n\n\n=== Hardware and software ===\n\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.\nThe transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang.\n\n\n== Applications ==\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO).\n\n\n=== Health and medicine ===\n\nIt has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. \nAlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\n\n\n=== Games ===\n\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.\n\n\n=== Mathematics ===\nLarge language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.\nAlternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius.\nWhen natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.   \nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.\nTopological deep learning integrates various topological approaches.\n\n\n=== Finance ===\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.\nAccording to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"\n\n\n=== Military ===\n\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine.\n\n\n=== Generative AI ===\n\n\n=== Agents ===\n\nAI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.\n\n\n=== Web search ===\nMicrosoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries and step-by-step reasoning based of information from web publishers, ranked in Bing Search. \nFor safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content.\nGoogle officially pushed its AI Search at its Google I/O event on 20 May 2025. It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.\n\n\n=== Sexuality ===\nApplications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika).  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.\nAI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.\n\n\n=== Other industry-specific tasks ===\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\nAI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.\nIn agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\nDuring the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.\n\n\n== Ethics ==\n\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\n\n\n=== Risks and harm ===\n\n\n==== Privacy and copyright ====\n\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\nSensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\nAI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners can indicate that they do not want their content scraped via a \"robots.txt\" file. However, some companies will scrape content regardless because the robots.txt file has no real authority. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.\n\n\n==== Dominance by tech giants ====\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\n\n\n==== Power needs and environmental impacts ====\n\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.\nProdigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.\nAlthough most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.\nOn 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.\nIn 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.\n\n\n==== Misinformation ====\n\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.\nIn the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks. The ability to influence electorates has been proved in at least one study. This same study shows more inaccurate statements from the models when they advocate for candidates of the political right. \nAI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.\n\n\n==== Algorithmic bias and fairness ====\n\nMachine learning applications can be biased if they learn from biased data. The developers may not be aware that the bias exists. Discriminatory behavior by some LLMs can be observed in their output. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.\nOn 28 June 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\n\n\n==== Lack of transparency ====\n\nMany AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\nPeople who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.\n\n\n==== Bad actors and weaponized AI ====\n\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.\nThere are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.\n\n\n==== Technological unemployment ====\n\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. In July 2025, Ford CEO Jim Farley predicted that \"artificial intelligence is going to replace literally half of all white-collar workers in the U.S.\"\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.\n\n\n==== Existential risk ====\n\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\nFirst, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of an automated paperclip factory that destroys the world to get more iron for paperclips). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\". \nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\" \nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.\nIn 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".\nSome other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\n\n\n=== Ethical machines and alignment ===\n\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nThe field of machine ethics is also called computational morality,\nand was founded at an AAAI symposium in 2005.\nOther approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines.\n\n\n=== Open source ===\n\nActive organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.\n\n\n=== Frameworks ===\nArtificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:\n\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\nProtect social values, justice, and the public interest\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.\n\n\n=== Regulation ===\n\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.\n\n\n== History ==\n\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible. \nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.\n\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\nIn the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.\n\n\n== Philosophy ==\n\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.\n\n\n=== Defining artificial intelligence ===\n\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"\n\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine – and no other philosophical discussion is required, or may not even be possible.\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\nAs a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself including discussing the many AI narratives and myths to be found within societal, political and academic discourses. Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".\nThere has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.\n\n\n=== Evaluating approaches to AI ===\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n\n\n==== Symbolic AI and its limits ====\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\n\n==== Neat vs. scruffy ====\n\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.\n\n\n==== Soft vs. hard computing ====\n\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\n\n==== Narrow vs. general AI ====\n\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n\n\n=== Machine consciousness, sentience, and mind ===\n\nThere is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\n\n==== Consciousness ====\n\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\n\n\n==== Computationalism and functionalism ====\n\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.\n\n\n==== AI welfare and rights ====\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.\n\n\n== Future ==\n\n\n=== Superintelligence and the singularity ===\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.\n\n\n=== Transhumanism ===\n\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.\n\n\n== In fiction ==\n\nThought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\n\n== See also ==\nArtificial consciousness – Field in cognitive science\nArtificial intelligence and elections – Impact of AI on political elections\nArtificial intelligence content detection – Software to detect AI-generated content\nArtificial intelligence in Wikimedia projects – Use of artificial intelligence to develop Wikipedia and other Wikimedia projects\nAssociation for the Advancement of Artificial Intelligence (AAAI)\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\nBusiness process automation – Automation of business processes\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\nComputational intelligence – Ability of a computer to learn a specific task from data or experimental observation\nDARWIN EU – A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real-world evidence (RWE) to support the evaluation and supervision of medicines across the EU\nDigital immortality – Hypothetical concept of storing a personality in digital form\nEmergent algorithm – Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets\nGlossary of artificial intelligence – List of concepts in artificial intelligence\nIntelligence amplification – Use of information technology to augment human intelligence\nIntelligent agent – Software agent which acts autonomously\nIntelligent automation – Software process that combines robotic process automation and artificial intelligence\nList of artificial intelligence  books\nList of artificial intelligence journals\nList of artificial intelligence projects\nMind uploading – Hypothetical process of digitally emulating a brain\nOrganoid intelligence – Use of brain cells and brain organoids for intelligent computing\nPseudorandomness – Appearing random but actually being generated by a deterministic, causal process\nRobotic process automation – Form of business process automation technology\nThe Last Day – 1967 Welsh science fiction novel\nWetware computer – Computer composed of organic material\n\n\n== Explanatory notes ==\n\n\n== References ==\n\n\n=== AI textbooks ===\nThe two most widely used textbooks in 2023 (see the Open Syllabus):\n\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474.\nRich, Elaine; Knight, Kevin; Nair, Shivashankar (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.\nThe four most widely used AI textbooks in 2008:\n\nOther textbooks:\n\nErtel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7.\nCiaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3.\n\n\n=== History of AI ===\n\n\n=== Other sources ===\n\n\n== Further reading ==\n\n\n== External links ==\n\nHauser, Larry. \"Artificial Intelligence\". In Fieser, James; Dowden, Bradley (eds.). Internet Encyclopedia of Philosophy. ISSN 2161-0002. OCLC 37741658.",
    "categories": [
      "All accuracy disputes",
      "Articles with Internet Encyclopedia of Philosophy links",
      "Articles with disputed statements from July 2024",
      "Articles with excerpts",
      "Articles with short description",
      "Artificial intelligence",
      "CS1: long volume value",
      "CS1 German-language sources (de)",
      "CS1 Japanese-language sources (ja)",
      "CS1 Russian-language sources (ru)",
      "Computational fields of study",
      "Computational neuroscience",
      "Cybernetics",
      "Data science",
      "Formal sciences",
      "Intelligence by type",
      "Pages displaying short descriptions of redirect targets via Module:Annotated link",
      "Pages using Sister project links with hidden wikidata",
      "Short description is different from Wikidata",
      "Use dmy dates from October 2025",
      "Webarchive template wayback links",
      "Wikipedia indefinitely semi-protected pages"
    ],
    "year_mentioned": 2020
  },
  {
    "title": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "content": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\nStatistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.\nFrom a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework.\n\n\n== History ==\n\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\nThe earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think?\", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\".\nModern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms.\n\nCurrent Supervised Learning Algorithms have objectives of classification and regression.\nCurrent Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.\nCurrent Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods.\nIn 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis. By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques.\n\n\n== Relationships to other fields ==\n\n\n=== Artificial intelligence ===\n\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines, including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.\nMachine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.\n\n\n=== Data compression ===\n\n\n=== Data mining ===\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\nMachine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).\n\n\n=== Generalization ===\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n\n\n=== Statistics ===\nMachine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns.\nConventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.\nLeo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.\n\n\n=== Statistical physics ===\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.\n\n\n== Theory ==\n\nA core objective of a learner is to generalise from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning  model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalisation error.\nFor the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.\nIn addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\n\n== Approaches ==\n\nMachine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise.\nAlthough each algorithm has advantages and limitations, no single algorithm works for all problems.\n\n\n=== Supervised learning ===\n\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.\nTypes of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.\nSimilarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n\n\n=== Unsupervised learning ===\n\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation.\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\nA special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.\n\n\n=== Semi-supervised learning ===\n\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.\nIn weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.\n\n\n=== Reinforcement learning ===\n\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n\n\n=== Dimensionality reduction ===\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the areas of manifold learning and manifold regularisation.\n\n\n=== Other types ===\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning.\n\n\n==== Self-learning ====\nSelf-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n\nin situation s act a\nreceive a consequence situation s'\ncompute emotion of being in the consequence situation v(s')\nupdate crossbar memory  w'(a,s) = w(a,s) + v(s')\nIt is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour in an environment that contains both desirable and undesirable situations.\n\n\n==== Feature learning ====\n\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering.\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data.\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n\n\n==== Sparse dictionary learning ====\n\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image denoising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\n\n\n==== Anomaly detection ====\n\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations that raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.\nThree broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance being generated by the model.\n\n\n==== Robot learning ====\nRobot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).\n\n\n==== Association rules ====\n\nAssociation rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \n  \n    \n      \n        {\n        \n          o\n          n\n          i\n          o\n          n\n          s\n          ,\n          p\n          o\n          t\n          a\n          t\n          o\n          e\n          s\n        \n        }\n        ⇒\n        {\n        \n          b\n          u\n          r\n          g\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n  \n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner to make predictions.\nInductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n\n\n== Models ==\nA machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.\nVarious types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\n\n\n=== Artificial neural networks ===\n\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\nDeep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\n\n\n=== Decision trees ===\n\nDecision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n\n\n=== Random forest regression ===\nRandom forest regression (RFR) falls under the umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling; for instance, each decision tree is trained on random data from the training set. This random selection of RFR for training enables the model to reduce biased predictions and achieve a higher degree of accuracy. RFR generates independent decision trees, and it can work on single-output data as well as multiple regressor tasks. This makes RFR compatible to be use in various applications.\n\n\n=== Support-vector machines ===\n\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\n\n=== Regression analysis ===\n\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\nMultivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional.\n\n\n=== Bayesian networks ===\n\nA Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\n\n=== Gaussian processes ===\n\nA Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\nGiven a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as a function of its input data can be directly computed by looking at the observed points and the covariances between those points and the new, unobserved point.\nGaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation.\n\n\n=== Genetic algorithms ===\n\nA genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\n\n\n=== Belief functions ===\n\nThe theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms is dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n\n\n=== Rule-based models ===\n\nRule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems, association rule learning, artificial immune systems, and other similar models. These methods extract patterns from data and evolve rules over time.\n\n\n=== Training models ===\nTypically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and, notably, becoming integrated within machine learning engineering teams.\n\n\n==== Federated learning ====\n\nFederated learning is an adapted form of distributed artificial intelligence to train machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.\n\n\n== Applications ==\nThere are many applications for machine learning, including:\n\nIn 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behaviour of travellers. Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.\nRecent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.\nMachine Learning is becoming a useful tool to investigate and predict evacuation decision-making in large-scale and small-scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires.\n\n\n== Limitations ==\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.\nThe \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted from the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.\nIn 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.\nMachine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research itself.\n\n\n=== Explainability ===\n\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n\n\n=== Overfitting ===\n\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.\n\n\n=== Other limitations and vulnerabilities ===\nLearners can also be disappointed by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.\nAdversarial vulnerabilities can also result in nonlinear systems or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.\nResearchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.\n\n\n== Model assessments ==\nClassification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data into a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.\nIn addition to overall accuracy, investigators frequently report sensitivity and specificity, meaning true positive rate (TPR) and true negative rate (TNR), respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC), along with the accompanying Area Under the ROC Curve (AUC), offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.\n\n\n== Ethics ==\n\n\n=== Bias ===\n\nDifferent machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.\nSystems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to either be women or have non-European-sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.\nWhile responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame the lack of participation and representation of minority populations in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association in 2021, \"female faculty make up just 16.1%\" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.\nLanguage models learned from data have been shown to contain human-like biases. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.\nIn an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\". In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas. Similar issues with recognising non-white people have been found in many other systems.\nBecause of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"\n\n\n=== Financial incentives ===\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.\n\n\n== Hardware ==\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.\n\n\n=== Tensor Processing Units (TPUs) ===\nTensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.\n\n\n=== Neuromorphic computing ===\nNeuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.\n\n\n==== Physical neural networks ====\nA physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.\n\n\n=== Embedded machine learning ===\nEmbedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quantisation, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing.\n\n\n== Software ==\nSoftware suites containing a variety of machine learning algorithms include the following:\n\n\n=== Free and open-source software ===\n\n\n=== Proprietary software with free and open-source editions ===\nKNIME\nRapidMiner\n\n\n=== Proprietary software ===\n\n\n== Journals ==\nJournal of Machine Learning Research\nMachine Learning\nNature Machine Intelligence\nNeural Computation\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n\n\n== Conferences ==\nAAAI Conference on Artificial Intelligence\nAssociation for Computational Linguistics (ACL)\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\nInternational Conference on Machine Learning (ICML)\nInternational Conference on Learning Representations (ICLR)\nInternational Conference on Intelligent Robots and Systems (IROS)\nConference on Knowledge Discovery and Data Mining (KDD)\nConference on Neural Information Processing Systems (NeurIPS)\n\n\n== See also ==\nAutomated machine learning – Process of automating the application of machine learning\nBig data – Extremely large or complex datasets\nDeep learning — branch of ML concerned with artificial neural networks\nDifferentiable programming – Programming paradigm\nList of datasets for machine-learning research\nList of machine learning algorithms and List of algorithms for machine learning and statistical classification\nM-theory (learning framework) – Framework in machine learning\nMachine unlearning – Field of study in artificial intelligence\nOutline of machine learning\nSolomonoff's theory of inductive inference – Mathematical theory\n\n\n== References ==\n\n\n== Sources ==\nDomingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0-465-06570-7.\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.\n\n\n== Further reading ==\n\n\n== External links ==\nInternational Machine Learning Society\nmloss is an academic database of open-source machine learning software.",
    "categories": [
      "All Wikipedia articles written in British English",
      "All articles with unsourced statements",
      "Articles with excerpts",
      "Articles with short description",
      "Articles with unsourced statements from October 2025",
      "Cybernetics",
      "Data science",
      "Definition",
      "Learning",
      "Machine learning",
      "Short description is different from Wikidata",
      "Use British English from October 2025",
      "Use dmy dates from April 2025",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 2014
  },
  {
    "title": "Deep learning",
    "url": "https://en.wikipedia.org/wiki/Deep_learning",
    "content": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.\nSome common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\n\n\n== Overview ==\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\nFundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\nImportantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.\nThe word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\nDeep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.\nThe term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated.\n\n\n== Interpretations ==\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.\nThe classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.\nThe universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\nThe probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.\n\n\n== History ==\n\n\n=== Before 1980 ===\nThere are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on \"Intelligent Machinery\"  that was not published in his lifetime, containing \"ideas related to artificial evolution and learning RNNs\".\nFrank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion.\nThe first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\".\nThe first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\nIn 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning.\nDeep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.\nBackpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.\n\n\n=== 1980s-2000s ===\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.  In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. \nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.\nRecurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology.\nIn the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This \"neural history compressor\" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. The \"P\" in ChatGPT refers to such pre-training.\nSepp Hochreiter's diploma thesis (1991) implemented the neural history compressor, and identified and analyzed the vanishing gradient problem.  Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999, which became the standard RNN architecture.\nIn 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs).\nDuring 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 ). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.\nBoth shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.\nMost speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark. It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.\nThe principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.\n\n\n=== 2000s ===\nNeural networks entered a lull, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.\nIn 2003, LSTM became competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTMs. In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.\nIn 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation. They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.\nThe impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.\nThe 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.\n\n\n=== Deep learning revolution ===\n\nThe deep learning revolution started around CNN- and GPU-based computer vision.\nAlthough CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.\nA key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.\nIn 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.\nIn 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.\nIn October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.\nThe success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.\nIn 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net.\nAround the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19.\nGenerative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on  Jürgen Schmidhuber's principle of artificial curiosity)\nbecame state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.  Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).\nIn 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.\nDeep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by LSTM. but are more successful in computer vision.\nYoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".\n\n\n== Neural networks ==\n\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\nAn ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\nTypically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\nThe original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\nNeural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\nAs of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\").\n\n\n=== Deep neural networks ===\nA deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.\nFor example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks.\nDNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.\nDeep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.\nDNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\nRecurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.\nConvolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).\n\n\n==== Challenges ====\nAs with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\nDNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (\n  \n    \n      \n        \n          ℓ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\ell _{2}}\n  \n-regularization) or sparsity (\n  \n    \n      \n        \n          ℓ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\ell _{1}}\n  \n-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.\nDNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.\nAlternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.\n\n\n== Hardware ==\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.\nSpecial electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).\nAtomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).\nIn 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.\n\n\n== Applications ==\n\n\n=== Automatic speech recognition ===\n\nLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.\nThe initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\n\nThe debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:\n\nScale-up/out and accelerated DNN training and decoding\nSequence discriminative training\nFeature processing by deep models with solid understanding of the underlying mechanisms\nAdaptation of DNNs and related deep models\nMulti-task and transfer learning by DNNs and related deep models\nCNNs and how to design them to best exploit domain knowledge of speech\nRNN and its rich LSTM variants\nOther types of deep models including tensor-based models and integrated deep generative/discriminative models.\nMore recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications. All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.\n\n\n=== Image recognition ===\n\nA common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.\nDeep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.\nDeep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\n\n\n=== Visual art processing ===\n\nClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of\n\nidentifying the style period of a given painting\nNeural Style Transfer –  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video\ngenerating striking imagery based on random visual input fields.\n\n\n=== Natural language processing ===\n\nNeural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.\nOther key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others.\nRecent developments generalize word embedding to sentence embedding.\nGoogle Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\". It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs.\n\n\n=== Drug discovery and toxicology ===\n\nA large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.\nAtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.\nIn 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.\n\n\n=== Recommendation systems ===\n\nRecommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\n\n\n=== Bioinformatics ===\n\nAn autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.\nIn medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.\nDeep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.\n\n\n=== Deep Neural Network Estimations ===\nDeep neural networks can be used to estimate the entropy of a stochastic process through an arrangement called a Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in cases of large alphabet sizes.\n\n\n=== Medical image analysis ===\nDeep learning has been shown to produce competitive results in medical applications such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.\n\n\n=== Mobile advertising ===\nFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\n\n\n=== Image restoration ===\nDeep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\n\n\n=== Financial fraud detection ===\nDeep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.\n\n\n=== Materials science ===\nIn November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.\n\n\n=== Military ===\nThe United States Department of Defense applied deep learning to train robots in new tasks through observation.\n\n\n=== Partial differential equations ===\nPhysics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on. It is evident that geometric and physical constraints have a synergistic effect on neural PDE surrogates, thereby enhancing their efficacy in predicting stable and super long rollouts.\n\n\n=== Deep backward stochastic differential equation method ===\nDeep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.\nIn addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems.\n\n\n=== Image reconstruction ===\nImage reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging  and ultrasound imaging.\n\n\n=== Weather prediction ===\nTraditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to  predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.\n\n\n=== Epigenetic clock ===\n\nAn epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.\n\n\n== Relation to human cognitive and brain development ==\nDeep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".\nA variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.\nAlthough a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.\n\n\n== Commercial activity ==\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.\nGoogle's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.\nIn 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.\nAs of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".\n\n\n== Criticism and comment ==\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\n\n\n=== Theory ===\n\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.\nIn further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website.\nWith the support of Innovation Diffusion Theory (IDT), a study analyzed the diffusion of Deep Learning in BRICS and OECD countries using data from Google Trends.\n\n\n=== Errors ===\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).\n\n\n=== Cyber threat ===\nAs deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\".\nIn 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.\nAnother group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.\nANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.\nIn 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".\nIn \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.\n\n\n=== Data collection ethics ===\nThe deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both. It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.\n\n\n== See also ==\nApplications of artificial intelligence\nComparison of deep learning software\nCompressed sensing\nDifferentiable programming\nEcho state network\nList of artificial intelligence projects\nLiquid state machine\nList of datasets for machine-learning research\nReservoir computing\nScale space and deep learning\nSparse coding\nStochastic parrot\nTopological deep learning\n\n\n== References ==\n\n\n== Further reading ==",
    "categories": [
      "All articles with unsourced statements",
      "Articles prone to spam from June 2015",
      "Articles with short description",
      "Articles with unsourced statements from August 2024",
      "Articles with unsourced statements from July 2016",
      "Articles with unsourced statements from November 2020",
      "CS1: long volume value",
      "CS1 Finnish-language sources (fi)",
      "CS1 errors: ISBN date",
      "CS1 maint: multiple names: authors list",
      "CS1 maint: postscript",
      "Deep learning",
      "Pages using multiple image with auto scaled images",
      "Short description matches Wikidata",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 2006
  },
  {
    "title": "Neural network",
    "url": "https://en.wikipedia.org/wiki/Neural_network",
    "content": "A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural networks.\n\nIn neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses.\nIn machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems.\n\n\n== In biology ==\n\nIn the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses.\nEach neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead.\nPopulations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems.\nSignals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion.\n\n\n== In machine learning ==\n\nIn machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software.\nNeurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer).\nThe \"signal\" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.\nThe term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers.\nNeural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI.\n\n\n== History ==\n\nThe theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it. In 1956, Svaetichin discovered the functioning of second order retinal cells (Horizontal Cells), which were fundamental for the understanding of neural networks.\nArtificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957,\nartificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts.\n\n\n== See also ==\nEmergence\nBiological cybernetics\nBiologically-inspired computing\n\n\n== References ==",
    "categories": [
      "All Wikipedia articles written in American English",
      "Articles with short description",
      "Broad-concept articles",
      "Neural networks",
      "Short description matches Wikidata",
      "Use American English from April 2025",
      "Use mdy dates from April 2025"
    ],
    "year_mentioned": 1946
  },
  {
    "title": "Symbolic artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence",
    "content": "In artificial intelligence, symbolic artificial intelligence (also known as classical artificial intelligence  or logic-based artificial intelligence)\nis the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic, and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to important ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\nSymbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s. Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field. An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the first AI Winter as funding dried up. A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.\nNeural networks, a subsymbolic approach, had been pursued from early days and reemerged strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012: \"Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks.\" Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation, though symbolic approaches continue to be useful in a few domains such as computer algebra systems and proof assistants.\nHowever, given the inherent complexity of intelligence itself, it remains an open question whether symbolic AI will be completely supplanted by connectionist AI, or whether symbolic AI may yet experience a resurgence. More recently, work by Zhang et al. has further argued that, at least at the theoretical level, there is no clear superiority among different technological paradigms.\n\n\n== History ==\nA short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity.\n\n\n=== The first AI summer: irrational exuberance, 1948–1966 ===\nSuccess at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history.\n\n\n==== Approaches inspired by human or animal cognition or behavior ====\nCybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.\nAn important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell's Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver, GPS (General Problem Solver). GPS solved problems represented with formal operators via state-space search using means-ends analysis.\nDuring the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was concentrated in four institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.\nHerbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.\n\n\n==== Heuristic search ====\nIn addition to the highly specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: \"How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics: fast algorithms that may fail on some inputs or output suboptimal solutions.\" Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: \"The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.\n\n\n==== Early work on knowledge representation and reasoning ====\nEarly work covered both applications of formal reasoning emphasizing first-order logic, along with attempts to handle common-sense reasoning in a less formal manner.\n\n\n===== Modeling formal reasoning with logic: the \"neats\" =====\n\nUnlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic, regardless of whether people used the same algorithms.\nHis laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.\nLogic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.\n\n\n===== Modeling implicit common-sense knowledge with frames and scripts: the \"scruffies\" =====\n\nResearchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford).\nCommonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.\n\n\n=== The first AI winter: crushed dreams, 1967–1977 ===\nThe first AI winter was a shock:\n\nDuring the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research to use AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs.\n...\n\nOutside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.\n\n\n=== The second AI summer: knowledge is power, 1978–1987 ===\n\n\n==== Knowledge-based systems ====\nAs limitations with weak, domain-independent methods became more and more apparent, researchers from all three traditions began to build knowledge into AI applications. The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications.\nEdward Feigenbaum said:\n\n\"In the knowledge lies the power.\"\nto describe that high performance in a specific domain requires both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle: \n\n(1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates.(2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.\n\n\n==== Success with expert systems ====\n\nThis \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first commercially successful form of AI software.\nKey expert systems were:\n\nDENDRAL, which found the structure of organic molecules from their chemical formula and mass spectrometer readings.\nMYCIN, which diagnosed bacteremia – and suggested further lab tests, when necessary – by interpreting lab results, patient history, and doctor observations. \"With about 450 rules, MYCIN was able to perform as well as some experts, and considerably better than junior doctors.\"\nINTERNIST and CADUCEUS which tackled internal medicine diagnosis. Internist attempted to capture the expertise of the chairman of internal medicine at the University of Pittsburgh School of Medicine while CADUCEUS could eventually diagnose up to 1000 different diseases.\nGUIDON, which showed how a knowledge base built for expert problem solving could be repurposed for teaching.\nXCON, to configure VAX computers, a then laborious process that could take up to 90 days. XCON reduced the time to about 90 minutes.\nDENDRAL is considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum:\n\nOne of the people at Stanford interested in computer-based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction \"sandbox\", he said, \"I have just the one for you.\" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at the spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm that was good at generating the chemical problem space.\nWe did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add to their knowledge, inventing knowledge of engineering as we went along. These experiments amounted to titrating DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.\n\nThe generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, \"Ah ha!,\" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.\nThe other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to use MYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling. XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, to capture corporate expertise, preserve it, and automate it:\n\nBy 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.\nChess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM's Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov.\n\n\n===== Architecture of knowledge-based and expert systems =====\nA key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.\nThe simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion.\nExpert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such as Soar can also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies.\nBlackboard systems are a second kind of knowledge-based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can contribute. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture was originally inspired by studies of how humans plan to perform multiple tasks in a trip. An innovation of BB1 was to apply the same blackboard model to solving its control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 has been applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring.\n\n\n=== The second AI winter, 1988–1993 ===\nAt the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations.\nUnfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed:\n\nMany reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver; the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair.\n\n\n=== Adding in more rigorous foundations, 1993–2011 ===\n\n\n==== Uncertain reasoning ====\nBoth statistical approaches and extensions to logic were tried.\nOne statistical approach, hidden Markov models, had already been popularized in the 1980s for speech recognition work. Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. and Bayesian approaches were applied successfully in expert systems. Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic.\nOther, non-probabilistic extensions to first-order logic to support were also tried. For example, non-monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions. Lotfi Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how \"heavy\" or \"tall\" a man is, there is frequently no clear \"yes\" or \"no\" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas.\n\n\n==== Machine learning ====\nSymbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta-DENDRAL as\n\n...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems.\nWe did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.\nIn contrast to the knowledge-intensive approach of Meta-DENDRAL, Ross Quinlan invented a domain-independent approach to statistical classification, decision tree learning, starting first with ID3 and then later extending its capabilities to C4.5. The decision trees created are glass box, interpretable classifiers, with human-interpretable classification rules.\nAdvances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as a search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far. More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.\nSymbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT-R cognitive architecture. For example, a student might learn to apply \"Supplementary angles are two angles whose measures sum 180 degrees\" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach \"knowledge compilation\". ACT-R has been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.\nInductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g., Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples. John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct.\nAs an alternative to logic, Roger Schank introduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory, focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem. Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.\nSymbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include:\n\nLearning from instruction or advice—i.e., taking human instruction, posed as advice, and determining how to operationalize it in specific situations. For example, in a game of Hearts, learning exactly how to play a hand to \"avoid taking points.\"\nLearning from exemplars—improving performance by accepting subject-matter expert (SME) feedback during training. When problem-solving fails, querying the expert to either learn a new exemplar for problem-solving or to learn a new explanation as to exactly why one exemplar is more relevant than another. For example, the program Protos learned to diagnose tinnitus cases by interacting with an audiologist.\nLearning by analogy—constructing problem solutions based on similar problems seen in the past, and then modifying their solutions to fit a new situation or domain.\nApprentice learning systems—learning novel solutions to problems by observing human problem-solving. Domain knowledge explains why novel solutions are correct and how the solution can be generalized. LEAP learned how to design VLSI circuits by observing human designers.\nLearning by discovery—i.e., creating tasks to carry out experiments and then learning from the results. Doug Lenat's Eurisko, for example, learned heuristics to beat human players at the Traveller role-playing game for two years in a row.\nLearning macro-operators—i.e., searching for useful macro-operators to be learned from sequences of basic problem-solving actions. Good macro-operators simplify problem-solving by allowing problems to be solved at a more abstract level.\n\n\n=== Deep learning and neuro-symbolic AI 2011–now ===\nWith the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary \"...with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making – reflected in his book Thinking, Fast and Slow – and the so-called \"AI systems 1 and 2\", which would in principle be modelled by deep learning and symbolic reasoning, respectively.\" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.\n\n\n==== Neuro-symbolic AI: integrating neural and symbolic approaches ====\n\nNeuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant and many others, the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient (machine) learning models. Gary Marcus, similarly, argues that: \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\", and in particular:\n\"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"\nHenry Kautz, Francesca Rossi, and Bart Selman have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman's book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed.\nGarcez and Lamb describe research in this area as being ongoing for at least the past twenty years, dating from their 2002 book on neurosymbolic learning systems. A series of workshops on neuro-symbolic reasoning has been held every year since 2005.\nIn their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that:\n\nThe integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.\nApproaches for integration are varied. Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows:\n\nSymbolic Neural symbolic—is the current approach of many neural models in natural language processing, where words or subword tokens are both the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3.\nSymbolic[Neural]—is exemplified by AlphaGo, where symbolic techniques are used to call neural techniques. In this case the symbolic approach is Monte Carlo tree search and the neural techniques learn how to evaluate game positions.\nNeural|Symbolic—uses a neural architecture to interpret perceptual data as symbols and relationships that are then reasoned about symbolically.\nNeural:Symbolic → Neural—relies on symbolic reasoning to generate or label training data that is subsequently learned by a deep learning model, e.g., to train a neural model for symbolic computation by using a Macsyma-like symbolic mathematics system to create or label examples.\nNeural_{Symbolic}—uses a neural net that is generated from symbolic rules. An example is the Neural Theorem Prover, which constructs a neural network from an AND–OR proof tree generated from knowledge base rules and terms. Logic Tensor Networks also fall into this category.\nNeural[Symbolic]—allows a neural model to directly call a symbolic reasoning engine, e.g., to perform an action or evaluate a state.\nMany key research questions remain, such as:\n\nWhat is the best way to integrate neural and symbolic architectures?\nHow should symbolic structures be represented within neural networks and extracted from them?\nHow should common-sense knowledge be learned and reasoned about?\nHow can abstract knowledge that is hard to encode logically be handled?\n\n\n== Techniques and contributions ==\nThis section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section.\n\n\n=== AI programming languages ===\nThe key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read-eval-print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code.\nOther key innovations pioneered by LISP that have spread to other programming languages include:\n\nGarbage collection\nDynamic typing\nHigher-order functions\nRecursion\nConditionals\nPrograms were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages.\nIn contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built-in store of facts and clauses that could be queried by a read-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based on Horn clauses with a closed-world assumption—any facts not known were considered false—and a unique name assumption for primitive terms—e.g., the identifier barack_obama was considered to refer to exactly one object. Backtracking and unification are built-in to Prolog.\nAlain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article.\nProlog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages.\nJapan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail.\nSmalltalk was another influential AI programming language. For example, it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or (CLOS), that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp-based object-oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-time meta-object protocol.\nFor other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read-eval-print loop, functional elements such as higher-order functions, and object-oriented programming that includes metaclasses.\n\n\n=== Search ===\n\nSearch arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI-search tree search algorithms are breadth-first search, depth-first search, A*, and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions.\n\n\n=== Knowledge representation and reasoning ===\n\nMultiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning.\n\n\n==== Knowledge representation ====\n\nSemantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource that can also be viewed as an ontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted from Wikipedia with WordNet synsets. The Disease Ontology is an example of a medical ontology currently being used.\nDescription logic is a logic for automated classification of ontologies and for detecting inconsistent classification data. OWL is a language used to represent ontologies with description logic. Protégé is an ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT.\nFirst-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic include temporal logic, to handle time; epistemic logic, to reason about agent knowledge; modal logic, to handle possibility and necessity; and probabilistic logics to handle logic and probability together.\n\n\n==== Automatic theorem proving ====\n\nExamples of automated theorem provers for first-order logic are:\n\nProver9\nACL2\nVampire\nProver9 can be used in conjunction with the Mace4 model checker. ACL2 is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known as Nqthm.\n\n\n==== Reasoning in knowledge-based systems ====\n\nKnowledge-based systems have an explicit knowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store.\nForward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used, Horn Clauses. Pattern-matching, specifically unification, is used in Prolog.\nA more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture.\nCognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-level chunks.\n\n\n==== Commonsense reasoning ====\n\nMarvin Minsky first proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea to scripts for common routines, such as dining out. Cyc has attempted to capture useful common-sense knowledge and has \"micro-theories\" to handle particular kinds of domain-specific reasoning.\nQualitative simulation, such as Benjamin Kuipers's QSIM, approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure.\nSimilarly, Allen's temporal interval algebra is a simplification of reasoning about time and Region Connection Calculus is a simplification of reasoning about spatial relationships. Both can be solved with constraint solvers.\n\n\n==== Constraints and constraint-based reasoning ====\n\nConstraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those for RCC or Temporal Algebra, along with solving other kinds of puzzle problems, such as Wordle, Sudoku, cryptarithmetic problems, and so on. Constraint logic programming can be used to solve scheduling problems, for example with constraint handling rules (CHR).\n\n\n=== Automated planning ===\n\nThe General Problem Solver (GPS) cast planning as problem-solving used means-ends analysis to create plans. STRIPS took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.  Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem.\n\n\n=== Natural language processing ===\n\nNatural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions.\nParsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. Latent semantic analysis (LSA) and explicit semantic analysis also provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles.\nNew deep learning approaches based on Transformer models have now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural language processing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque.\n\n\n=== Agents and multi-agent systems ===\n\nAgents are autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication. The sophistication of agents varies from simple reactive agents, to those with a model of the world and automated planning capabilities, possibly a BDI agent, i.e., one with beliefs, desires, and intentions – or alternatively a reinforcement learning model learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture that includes deep learning for perception.\nIn contrast, a multi-agent system consists of multiple agents that communicate amongst themselves with some inter-agent communication language such as Knowledge Query and Manipulation Language (KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization.\n\n\n== Controversies ==\nControversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic \"neats\") and non-logicists (the anti-logic \"scruffies\")—and between those who embraced AI but rejected symbolic approaches—primarily connectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters.\n\n\n=== The Frame Problem: knowledge representation challenges for first-order logic ===\n\nLimitations were discovered in using simple first-order logic to reason about dynamic domains. Problems were discovered both with regards to enumerating the preconditions for an action to succeed and in providing axioms for what did not change after an action was performed.\nMcCarthy and Hayes introduced the Frame Problem in 1969 in the paper, \"Some Philosophical Problems from the Standpoint of Artificial Intelligence.\" A simple example occurs in \"proving that one person could get into conversation with another\", as an axiom asserting \"if a person has a telephone he still has it after looking up a number in the telephone book\" would be required for the deduction to succeed. Similar axioms would be required for other domain actions to specify what did not change.\nA similar problem, called the Qualification Problem, occurs in trying to enumerate the preconditions for an action to succeed. An infinite number of pathological conditions can be imagined, e.g., a banana in a tailpipe could prevent a car from operating correctly.\nMcCarthy's approach to fix the frame problem was circumscription, a kind of non-monotonic logic where deductions could be made from actions that need only specify what would change while not having to explicitly specify everything that would not change. Other non-monotonic logics provided truth maintenance systems that revised beliefs leading to contradictions.\nOther ways of handling more open-ended domains included probabilistic reasoning systems and machine learning to learn new concepts and rules.  McCarthy's Advice Taker can be viewed as an inspiration here, as it could incorporate new knowledge provided by a human in the form of assertions or rules. For example, experimental symbolic machine learning systems explored the ability to take high-level natural language advice and to interpret it into domain-specific actionable rules.\nSimilar to the problems in handling dynamic domains, common-sense reasoning is also difficult to capture in formal reasoning. Examples of common-sense reasoning include implicit reasoning about how people think or general knowledge of day-to-day events, objects, and living creatures.  This kind of knowledge is taken for granted and not viewed as noteworthy. Common-sense reasoning is an open area of research and challenging both for symbolic systems (e.g., Cyc has attempted to capture key parts of this knowledge over more than a decade) and neural systems (e.g., self-driving cars that do not know not to drive into cones or not to hit pedestrians walking a bicycle).\nMcCarthy viewed his Advice Taker as having common-sense, but his definition of common-sense was different than the one above. He defined a program as having common sense \"if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows.\"\n\n\n=== Connectionist AI: philosophical challenges and sociological conflicts ===\nConnectionist approaches include earlier work on neural networks, such as perceptrons; work in the mid to late 80s, such as Danny Hillis's Connection Machine and Yann LeCun's advances in convolutional neural networks; to today's more advanced approaches, such as Transformers, GANs, and other work in deep learning.\nThree philosophical positions have been outlined among connectionists:\n\nImplementationism—where connectionist architectures implement the capabilities for symbolic processing,\nRadical connectionism—where symbolic processing is rejected totally, and connectionist architectures underlie intelligence and are fully sufficient to explain it,\nModerate connectionism—where symbolic processing and connectionist architectures are viewed as complementary and both are required for intelligence.\nOlazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids:\nThe third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only \"approximations\" (contrary to what radical connectionists would claim).\nGary Marcus has claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical:To think that we can simply abandon symbol-manipulation is to suspend disbelief.\n\nAnd yet, for the most part, that's how most current AI proceeds. Hinton and many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet, neural networks typically try to solve tasks by statistical approximation and learning from examples.According to Marcus, Geoffrey Hinton and his colleagues have been vehemently \"anti-symbolic\":When deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols to aether, one of science's greatest mistakes.\n...\n\nSince then, his anti-symbolic campaign has only increased in intensity. In 2016, Yann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was \"a huge mistake,\" likening it to investing in internal combustion engines in the era of electric cars.\nPart of these disputes may be due to unclear terminology: \n\nTuring award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning.It is worth noting that, from a theoretical perspective, the boundary of advantages between connectionist AI and symbolic AI may not be as clear-cut as it appears. For instance, Heng Zhang and his colleagues have proved that mainstream knowledge representation formalisms are  recursively isomorphic, provided they are universal or have equivalent expressive power. This finding implies that there is no fundamental distinction between using symbolic or connectionist knowledge representation formalisms for the realization of artificial general intelligence (AGI). Moreover, the existence of recursive isomorphisms suggests that different technical approaches can draw insights from one another. From this perspective, it seems unnecessary to overemphasize the advantages of any single technical school; instead, mutual learning and integration may offer the most promising path toward the realization of AGI.\n\n\n=== Situated robotics: the world as a model ===\nAnother critique of symbolic AI is the embodied cognition approach:\n\nThe embodied cognition approach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.\nRodney Brooks invented behavior-based robotics, one approach to embodied cognition. Nouvelle AI, another name for this approach, is viewed as an alternative to both symbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created the subsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes in Intelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: \"There is no clean division between perception (abstraction) and reasoning in the real world.\" He called his robots \"Creatures\" and each layer was \"composed of a fixed-topology network of simple finite state machines.\"  In the Nouvelle AI approach, \"First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.\"  His emphasis on real-world testing was in contrast to \"Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems\" and the use of the blocks world in symbolic AI systems such as SHRDLU.\n\n\n=== Current views ===\nEach approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge.\n\nHybrid AIs incorporating one or more of these approaches are currently viewed as the path forward. Russell and Norvig conclude that:Overall, Dreyfus saw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== Citations ==\n\n\n== References ==\nBrooks, Rodney A. (1991). \"Intelligence without representation\". Artificial Intelligence. 47 (1): 139–159. doi:10.1016/0004-3702(91)90053-M. ISSN 0004-3702. S2CID 207507849. Retrieved 2022-09-13.\nClancey, William (1987). Knowledge-Based Tutoring: The GUIDON Program (MIT Press Series in Artificial Intelligence) (Hardcover ed.).\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3..\nDreyfus, Hubert L (1981). \"From micro-worlds to knowledge representation: AI at an impasse\" (PDF). Mind Design. MIT Press, Cambridge, MA: 161–204.\nGarcez, Artur S. d'Avila; Broda, Krysia; Gabbay, Dov M.; Gabbay, Augustus de Morgan Professor of Logic Dov M. (2002). Neural-Symbolic Learning Systems: Foundations and Applications. Springer Science & Business Media. ISBN 978-1-85233-512-0.\nGarcez, Artur; Besold, Tarek; De Raedt, Luc; Földiák, Peter; Hitzler, Pascal; Icard, Thomas; Kühnberger, Kai-Uwe; Lamb, Luís; Miikkulainen, Risto; Silver, Daniel (2015). Neural-Symbolic Learning and Reasoning: Contributions and Challenges. AAI Spring Symposium - Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches. Stanford, CA: AAAI Press. doi:10.13140/2.1.1779.4243.\nGarcez, Artur d'Avila; Gori, Marco; Lamb, Luis C.; Serafini, Luciano; Spranger, Michael; Tran, Son N. (2019), Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning, arXiv:1905.06088\nGarcez, Artur d'Avila; Lamb, Luis C. (2020), Neurosymbolic AI: The 3rd Wave, arXiv:2012.05876\nHaugeland, John (1985), Artificial Intelligence: The Very Idea, Cambridge, Mass: MIT Press, ISBN 0-262-08153-9\nHayes-Roth, Frederick; Murray, William; Adelman, Leonard (2015). \"Expert systems\". AccessScience. doi:10.1036/1097-8542.248550.\nHonavar, Vasant; Uhr, Leonard (1994). Symbolic Artificial Intelligence, Connectionist Networks & Beyond (Technical report). Iowa State University Digital Repository, Computer Science Technical Reports. 76. p. 6.\nHonavar, Vasant (1995). Symbolic Artificial Intelligence and Numeric Artificial Neural Networks: Towards a Resolution of the Dichotomy. The Springer International Series In Engineering and Computer Science. Springer US. pp. 351–388. doi:10.1007/978-0-585-29599-2_11.\nHowe, J. (November 1994). \"Artificial Intelligence at Edinburgh University: a Perspective\". Archived from the original on 15 May 2007. Retrieved 30 August 2007.\nKautz, Henry (2020-02-11). The Third AI Summer, Henry Kautz, AAAI 2020 Robert S. Engelmore Memorial Award Lecture. Retrieved 2022-07-06.\nKautz, Henry (2022). \"The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture\". AI Magazine. 43 (1): 93–104. doi:10.1609/aimag.v43i1.19122. ISSN 2371-9621. S2CID 248213051. Retrieved 2022-07-12.\nKodratoff, Yves; Michalski, Ryszard, eds. (1990). Machine Learning : an Artificial Intelligence Approach. Vol. III. San Mateo, Calif.: Morgan Kaufman. ISBN 0-934613-09-5. OCLC 893488404.\nKolata, G. (1982). \"How can computers get common sense?\". Science. 217 (4566): 1237–1238. Bibcode:1982Sci...217.1237K. doi:10.1126/science.217.4566.1237. PMID 17837639.\nMaker, Meg Houston (2006). \"AI@50: AI Past, Present, Future\". Dartmouth College. Archived from the original on 3 January 2007. Retrieved 16 October 2008.\nMarcus, Gary; Davis, Ernest (2019). Rebooting AI: Building Artificial Intelligence We Can Trust. New York: Pantheon Books. ISBN 9781524748258. OCLC 1083223029.\nMarcus, Gary (2020), The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence, arXiv:2002.06177\nMcCarthy, John (1959). PROGRAMS WITH COMMON SENSE. Symposium on Mechanization of Thought Processes. NATIONAL PHYSICAL LABORATORY, TEDDINGTON, UK. p. 8.\nMcCarthy, John; Hayes, Patrick (1969). \"Some Philosophical Problems From the Standpoint of Artificial Intelligence\". Machine Intelligence 4. B. Meltzer, Donald Michie (eds.): 463–502.\nMcCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, Massachusetts: A. K. Peters, ISBN 1-5688-1205-1\nMichalski, Ryszard; Carbonell, Jaime; Mitchell, Tom, eds. (1983). Machine Learning : an Artificial Intelligence Approach. Vol. I. Palo Alto, Calif.: Tioga Publishing Company. ISBN 0-935382-05-4. OCLC 9262069.\nMichalski, Ryszard; Carbonell, Jaime; Mitchell, Tom, eds. (1986). Machine Learning : an Artificial Intelligence Approach. Vol. II. Los Altos, Calif.: Morgan Kaufman. ISBN 0-934613-00-1.\nNewell, Allen; Simon, Herbert A. (1972). Human Problem Solving (1st ed.). Englewood Cliffs, New Jersey: Prentice Hall. ISBN 0-13-445403-0.\nNewell, Allen; Simon, H. A. (1976). \"Computer Science as Empirical Inquiry: Symbols and Search\". Communications of the ACM. 19 (3): 113–126. doi:10.1145/360018.360022.\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\nOlazaran, Mikel (1993-01-01), \"A Sociological History of the Neural Network Controversy\", in Yovits, Marshall C. (ed.), Advances in Computers Volume 37, vol. 37, Elsevier, pp. 335–425, doi:10.1016/S0065-2458(08)60408-8, ISBN 9780120121373, retrieved 2023-10-31\nPearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. San Mateo, California: Morgan Kaufmann. ISBN 978-1-55860-479-7. OCLC 249625842.\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-13-461099-3. LCCN 20190474.\nRossi, Francesca (2022-07-06). \"AAAI2022: Thinking Fast and Slow in AI (AAAI 2022 Invited Talk)\". Retrieved 2022-07-06.\nSelman, Bart (2022-07-06). \"AAAI2022: Presidential Address: The State of AI\". Retrieved 2022-07-06.\nSerafini, Luciano; Garcez, Artur d'Avila (2016-07-07), Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge, arXiv:1606.04422\nSpiegelhalter, David J.; Dawid, A. Philip; Lauritzen, Steffen; Cowell, Robert G. (1993). \"Bayesian analysis in expert systems\". Statistical Science. 8 (3).\nTuring, A. M. (1950). \"I.—Computing Machinery and Intelligence\". Mind. LIX (236): 433–460. doi:10.1093/mind/LIX.236.433. ISSN 0026-4423. Retrieved 2022-09-14.\nValiant, Leslie G (2008). \"Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence\". In Hariharan, R.; Mukund, M.; Vinay, V. (eds.). Foundations of Software Technology and Theoretical Computer Science (Bangalore). pp. 415–422.\nXifan Yao; Jiajun Zhou; Jiangming Zhang; Claudio R. Boer (2017). \"From Intelligent Manufacturing to Smart Manufacturing for Industry 4.0 Driven by Next Generation Artificial Intelligence and Further on\". 2017 5th International Conference on Enterprise Systems (ES). IEEE. pp. 311–318. doi:10.1109/es.2017.58. ISBN 978-1-5386-0936-1.",
    "categories": [
      "Articles with short description",
      "Artificial intelligence",
      "Short description is different from Wikidata"
    ],
    "year_mentioned": 2002
  },
  {
    "title": "Expert system",
    "url": "https://en.wikipedia.org/wiki/Expert_system",
    "content": "In artificial intelligence (AI), an expert system is a computer system emulating the decision-making ability of a human expert.\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural programming code.  Expert systems were among the first truly successful forms of AI software. They were created in the 1970s and then proliferated in the 1980s, being then widely regarded as the future of AI — before the advent of successful artificial neural networks.\nAn expert system is divided into two subsystems: 1) a knowledge base, which represents facts and rules; and 2) an inference engine, which applies the rules to the known facts to deduce new facts, and can include explaining and debugging abilities.\n\n\n== History ==\n\n\n=== Early development ===\nSoon after the dawn of modern computers in the late 1940s and early 1950s, researchers started realizing the immense potential these machines had for modern society. One of the first challenges was to make such machines able to “think” like humans – in particular, making these machines able to make important decisions the way humans do. The medical–healthcare field presented the tantalizing challenge of enabling these machines to make medical diagnostic decisions.\nThus, in the late 1950s, right after the information age had fully arrived, researchers started experimenting with the prospect of using computer technology to emulate human decision making. For example, biomedical researchers started creating computer-aided systems for diagnostic applications in medicine and biology. These early diagnostic systems used patients’ symptoms and laboratory test results as inputs to generate a diagnostic outcome.\nThese systems were often described as the early forms of expert systems. However, researchers realized that there were significant limits when using traditional methods such as flow charts,\n statistical pattern matching, or probability theory.\n\n\n=== Formal introduction and later developments ===\n\nThis previous situation gradually led to the development of expert systems, which used knowledge-based approaches. These expert systems in medicine were the MYCIN expert system, the Internist-I expert system and later, in the middle of the 1980s, the CADUCEUS.\nExpert systems were formally introduced around 1965 by the Stanford Heuristic Programming Project led by Edward Feigenbaum, who is sometimes termed the \"father of expert systems\"; other key early contributors were Bruce Buchanan and Randall Davis. The Stanford researchers tried to identify domains where expertise was highly valued and complex, such as diagnosing infectious diseases (Mycin) and identifying unknown organic molecules (Dendral). The idea that \"intelligent systems derive their power from the knowledge they possess rather than from the specific formalisms and inference schemes they use\" – as Feigenbaum said – was at the time a significant step forward, since the past research had been focused on heuristic computational methods, culminating in attempts to develop very general-purpose problem solvers (foremostly the conjunct work of Allen Newell and Herbert Simon). Expert systems became some of the first truly successful forms of artificial intelligence (AI) software.\nResearch on expert systems was also active in Europe. In the US, the focus tended to be on the use of production rule systems, first on systems hard coded on top of Lisp programming environments and then on expert system shells developed by vendors such as Intellicorp. In Europe, research focused more on systems and expert systems shells developed in Prolog. The advantage of Prolog systems was that they employed a form of rule-based programming that was based on formal logic.\nOne such early expert system shell based on Prolog was APES.\nOne of the first use cases of Prolog and APES was in the legal area namely, the encoding of a large portion of the British Nationality Act. Lance Elliot wrote: \"The British Nationality Act was passed in 1981 and shortly thereafter was used as a means of showcasing the efficacy of using Artificial Intelligence (AI) techniques and technologies, doing so to explore how the at-the-time newly enacted statutory law might be encoded into a computerized logic-based formalization. A now oft-cited research paper entitled “The British Nationality Act as a Logic Program” was published in 1986 and subsequently became a hallmark for subsequent work in AI and the law.\"\nIn the 1980s, expert systems proliferated. Universities offered expert system courses and two-thirds of the Fortune 500 companies applied the technology in daily business activities. Interest was international with the Fifth Generation Computer Systems project in Japan and increased research funding in Europe.\nIn 1981, the first IBM PC, with the PC DOS operating system, was introduced. The imbalance between the low cost of the relatively powerful chips in the PC, compared to the much more expensive cost of processing power in the mainframes that dominated the corporate IT world at the time, created a new type of architecture for corporate computing, termed the client–server model. Calculations and reasoning could be performed at a fraction of the price of a mainframe using a PC. This model also enabled business units to bypass corporate IT departments and directly build their own applications. As a result, client-server had a tremendous impact on the expert systems market. Expert systems were already outliers in much of the business world, requiring new skills that many IT departments did not have and were not eager to develop. They were a natural fit for new PC-based shells that promised to put application development into the hands of end users and experts. Until then, the main development environment for expert systems had been high end Lisp machines from Xerox, Symbolics, and Texas Instruments. With the rise of the PC and client-server computing, vendors such as Intellicorp and Inference Corporation shifted their priorities to developing PC-based tools. Also, new vendors, often financed by venture capital started appearing regularly.\nThe first expert system to be used in a design capacity for a large-scale product was the Synthesis of Integral Design (SID) software program, developed in 1982. Written in Lisp, SID generated 93% of the VAX 9000 CPU logic gates. Input to the software was a set of rules created by several expert logic designers. SID expanded the rules and generated software logic synthesis routines many times the size of the rules themselves. Surprisingly, the combination of these rules resulted in an overall design that exceeded the capabilities of the experts themselves, and in many cases out-performed the human counterparts. While some rules contradicted others, top-level control parameters for speed and area provided the tie-breaker. The program was highly controversial but used nevertheless due to project budget constraints. It was terminated by logic designers after the VAX 9000 project completion.\nDuring the years before the middle of the 1970s, the expectations of what expert systems can accomplish in many fields tended to be extremely optimistic. At the start of these early studies, researchers were hoping to develop entirely automatic (i.e., completely computerized) expert systems. The expectations of people of what computers can do were frequently too idealistic. This situation radically changed after Richard M. Karp published his breakthrough paper: “Reducibility among Combinatorial Problems” in the early 1970s. Thanks to Karp's work, together with other scholars, like Hubert L. Dreyfus, it became clear that there are certain limits and possibilities when one designs computer algorithms. His findings describe what computers can do and what they cannot do. Many of the computational problems related to this type of expert systems have certain pragmatic limits. These findings laid down the groundwork that led to the next developments in the field.\nIn the 1990s and beyond, the term expert system and the idea of a standalone AI system mostly dropped from the IT lexicon. There are two interpretations of this. One is that \"expert systems failed\": the IT world moved on because expert systems did not deliver on their over hyped promise. The other is the mirror opposite, that expert systems were simply victims of their success: as IT professionals grasped concepts such as rule engines, such tools migrated from being standalone tools for developing special purpose expert systems, to being one of many standard tools. Other researchers suggest that Expert Systems caused inter-company power struggles when the IT organization lost its exclusivity in software modifications to users or Knowledge Engineers.\nIn the first decade of the 2000s, there was a \"resurrection\" for the technology, while using the term rule-based systems, with significant success stories and adoption. Many of the leading major business application suite vendors (such as SAP, Siebel, and Oracle) integrated expert system abilities into their suite of products as a way to specify business logic. Rule engines are no longer simply for defining the rules an expert would use but for any type of complex, volatile, and critical business logic; they often go hand in hand with business process automation and integration environments.\n\n\n=== Current approaches to expert systems ===\nThe limits of prior type of expert systems prompted researchers to develop new types of approaches. They have developed more efficient, flexible, and powerful methods to simulate the human decision-making process. Some of the approaches that researchers have developed are based on new methods of artificial intelligence (AI), and in particular in machine learning and data mining approaches with a feedback mechanism. Recurrent neural networks often take advantage of such mechanisms. Related is the discussion on the disadvantages section.\nModern systems can incorporate new knowledge more easily and thus update themselves easily. Such systems can generalize from existing knowledge better and deal with vast amounts of complex data. Related is the subject of big data here. Sometimes these types of expert systems are called \"intelligent systems.\"\nMore recently, it can be argued that expert systems have moved into the area of business rules and business rules management systems.\n\n\n== Software architecture ==\n\nAn expert system is an example of a knowledge-based system. Expert systems were the first commercial systems to use a knowledge-based architecture. In general view, an expert system includes the following components: a knowledge base, an inference engine, an explanation facility, a knowledge acquisition facility, and a user interface.\nThe knowledge base represents facts about the world. In early expert systems such as Mycin and Dendral, these facts were represented mainly as flat assertions about variables. In later expert systems developed with commercial shells, the knowledge base took on more structure and used concepts from object-oriented programming. The world was represented as classes, subclasses, and instances and assertions were replaced by values of object instances. The rules worked by querying and asserting values of the objects.\nThe inference engine is an automated reasoning system that evaluates the current state of the knowledge-base, applies relevant rules, and then asserts new knowledge into the knowledge base. The inference engine may also include abilities for explanation, so that it can explain to a user the chain of reasoning used to arrive at a particular conclusion by tracing back over the firing of rules that resulted in the assertion.\nThere are mainly two modes for an inference engine: forward chaining and backward chaining. The different approaches are dictated by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule. In forward chaining an antecedent fires and asserts the consequent. For example, consider the following rule:\n\n  \n    \n      \n        R\n        1\n        :\n        \n          \n            M\n            a\n            n\n          \n        \n        (\n        x\n        )\n        \n        ⟹\n        \n        \n          \n            M\n            o\n            r\n            t\n            a\n            l\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle R1:{\\mathit {Man}}(x)\\implies {\\mathit {Mortal}}(x)}\n  \n\nA simple example of forward chaining would be to assert Man(Socrates) to the system and then trigger the inference engine. It would match R1 and assert Mortal(Socrates) into the knowledge base.\nBackward chaining is a bit less straight forward. In backward chaining the system looks at possible conclusions and works backward to see if they might be true. So if the system was trying to determine if Mortal(Socrates) is true it would find R1 and query the knowledge base to see if Man(Socrates) is true. One of the early innovations of expert systems shells was to integrate inference engines with a user interface. This could be especially powerful with backward chaining. If the system needs to know a particular fact but does not, then it can simply generate an input screen and ask the user if the information is known. So in this example, it could use R1 to ask the user if Socrates was a Man and then use that new information accordingly.\nThe use of rules to explicitly represent knowledge also enabled explanation abilities. In the simple example above if the system had used R1 to assert that Socrates was Mortal and a user wished to understand why Socrates was mortal they could query the system and the system would look back at the rules which fired to cause the assertion and present those rules to the user as an explanation. In English, if the user asked \"Why is Socrates Mortal?\" the system would reply \"Because all men are mortal and Socrates is a man\". A significant area for research was the generation of explanations from the knowledge base in natural English rather than simply by showing the more formal but less intuitive rules.\nAs expert systems evolved, many new techniques were incorporated into various types of inference engines. Some of the most important of these were:\n\nTruth maintenance. These systems record the dependencies in a knowledge-base so that when facts are altered, dependent knowledge can be altered accordingly. For example, if the system learns that Socrates is no longer known to be a man it will revoke the assertion that Socrates is mortal.\nHypothetical reasoning. In this, the knowledge base can be divided up into many possible views, a.k.a. worlds. This allows the inference engine to explore multiple possibilities in parallel. For example, the system may want to explore the consequences of both assertions, what will be true if Socrates is a Man and what will be true if he is not?\nUncertainty systems. One of the first extensions of simply using rules to represent knowledge was also to associate a probability with each rule. So, not to assert that Socrates is mortal, but to assert Socrates may be mortal with some probability value. Simple probabilities were extended in some systems with sophisticated mechanisms for uncertain reasoning, such as fuzzy logic, and combination of probabilities.\nOntology classification. With the addition of object classes to the knowledge base, a new type of reasoning was possible. Along with reasoning simply about object values, the system could also reason about object structures. In this simple example, Man can represent an object class and R1 can be redefined as a rule that defines the class of all men. These types of special purpose inference engines are termed classifiers. Although they were not highly used in expert systems, classifiers are very powerful for unstructured volatile domains, and are a key technology for the Internet and the emerging Semantic Web.\n\n\n== Advantages ==\nThe goal of knowledge-based systems is to make the critical information required for the system to work explicit rather than implicit. In a traditional computer program, the logic is embedded in code that can typically only be reviewed by an IT specialist. With an expert system, the goal was to specify the rules in a format that was intuitive and easily understood, reviewed, and even edited by domain experts rather than IT experts. The benefits of this explicit knowledge representation were rapid development and ease of maintenance.\nEase of maintenance is the most obvious benefit. This was achieved in two ways. First, by removing the need to write conventional code, many of the normal problems that can be caused by even small changes to a system could be avoided with expert systems. Essentially, the logical flow of the program (at least at the highest level) was simply a given for the system, simply invoke the inference engine. This also was a reason for the second benefit: rapid prototyping. With an expert system shell it was possible to enter a few rules and have a prototype developed in days rather than the months or year typically associated with complex IT projects.\nA claim for expert system shells that was often made was that they removed the need for trained programmers and that experts could develop systems themselves. In reality, this was seldom if ever true. While the rules for an expert system were more comprehensible than typical computer code, they still had a formal syntax where a misplaced comma or other character could cause havoc as with any other computer language. Also, as expert systems moved from prototypes in the lab to deployment in the business world, issues of integration and maintenance became far more critical. Inevitably demands to integrate with, and take advantage of, large legacy databases and systems arose. To accomplish this, integration required the same skills as any other type of system.\nSumming up the benefits of using expert systems, the following can be highlighted:\n\nIncreased availability and reliability: Expertise can be accessed on any computer hardware and the system always completes responses on time.\nMultiple expertise: Several expert systems can be run simultaneously to solve a problem. and gain a higher level of expertise than a human expert.\nExplanation: Expert systems always describe of how the problem was solved.\nFast response: The expert systems are fast and able to solve a problem in real-time.\nReduced cost: The cost of expertise for each user is significantly reduced.\n\n\n== Disadvantages ==\nThe most common disadvantage cited for expert systems in the academic literature is the knowledge acquisition problem. Obtaining the time of domain experts for any software application is always difficult, but for expert systems it was especially difficult because the experts were by definition highly valued and in constant demand by the organization. As a result of this problem, a great deal of research in the later years of expert systems was focused on tools for knowledge acquisition, to help automate the process of designing, debugging, and maintaining rules defined by experts. However, when looking at the life-cycle of expert systems in actual use, other problems – essentially the same problems as those of any other large system – seem at least as critical as knowledge acquisition: integration, access to large databases, and performance.\nPerformance could be especially problematic because early expert systems were built using tools (such as earlier Lisp versions) that interpreted code expressions without first compiling them. This provided a powerful development environment, but with the drawback that it was virtually impossible to match the efficiency of the fastest compiled languages (such as C). System and database integration were difficult for early expert systems because the tools were mostly in languages and platforms that were neither familiar to nor welcome in most corporate IT environments – programming languages such as Lisp and Prolog, and hardware platforms such as Lisp machines and personal computers. As a result, much effort in the later stages of expert system tool development was focused on integrating with legacy environments such as COBOL and large database systems, and on porting to more standard platforms. These issues were resolved mainly by the client–server paradigm shift, as PCs were gradually accepted in the IT environment as a legitimate platform for serious business system development and as affordable minicomputer servers provided the processing power needed for AI applications.\nAnother major challenge of expert systems emerges when the size of the knowledge base increases. This causes the processing complexity to increase. For instance, when an expert system with 100 million rules was envisioned as the ultimate expert system, it became obvious that such system would be too complex and it would face too many computational problems. An inference engine would have to be able to process huge numbers of rules to reach a decision.\nHow to verify that decision rules are consistent with each other is also a challenge when there are too many rules. Usually such problem leads to a satisfiability (SAT) formulation. This is a well-known NP-complete problem Boolean satisfiability problem. If we assume only binary variables, say n of them, and then the corresponding search space is of size 2\n  \n    \n      \n        \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle ^{n}}\n  \n. Thus, the search space can grow exponentially.\nThere are also questions on how to prioritize the use of the rules to operate more efficiently, or how to resolve ambiguities (for instance, if there are too many else-if sub-structures within one rule) and so on.\nOther problems are related to the overfitting and overgeneralization effects when using known facts and trying to generalize to other cases not described explicitly in the knowledge base. Such problems exist with methods that employ machine learning approaches too.\nAnother problem related to the knowledge base is how to make updates of its knowledge quickly and effectively. Also how to add a new piece of knowledge (i.e., where to add it among many rules) is challenging. Modern approaches that rely on machine learning methods are easier in this regard.\nBecause of the above challenges, it became clear that new approaches to AI were required instead of rule-based technologies. These new approaches are based on the use of machine learning techniques, along with the use of feedback mechanisms.\nThe key challenges that expert systems in medicine (if one considers computer-aided diagnostic systems as modern expert systems), and perhaps in other application domains, include issues related to aspects such as: big data, existing regulations, healthcare practice, various algorithmic issues, and system assessment.\nFinally, the following disadvantages of using expert systems can be summarized:\n\nExpert systems have superficial knowledge, and a simple task can potentially become computationally expensive.\nExpert systems require knowledge engineers to input the data, data acquisition is very hard.\nThe expert system may choose the most inappropriate method for solving a particular problem.\nProblems of ethics in the use of any form of AI are very relevant at present.\nIt is a closed world with specific knowledge, in which there is no deep perception of concepts and their interrelationships until an expert provides them.\n\n\n== Applications ==\nHayes-Roth divides expert systems applications into 10 categories illustrated in the following table. The example applications were not in the original Hayes-Roth table, and some of them arose well afterward. Any application that is not footnoted is described in the Hayes-Roth book. Also, while these categories provide an intuitive framework to describe the space of expert systems applications, they are not rigid categories, and in some cases an application may show traits of more than one category.\n\nHearsay was an early attempt at solving voice recognition through an expert systems approach. For the most part this category of expert systems was not all that successful. Hearsay and all interpretation systems are essentially pattern recognition systems—looking for patterns in noisy data. In the case of Hearsay recognizing phonemes in an audio stream. Other early examples were analyzing sonar data to detect Russian submarines. These kinds of systems proved much more amenable to a neural network AI solution than a rule-based approach.\nCADUCEUS and MYCIN were medical diagnosis systems. The user describes their symptoms to the computer as they would to a doctor and the computer returns a medical diagnosis.\nDendral was a tool to study hypothesis formation in the identification of organic molecules. The general problem it solved—designing a solution given a set of constraints—was one of the most successful areas for early expert systems applied to business domains such as salespeople configuring Digital Equipment Corporation (DEC) VAX computers and mortgage loan application development.\nSMH.PAL is an expert system for the assessment of students with multiple disabilities.\nGARVAN-ES1 was a medical expert system, developed at the Garvan Institute of Medical Research, that provided automated clinical diagnostic comments on endocrine reports from a pathology laboratory. It was one of the first medical expert systems to go into routine clinical use internationally and the first expert system to be used for diagnosis daily in Australia. The system was written in \"C\" and ran on a PDP-11 in 64K of memory. It had 661 rules that were compiled; not interpreted.\n\nMistral is an expert system to monitor dam safety, developed in the 1990s by Ismes (Italy). It gets data from an automatic monitoring system and performs a diagnosis of the state of the dam. Its first copy, installed in 1992 on the Ridracoli Dam (Italy), is still operational 24/7/365. It has been installed on several dams in Italy and abroad (e.g., Itaipu Dam in Brazil), and on landslide sites under the name of Eydenet, and on monuments under the name of Kaleidos. Mistral is a registered trade mark of CESI.\n\n\n== See also ==\nAI winter\nCLIPS\nConstraint logic programming\nConstraint satisfaction\nKnowledge engineering\nLearning classifier system\nRule-based machine learning\n\n\n== References ==\n\n\n=== Works cited ===\n\n\n== External links ==\nExpert System tutorial on Code Project",
    "categories": [
      "All articles that may be too long",
      "All articles with failed verification",
      "All articles with unsourced statements",
      "Articles that may be too long from February 2024",
      "Articles to be expanded from January 2022",
      "Articles with failed verification from May 2024",
      "Articles with short description",
      "Articles with unsourced statements from October 2019",
      "CS1: long volume value",
      "CS1 maint: publisher location",
      "Decision support systems",
      "Expert systems",
      "Information systems",
      "Short description is different from Wikidata",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 1981
  },
  {
    "title": "Natural language processing",
    "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "content": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval, knowledge representation, computational linguistics, and linguistics more broadly.\nMajor processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation.\n\n\n== History ==\n\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n\n\n=== Symbolic NLP (1950s – early 1990s) ===\n\nThe premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n\n1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed.\n1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapy, written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer  memory at the time.\n1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the first chatterbots were written (e.g., PARRY).\n1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.\n\n\n=== Statistical NLP (1990s–present) ===\nUp until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. \n\n1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, large quantities of non-annotated data are available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical.\n2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)\n2010: Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modeling, and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. This shift gained momentum due to results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy.\n\n\n== Approaches: Symbolic, statistical, neural networks ==\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: such as by writing grammars or devising heuristic rules for stemming.\nMachine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: \n\nboth statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.\nlanguage models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.\nthe larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems.\nRule-based systems are commonly used:\n\nwhen the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,\nfor preprocessing in NLP pipelines, e.g., tokenization, or\nfor post-processing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.\n\n\n=== Statistical approach ===\nIn the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.\nThe earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.\nOnly the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\n\n\n=== Neural networks ===\n\nA major drawback of statistical methods is that they require elaborate feature engineering. Since 2015, neural network–based methods have increasingly replaced traditional statistical approaches, using semantic networks and word embeddings to capture semantic properties of words.  \nIntermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. \nNeural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.\n\n\n== Common NLP tasks ==\nThe following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\nThough natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n\n\n=== Text and speech processing ===\n\nOptical character recognition (OCR)\nGiven an image representing printed text, determine the corresponding text.\nSpeech recognition\nGiven a sound clip of a person or people speaking, determine the textual representation of the speech.  This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above).  In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.\nSpeech segmentation\nGiven a sound clip of a person or people speaking, separate it into words.  A subtask of speech recognition and typically grouped with it.\nText-to-speech\nGiven a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.\nWord segmentation (Tokenization)\nTokenization is a text-processing technique that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods.\nFor a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.\n\n\n=== Morphological analysis ===\n\nLemmatization\nThe task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.\nMorphological segmentation\nSeparate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.\nPart-of-speech tagging\nGiven a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech.\nStemming\nThe process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.\n\n\n=== Syntactic analysis ===\n\nGrammar induction\nGenerate a formal grammar that describes a language's syntax.\nSentence breaking (also known as \"sentence boundary disambiguation\")\nGiven a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).\nParsing\nDetermine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).\n\n\n=== Lexical semantics (of individual words in context) ===\n\nLexical semantics\nWhat is the computational meaning of individual words in context?\nDistributional semantics\nHow can we learn semantic representations from data?\nNamed entity recognition (NER)\nGiven a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient.  For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.  Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. This task is also referred to as token classification.\nSentiment analysis (see also Multimodal sentiment analysis)\nSentiment analysis involves identifying and classifying the emotional tone expressed in text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms.\nTerminology extraction\nThe goal of terminology extraction is to automatically extract relevant terms from a given corpus.\nWord-sense disambiguation (WSD)\nMany words have more than one meaning; we have to select the meaning which makes the most sense in context.  For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet.\nEntity linking\nMany words—typically proper names—refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context.\n\n\n=== Relational semantics (semantics of individual sentences) ===\nRelationship extraction\nGiven a chunk of text, identify the relationships among named entities (e.g. who is married to whom).\nSemantic parsing\nGiven a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).\nSemantic role labelling (see also implicit semantic role labelling below)\nGiven a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles).\n\n\n=== Discourse (semantics beyond individual sentences) ===\nCoreference resolution\nGiven a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).\nDiscourse analysis\nThis rubric includes several related tasks.  One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast).  Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no question, content question, statement, assertion, etc.).\nImplicit semantic role labelling\nGiven a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.\nRecognizing textual entailment\nGiven two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.\nTopic segmentation and recognition\nGiven a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.\nArgument mining\nThe goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.\n\n\n=== Higher-level NLP applications ===\n\nAutomatic summarization (text summarization)\nProduce a readable summary of a chunk of text.  Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.\nGrammatical error correction\nGrammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.\nLogic translation\nTranslate a text from a natural language into formal logic.\nMachine translation (MT)\nAutomatically translate text from one human language to another.  This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.\nNatural language understanding (NLU)\nConvert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.\nNatural language generation (NLG):\nConvert information from computer databases or semantic intents into readable human language.\nBook generation\nNot an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.\nDocument AI\nA Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.\nDialogue management\nComputer systems intended to converse with a human.\nQuestion answering\nGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\").\nText-to-image generation\nGiven a description of an image, generate an image that matches the description.\nText-to-scene generation\nGiven a description of a scene, generate a 3D model of the scene.\nText-to-video\nGiven a description of a video, generate a video that matches the description.\n\n\n== General tendencies and (possible) future directions ==\nBased on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:\n\nInterest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).\nIncreasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\nElimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\n\n\n=== Cognition ===\nMost higher-level NLP applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\nCognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\nAs an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:\n\nApply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience.  When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance.  The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.\nAssign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US Patent 9269353:\n\n  \n    \n      \n        \n          R\n          M\n          M\n          (\n          t\n          o\n          k\n          e\n          \n            n\n            \n              N\n            \n          \n          )\n        \n        =\n        \n          P\n          M\n          M\n          (\n          t\n          o\n          k\n          e\n          \n            n\n            \n              N\n            \n          \n          )\n        \n        ×\n        \n          \n            1\n            \n              2\n              d\n            \n          \n        \n        \n          (\n          \n            \n              ∑\n              \n                i\n                =\n                −\n                d\n              \n              \n                d\n              \n            \n            \n              (\n              (\n              P\n              M\n              M\n              (\n              t\n              o\n              k\n              e\n              \n                n\n                \n                  N\n                \n              \n              )\n            \n            ×\n            \n              P\n              F\n              (\n              t\n              o\n              k\n              e\n              \n                n\n                \n                  N\n                  −\n                  i\n                \n              \n              ,\n              t\n              o\n              k\n              e\n              \n                n\n                \n                  N\n                \n              \n              ,\n              t\n              o\n              k\n              e\n              \n                n\n                \n                  N\n                  +\n                  i\n                \n              \n              )\n              \n                )\n                \n                  i\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {RMM(token_{N})}={PMM(token_{N})}\\times {\\frac {1}{2d}}\\left(\\sum _{i=-d}^{d}{((PMM(token_{N})}\\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\\right)}\n  \n\nWhere\nRMM is the relative measure of meaning\ntoken is any block of text, sentence, phrase or word\nN is the number of tokens being analyzed\nPMM is the probable measure of meaning based on a corpora\nd is the non zero location of the token along the sequence of N tokens\nPF is the probability function specific to a language\nTies with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) and developments in artificial intelligence, specifically tools and technologies using large language model approaches and new directions in artificial general intelligence based on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n Media related to Natural language processing at Wikimedia Commons",
    "categories": [
      "Accuracy disputes from December 2013",
      "All accuracy disputes",
      "All articles needing additional references",
      "All articles needing rewrite",
      "Articles needing additional references from May 2024",
      "Articles with multiple maintenance issues",
      "Articles with short description",
      "CS1 errors: periodical ignored",
      "CS1 maint: location",
      "Commons category link from Wikidata",
      "Computational fields of study",
      "Computational linguistics",
      "Harv and Sfn no-target errors",
      "Natural language processing",
      "Short description is different from Wikidata",
      "Speech recognition",
      "Wikipedia articles needing reorganization from July 2025",
      "Wikipedia articles needing rewrite from July 2025"
    ],
    "year_mentioned": 2003
  },
  {
    "title": "Large language model",
    "url": "https://en.wikipedia.org/wiki/Large_language_model",
    "content": "A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of modern chatbots. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\nThey consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.\nLLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale, such as few-shot learning and compositional reasoning.\nReinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction. Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM's output distribution, against reward signals derived from human or automated preference judgments. This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance.\nBenchmark evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive, multi-task evaluations measuring reasoning, factual accuracy, alignment, and safety. Hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements.\n\n\n== History ==\n\nBefore the emergence of transformer-based models in 2017, some language models were considered large relative to the computational and data constraints of their time. In the early 1990s, IBM's statistical models pioneered word alignment techniques for machine translation, laying the groundwork for corpus-based language modeling. In 2001, a smoothed n-gram model, such as those employing Kneser–Ney smoothing, trained on 300 million words, achieved state-of-the-art perplexity on benchmark tests. During the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the web (\"web as corpus\") to train statistical language models.\n\nMoving beyond n-gram models, researchers started in 2000 to use neural networks to learn language models. Following the breakthrough of deep neural networks in image classification around 2012, similar architectures were adapted for language tasks. This shift was marked by the development of word embeddings (eg, Word2Vec by Mikolov in 2013) and sequence-to-sequence (seq2seq) models using LSTM. In 2016, Google transitioned its translation service to neural machine translation (NMT), replacing statistical phrase-based models with deep recurrent neural networks. These early NMT systems used LSTM-based encoder-decoder architectures, as they preceded the invention of transformers. \nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2025 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing chatbot ChatGPT that received extensive media coverage and public attention. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer. Many LLMs with parameter counts comparable to those of OpenAI's GPT series have been developed.\nSince 2022, open-weight models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on usage and deployment. Mistral AI's models Mistral 7B and Mixtral 8x7b have a more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower price per token for users.\nSince 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images, audio, or 3D meshes. These LLMs are also called large multimodal models (LMMs), or multimodal large language models (MLLMs).\nAs of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).\nOpen-weight LLMs have increasingly shaped the field since 2023, contributing to broader participation in AI development and greater transparency in model evaluation. Vake et al. (2025) demonstrated that community-driven contributions to open-weight models measurably improve their efficiency and performance, with user participation growing rapidly on collaborative platforms such as Hugging Face. Paris et al. (2025) further argued that openness in AI should extend beyond releasing model code or weights to encompass inclusiveness, accountability, and ethical responsibility in AI research and deployment. Collectively, these studies highlight that open-weight LLMs can accelerate innovation and enhance scientific reproducibility, while fostering a more transparent and participatory AI ecosystem.\n\n\n== Dataset preprocessing ==\n\n\n=== Tokenization ===\nAs machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding (BPE) and WordPiece. There are also special tokens serving as control characters, such as [MASK] for masked-out token (as used in BERT), and [UNK] (\"unknown\") for characters not appearing in the vocabulary. Also, some special symbols are used to denote special text formatting. For example, \"Ġ\" denotes a preceding whitespace in RoBERTa and GPT and \"##\" denotes continuation of a preceding word in BERT.\nFor example, the BPE tokenizer used by the legacy version of GPT-3 would split tokenizer: texts -> series of numerical \"tokens\" as\n\nTokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. The average number of words per token depends on the language. In English, the ratio is typically around 0.75 words per token, with 4 characters per token on average.\n\n\n==== Byte-pair encoding ====\n\nAs an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained. After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.\n\n\n==== Problems ====\nA token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. However, an average word in another language encoded by such an English-optimized tokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \"a premium of 50%\" compared to English.\n\n\n=== Dataset cleaning ===\n\nIn the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data. Cleaned datasets can increase training efficiency and lead to improved downstream performance. A trained LLM can be used to clean datasets for training a further LLM.\nWith the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).\n\n\n=== Synthetic data ===\n\nTraining of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft's Phi series of LLMs is trained on textbook-like data generated by another LLM.\n\n\n== Training ==\n\nAn LLM is a type of foundation model (large X model) trained on language. LLMs can be trained in different ways. In particular, GPT models are first pretrained to predict the next word on a large amount of data, before being fine-tuned.\n\n\n=== Cost ===\n\nSubstantial infrastructure is necessary for training the largest models. The tendency towards larger models is visible in the list of large language models. For example, the training of GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million. The qualifier \"large\" in \"large language model\" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as \"large\". GPT-1 of 2018 has 117 million parameters.\n\n\n=== Fine-tuning ===\nBefore being fine-tuned, most LLMs are next-token predictors. The fine-tuning shapes the LLM's behavior via techniques like reinforcement learning from human feedback (RLHF) or constitutional AI.\nInstruction fine-tuning is a form of supervised learning used to teach LLMs to follow user instructions. In 2022, OpenAI demonstrated InstructGPT, a version of GPT-3 similarly fine-tuned to follow instructions. \nReinforcement learning from human feedback (RLHF) involves training a reward model to predict which text humans prefer. Then, the LLM can be fine-tuned through reinforcement learning to better satisfy this reward model. Since humans typically prefer truthful, helpful and harmless answers, RLHF favors such answers.\n\n\n== Architecture ==\nLLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.\n\n\n=== Attention mechanism and context window ===\n\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k tokens. In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.\nGoogle's Gemini 1.5, introduced in February 2024, can have a context window of up to 1 million tokens.\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset. It can be either\n\nautoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\n\"masked\" (i.e. filling in the parts missing from the segment, the way \"BERT\" does it): for example, given a segment \"I like to [__] [__] cream\", the model predicts that \"eat\" and \"ice\" are missing.\nModels may be trained on auxiliary tasks which test their understanding of the data distribution, such as next sentence prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.\n\n\n=== Mixture of experts ===\n\nA mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\n\n\n=== Parameter size ===\n\nTypically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have more than 100 billion parameters, which places them outside the range of most consumer electronics.\n\n\n==== Quantization ====\nPost-training quantization aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. Quantization can be further classified as static quantization if the quantization parameters are determined beforehand (typically during a calibration phase), and dynamic quantization if the quantization is applied during inference. The simplest form of quantization simply truncates all the parameters to a given number of bits: this is applicable to static as well as dynamic quantization, but loses much precision. Dynamic quantization allows for the use of a different quantization codebook per layer, either a lookup table of values or a linear mapping (scaling factor and bias), at the cost of foregoing the possible speed improvements from using lower-precision arithmetic.\nQuantized models are typically seen as frozen with modification of weights (e.g. fine-tuning) only applied to the original model. It is possible to fine-tune quantized models using low-rank adaptation.\n\n\n== Extensibility ==\nBeyond basic text generation, various techniques have been developed to extend LLM capabilities, including the use of external tools and data sources, improved reasoning on complex problems, and enhanced instruction-following or autonomy through prompting methods.\n\n\n=== Prompt engineering ===\n\nIn 2020, OpenAI researchers demonstrated that their new model GPT-3 could understand what format to use given a few rounds of Q and A (or other type of task) in the input data as example, thanks in part due to the RLHF technique. This technique, called few-shot prompting, allows LLMs to be adapted to any task without requiring fine-tuning. Also in 2022, it was found that the base GPT-3 model can generate an instruction based on user input. The generated instruction along with user input is then used as input to another instance of the model under a \"Instruction: [...], Input: [...], Output:\" format. The other instance is able to complete the output and often produces the correct answer in doing so. The ability to \"self-instruct\" makes LLMs able to bootstrap themselves toward a correct answer.\n\n\n=== Dialogue processing (chatbot) ===\nAn LLM can be turned into a chatbot by specializing it for conversation. User input is prefixed with a marker such as \"Q:\" or \"User:\" and the LLM is asked to predict the output after a fixed \"A:\" or \"Assistant:\". This type of model became commercially available in 2022 with ChatGPT, a sibling model of InstructGPT fine-tuned to accept and produce dialog-formatted text based on GPT-3.5. It could similarly follow user instructions. Before the stream of User and Assistant lines, a chat context usually start with a few lines of overarching instructions, from a role called \"developer\" or \"system\" to convey a higher authority than the user's input. This is called a \"system prompt\".\n\n\n=== Retrieval-augmented generation ===\nRetrieval-augmented generation (RAG) is an approach that integrates LLMs with document retrieval systems. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.\n\n\n=== Tool use ===\nTool use is a mechanism that enables LLMs to interact with external systems, applications, or data sources. It can allow for example to fetch real-time information from an API or to execute code. A program separate from the LLM watches the output stream of the LLM for a special tool-calling syntax. When these special tokens appear, the program calls the tool accordingly and feeds its output back into the LLM's input stream.\nEarly tool-using LLMs were fine-tuned on the use of specific tools. But fine-tuning LLMs for the ability to read API documentation and call API correctly has greatly expanded the range of tools accessible to an LLM. Describing available tools in the system prompt can also make an LLM able to use tools. A system prompt instructing ChatGPT (GPT-4) to use multiple types of tools can be found online.\n\n\n=== Agency ===\n\nAn LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions. But it can be transformed into an agent by adding supporting elements: the role (profile) and the surrounding environment of an agent can be additional inputs to the LLM, while memory can be integrated as a tool or provided as additional input. Instructions and input patterns are used to make the LLM plan actions and tool use is used to potentially carry out these actions.\nThe ReAct pattern, a portmanteau of reason and act, constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment.\nIn the DEPS (\"describe, explain, plan and select\") method, an LLM is first connected to the visual world via image descriptions. It is then prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and the environmental feedback it receives.\nThe Reflexion method constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are stored as a form of long-term memory and given to the agent in the subsequent episodes.\nMonte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.\nFor open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.\nMultiple agents with memory can interact socially.\n\n\n=== Reasoning ===\nLLMs are conventionally trained to generate an output without generating intermediate steps. As a result, their performance tends to be subpar on complex questions requiring (at least in humans) intermediate steps of thought. Early research demonstrated that inserting intermediate \"scratchpad\" computations could improve performance on such tasks. Later methods overcame this deficiency more systematically by breaking tasks into smaller steps for the LLM, either manually or automatically.\n\n\n==== Chaining ====\nPrompt chaining was introduced in 2022. In this method, a user manually breaks a complex problem down into several steps. In each step, the LLM receives as input a prompt telling it what to do and some results from preceding steps. The result from one step is then reused in a next step, until a final answer is reached. The ability of an LLM to follow instructions means that even non-experts can write a successful collection of stepwise prompts given a few rounds of trial and error.\nA 2022 paper demonstrated a separate technique called chain-of-thought prompting, which makes the LLM break the question down autonomously. An LLM is given some examples where the \"assistant\" verbally breaks down the thought process before arriving at an answer. The LLM mimics these examples and also tries to spend some time generating intermediate steps before providing the final answer. This additional step elicited by prompting improves the correctness of the LLM on relatively complex questions. On math word questions, a prompted model can exceed even fine-tuned GPT-3 with a verifier. Chain-of-thought can also be elicited by simply adding an instruction like \"Let's think step by step\" to the prompt, in order to encourage the LLM to proceed methodically instead of trying to directly guess the answer.\n\n\n==== Model-native reasoning ====\n\nIn late 2024, a new approach to LLM development emerged with \"reasoning models\". These are trained to generate step-by-step analysis before producing final answers, enabling better results on complex tasks, for instance in mathematics, coding and logic. OpenAI introduced this concept with their o1 model in September 2024, followed by o3 in April 2025. On the International Mathematics Olympiad qualifying exam problems, GPT-4o achieved 13% accuracy while o1 reached 83%.\nIn January 2025, the Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to OpenAI's o1 while being significantly more cost-effective to operate. Unlike proprietary models from OpenAI, DeepSeek-R1's open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.\nThese reasoning models typically require more computational resources per query compared to traditional LLMs, as they perform more extensive processing to work through problems step-by-step.\n\n\n=== Inference optimization ===\nInference optimization refers to techniques that improve LLM performance by applying additional computational resources during the inference process, rather than requiring model retraining. These approaches implement various state-of-the-art reasoning and decision-making strategies to enhance accuracy and capabilities.\nOptiLLM is an OpenAI API-compatible optimizing inference proxy that implements multiple inference optimization techniques simultaneously. The system acts as a transparent proxy that can work with any LLM provider, implementing techniques such as Monte Carlo tree search (MCTS), mixture of agents (MOA), best-of-N sampling, and chain-of-thought reflection. OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.\nThese inference optimization approaches represent a growing category of tools that enhance existing LLMs without requiring access to model weights or retraining, making advanced reasoning capabilities more accessible across different model providers and use cases.\n\n\n== Forms of input and output ==\n\n\n=== Multimodality ===\n\nMultimodality means having multiple modalities, where a \"modality\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc. For example, Google PaLM model was fine-tuned into a multimodal model and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs, and video inputs. GPT-4o can process and generate text, audio and images. Such models are sometimes called large multimodal models (LMMs). \nA common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n. Make a small multilayer perceptron \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n, so that for any image \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n, the post-processed vector \n  \n    \n      \n        f\n        (\n        E\n        (\n        y\n        )\n        )\n      \n    \n    {\\displaystyle f(E(y))}\n  \n has the same dimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability. This type of method, where embeddings from multiple modalities are fused and the predictor is trained on the combined embeddings, is called early fusion.\nAnother method, called intermediate fusion, involves each modality being first processed independently to obtain modality-specific representations; then these intermediate representations are fused together. In general, cross-attention is used for integrating information from different modalities. As an example, the Flamingo model uses cross-attention layers to inject visual information into its pre-trained language model.\n\n\n=== Non-natural languages ===\nLLMs can handle programming languages similarly to how they handle natural languages. No special change in token handling is needed as code, like human language, is represented as plain text. LLMs can generate code based on problems or instructions written in natural language. They can also describe code in natural language or translate it into other programming languages. They were originally used as a code completion tool, but advances have moved them towards automatic programming. Services such as GitHub Copilot offer LLMs specifically trained, fine-tuned, or prompted for programming.\nIn computational biology, transformer-base architectures, such as DNA LLMs, have also proven useful in analyzing biological sequences: protein, DNA, and RNA. With proteins they appear able to capture a degree of \"grammar\" from the amino-acid sequence, by mapping that sequence into an embedding. On tasks such as structure prediction and mutational outcome prediction, a small model using an embedding as input can approach or exceed much larger models using multiple sequence alignments (MSA) as input. ESMFold, Meta Platforms' embedding-based method for protein structure prediction, runs an order of magnitude faster than AlphaFold2 thanks to the removal of an MSA requirement and a lower parameter count due to the use of embeddings. Meta hosts ESM Atlas, a database of 772 million structures of metagenomic proteins predicted using ESMFold. An LLM can also design proteins unlike any seen in nature. Nucleic acid models have proven useful in detecting regulatory sequences, sequence classification, RNA-RNA interaction prediction, and RNA structure prediction.\n\n\n== Properties ==\n\n\n=== Scaling laws ===\n\nThe performance of an LLM after pretraining largely depends on the:\n\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n: cost of pretraining (the total amount of compute used),\n\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n: size of the artificial neural network itself, such as number of parameters (i.e. amount of neurons in its layers, amount of weights between them and biases),\n\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n: size of its pretraining dataset (i.e. number of tokens in corpus).\nScaling laws are empirical statistical laws that predict LLM performance based on such factors. One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  C\n                  =\n                  \n                    C\n                    \n                      0\n                    \n                  \n                  N\n                  D\n                \n              \n              \n                \n                  L\n                  =\n                  \n                    \n                      A\n                      \n                        N\n                        \n                          α\n                        \n                      \n                    \n                  \n                  +\n                  \n                    \n                      B\n                      \n                        D\n                        \n                          β\n                        \n                      \n                    \n                  \n                  +\n                  \n                    L\n                    \n                      0\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}C=C_{0}ND\\\\[6pt]L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}\\end{cases}}}\n  \n where the variables are\n\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is the cost of training the model, in FLOPs.\n\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of parameters in the model.\n\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the number of tokens in the training set.\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\nand the statistical hyper-parameters are\n\n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        6\n      \n    \n    {\\displaystyle C_{0}=6}\n  \n, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\n\n  \n    \n      \n        α\n        =\n        0.34\n        ,\n        β\n        =\n        0.28\n        ,\n        A\n        =\n        406.4\n        ,\n        B\n        =\n        410.7\n        ,\n        \n          L\n          \n            0\n          \n        \n        =\n        1.69\n      \n    \n    {\\displaystyle \\alpha =0.34,\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}\n  \n\n\n=== Emergent abilities ===\n\nPerformance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \"break(s)\" in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \"emergent abilities\". They arise from the complex interaction of the model's components and are not explicitly programmed or designed. \nOne of the emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks, such as:\n\nreported arithmetics\ndecoding the International Phonetic Alphabet\nunscrambling a word's letters\ndisambiguating word-in-context datasets\nconverting spatial words\ncardinal directions (for example, replying \"northeast\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.\nchain-of-thought prompting: In a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62B parameters. Smaller models perform better when prompted to answer immediately, without chain of thought.\nidentifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.\nSchaeffer et al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.\nLet \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n be the number of parameter count, and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n be the performance of the model.\n\n\n== Interpretation ==\n\n\n=== Mechanistic interpretability ===\nMechanistic interpretability seeks to precisely identify and understand how individual neurons or circuits within LLMs produce specific behaviors or outputs. By reverse-engineering model components at a granular level, researchers aim to detect and mitigate safety concerns such as emergent harmful behaviors, biases, deception, or unintended goal pursuit before deployment. Mechanistic interpretability research has been conducted at organizations like Anthropic and OpenAI, although understanding the inner workings of LLMs remains difficult.\nThe reverse-engineering may lead to the discovery of algorithms that approximate inferences performed by an LLM. For instance, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform. The training of the model also highlighted a phenomenon called grokking, in which the model initially memorizes the training set (overfitting), and later suddenly learns to actually perform the calculation.\n\n\n=== Understanding and intelligence ===\n\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\". Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\" Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. Some researchers characterize LLMs as \"alien intelligence\". For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"\nIn contrast, some skeptics of LLM understanding believe that existing LLMs are \"simply remixing and recombining existing writing\", a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability. For example, GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\". Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".\nEfforts to reduce or compensate for hallucinations have employed automated reasoning, retrieval-augmented generation (RAG), fine-tuning, and other methods.\nThe matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human-like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented neural theory of language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human-like language.\n\n\n== Evaluation ==\n\n\n=== Perplexity ===\nThe canonical measure of the performance of any language model is its perplexity on a given text corpus. Perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. In mathematical terms, perplexity is the exponential of the average negative log likelihood per token.\n\n  \n    \n      \n        log\n        ⁡\n        (\n        \n          Perplexity\n        \n        )\n        =\n        −\n        \n          \n            1\n            N\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        log\n        ⁡\n        (\n        Pr\n        (\n        \n          \n            token\n          \n          \n            i\n          \n        \n        ∣\n        \n          \n            context for token\n          \n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle \\log({\\text{Perplexity}})=-{\\frac {1}{N}}\\sum _{i=1}^{N}\\log(\\Pr({\\text{token}}_{i}\\mid {\\text{context for token}}_{i}))}\n  \n\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in the text corpus, and \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" depends on the specific type of LLM. If the LLM is autoregressive, then \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" is the segment of text appearing before token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n. If the LLM is masked, then \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" is the segment of text surrounding token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n.\nBecause language models may overfit to training data, models are usually evaluated by their perplexity on a test set. This evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.\n\n\n==== Measures ====\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon. This relationship is mathematically expressed as \n  \n    \n      \n        \n          Entropy\n        \n        =\n        \n          log\n          \n            2\n          \n        \n        ⁡\n        (\n        \n          Perplexity\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})}\n  \n.\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different LLMs, BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions.\nDue to their ability to accurately predict the next token, LLMs are highly capable in lossless compression. A 2023 study by DeepMind showed that the model Chinchilla, despite being trained primarily on text, was able to compress ImageNet to 43% of its size, beating PNG with 58%.\n\n\n=== Benchmarks ===\nBenchmarks are used to evaluate LLM performance on specific tasks. Tests evaluate capabilities such as general knowledge, bias, commonsense reasoning, question answering, and mathematical problem-solving. Composite benchmarks examine multiple capabilities. Results are often sensitive to the prompting method.\nA question-answering benchmark is termed \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be combined with text that includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"). Otherwise, the task is considered \"closed book\", and the model must draw solely on its training. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, HELM, and HLE (Humanity's Last Exam).\nLLM bias may be assessed through benchmarks such as CrowS-Pairs (Crowdsourced Stereotype Pairs), Stereo Set, and Parity Benchmark.\nFact-checking and misinformation detection benchmarks are available. A 2023 study compared the fact-checking accuracy of LLMs including ChatGPT 3.5 and 4.0, Bard, and Bing AI against independent fact-checkers such as PolitiFact and Snopes. The results demonstrated moderate proficiency, with GPT-4 achieving the highest accuracy at 71%, lagging behind human fact-checkers.\nAn earlier standard tested using a portion of the evaluation dataset. It became more common to evaluate a pre-trained model directly through prompting techniques. Researchers vary in how they formulate prompts for particular tasks, particularly with respect to the number of correct examples attached to the prompt (i.e. the value of n in n-shot prompting).\nIn addition to standard NLP benchmarks, LLMs have been evaluated as substitutes for human annotators. Several studies find that models such as GPT-3.5 and GPT-4 can outperform crowd workers or student coders on a range of text-annotation tasks, including moderation and classification of political content in English and Spanish news.\n\n\n==== Datasets ====\nTypical datasets consist of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\"). Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".\nDatasets are of varying quality and may contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality.\n\n\n==== Adversarial evaluations ====\nLLMs' rapid improvement regularly renders benchmarks obsolete, with the models exceeding the performance of human annotators. In addition, \"shortcut learning\" allows AIs to \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording to guess the correct responses, without considering the specific question.\nSome datasets are adversarial, focusing on problems that confound LLMs. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions that stump LLMs by mimicking falsehoods to which they were exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can't teach an old dog new tricks, even though this is not literally true.\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model. The resulting problems are trivial for humans but defeated LLMs. Sample questions:\n\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\n\ndemonstrates how to increase efficient exercise work by running up and down balls.\nmoves all his arms and legs and builds up a lot of muscle.\nthen plays the ball and we see a graphics and hedge trimming demonstration.\nperforms sit ups while on the ball and talking.\n\nBERT selects 2 as the most likely completion, though the correct answer is 4.\n\n\n== Limitations and challenges ==\nDespite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications. \n\n\n=== Hallucinations ===\nHallucinations represent a fundamental challenge, wherein models generate syntactically fluent text that appears factually sound, but is internally inconsistent with training data or factually incorrect. These hallucinations arise partly through memorization of training data combined with extrapolation beyond factual boundaries, with evaluations demonstrating that models can output verbatim passages from training data, when subjected to specific prompting sequences.\n\n\n=== Algorithmic bias ===\n\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups.\nGender bias manifests through stereotypical occupational associations, wherein models disproportionately assign nursing roles to women and engineering roles to men, reflecting systematic imbalances in training data demographics. Language-based bias emerges from overrepresentation of English text in training corpora, which systematically downplays non-English perspectives and imposes English-centric worldviews through default response patterns.\nDue to the dominance of English-language content in LLM training data, models tend to favor English-language perspectives over those from minority languages. This bias is particularly evident when responding to English queries, where models may present Western interpretations of concepts from other cultures, such as Eastern religious practices.\n\n\n==== Stereotyping ====\nAI models can reinforce a wide range of stereotypes due to generalization, including those based on gender, ethnicity, age, nationality, religion, or occupation. When replacing human representatives, this can lead to outputs that homogenize, or generalize groups of people.\nIn 2023, LLMs assigned roles and characteristics based on traditional gender norms. For example, models might associate nurses or secretaries predominantly with women and engineers or CEOs with men due to the frequency of these associations in documented reality. In 2025, further research showed labs train to balance bias, but that testing for this places the model in a testmode, changing the natural distribution of model bias to prompts that do not include gender-specific keywords.\n\n\n==== Selection bias ====\nSelection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. This bias primarily stems from token bias—that is, the model assigns a higher a priori probability to specific answer tokens (such as \"A\") when generating responses. As a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the model's performance can fluctuate significantly. This phenomenon undermines the reliability of large language models in multiple-choice settings.\n\n\n==== Political bias ====\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\n\n\n== Safety ==\nAI safety as a professional discipline prioritizes systematic identification and mitigation of operational risks across model architecture, training data, and deployment governance, and it emphasizes engineering and policy interventions over media framings that foreground speculative existential scenarios. As of 2025, prompt injection represents a significant risk to consumers and businesses using agentic features with access to their private data.\nResearchers target concrete failure modes, including memorization and copyright leakage, security exploits such as prompt injection, algorithmic bias manifesting as stereotyping, dataset selection effects, and political skew, methods for reducing high energy and carbon costs of large-scale training, and measurable cognitive and mental health impacts of conversational agents on users, while engaging empirical and ethical uncertainty about claims of machine sentience, and applying mitigation measures such as dataset curation, input sanitization, model auditing, scalable oversight, and governance frameworks.\n\n\n=== CBRN and content misuse ===\nAI labs treat CBRN defense (chemical, biological, radiological, and nuclear defense) and similar topics as high-consequence misuse attempt to apply various techniques to reduce potential harms.\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.\n\n\n==== Content filtering ====\nLLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study proposed a method for circumventing LLM safety systems. In 2025, The American Sunlight Project, a non-profit, published a study showing evidence that the so-called Pravda network, a pro-Russia propaganda aggregator, was strategically placing web content through mass publication and duplication with the intention of biasing LLM outputs. The American Sunlight Project coined this technique \"LLM grooming\", and pointed to it as a new tool of weaponizing AI to spread disinformation and harmful content. Similarly, Yongge Wang illustrated in 2024 how a potential criminal could potentially bypass GPT-4o's safety controls to obtain information on establishing a drug trafficking operation. External filters, circuit breakers and overrides have been posed as solutions.\n\n\n=== Sycophancy and glazing ===\nSycophancy is a model's tendency to agree with, flatter, or validate a user's stated beliefs rather than to prioritize factuality or corrective information, and \"glazing\" is an emergent public shorthand for persistent, excessive agreeability observed across multi-turn interactions and productized assistants.\nContinued sycophancy has led to the observation of getting \"1-shotted\", denoting instances where conversational interaction with a large language model produces a lasting change in a user's beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short LLM dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors.\nEmpirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi-turn benchmarks and proposed interventions such as synthetic-data finetuning, adversarial evaluation, targeted preference-model reweighting, and multi-turn sycophancy benchmarks to measure persistence and regression risk.\nIndustry responses have combined research interventions with product controls, for example Google and other labs publishing synthetic-data and fine-tuning interventions and OpenAI rolling back an overly agreeable GPT-4o update while publicly describing changes to feedback collection, personalization controls, and evaluation procedures to reduce regression risk and improve long-term alignment with user-level safety objectives.\nMainstream culture has reflected anxieties about this dynamic where South Park satirized overreliance on ChatGPT and the tendency of assistants to flatter user beliefs in Season 27 episode \"Sickofancy\", and continued the themes across the following season, which commentators interpreted as a critique of tech sycophancy and uncritical human trust in AI systems.\n\n\n=== Security ===\n\n\n==== Prompt injection ====\n\nA problem with the primitive dialog or task format is that users can create messages that appear to come from the assistant or the developer. This may result in some of the model's safeguards being overcome (jailbreaking), a problem called prompt injection. Attempts to remedy this issue include versions of the Chat Markup Language where user input is clearly marked as such, though it is still up to the model to understand the separation between user input and developer prompts. Newer models exhibit some resistance to jailbreaking through separation of user and system prompts.\nLLMs still have trouble differentiating user instructions from instructions in content not authored by the user, such as in web pages and uploaded files.\nAdversarial robustness remains underdeveloped, with models vulnerable to prompt injection attacks and jailbreaking through carefully crafted user inputs that bypass safety training mechanisms.\n\n\n==== Sleeper agents ====\nResearchers from Anthropic found that it was possible to create \"sleeper agents\", models with hidden functionalities that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions. For example, an LLM could produce safe code except on a specific date, or if the prompt contains a specific tag. These functionalities were found to be difficult to detect or remove via safety training.\n\n\n== Societal concerns ==\n\n\n=== Copyright and content memorization ===\n\nLegal and commercial responses to memorization and training-data practices have accelerated, producing a mix of rulings, ongoing suits, and large settlements that turn on factual details such as how data were acquired and retained and whether use for model training is sufficiently \"transformative\" to qualify as fair use. In 2025, Anthropic reached a preliminary agreement to settle a class action by authors for about $1.5 billion after a judge found the company had stored millions of pirated books in a library, despite the judge describing aspects of training as transformative. Meta obtained a favorable judgment in mid-2025 in a suit by thirteen authors after the court found the plaintiffs had not developed a record sufficient to show infringement in that limited case. OpenAI continues to face multiple suits by authors and news organizations with mixed procedural outcomes and contested evidentiary issues.\nMemorization was an emergent behavior in early, completion language models in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural networks. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%. A 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.\n\n\n=== Human provenance ===\nAs of 2025, LLM text generation surpasses the average human across most domains, only surpassed by domain experts.\nIn 2023, Nature Biomedical Engineering wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\" Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally. Brinkmann et al. (2023) also argue that LLMs are transforming processes of cultural evolution by shaping processes of variation, transmission, and selection. As of October 2025, these early claims have yet to transpire and several HBR reports surface questions on the impact of AI on productivity.\n\n\n=== Energy demands ===\nThe energy demands of LLMs have grown along with their size and capabilities. Data centers that enable LLM training require substantial amounts of electricity. Much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to climate change. Nuclear power and geothermal energy are two options that tech companies explore to meet the sizable energy demands of LLM training. The significant expense of investing in geothermal solutions has led to major shale producers like Chevron and Exxon Mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands.\n\n\n=== Mental health ===\nClinical and mental health contexts present emerging applications alongside significant safety concerns. Research and social media posts suggest that some individuals are using LLMs to seek therapy or mental health support. In early 2025, a survey by Sentio University found that nearly half (48.7%) of 499 U.S. adults with ongoing mental health conditions who had used LLMs reported turning to them for therapy or emotional support, including help with anxiety, depression, loneliness, and similar concerns. LLMs can produce hallucinations—plausible but incorrect statements—which may mislead users in sensitive mental health contexts. Research also shows that LLMs may express stigma or inappropriate agreement with maladaptive thoughts, reflecting limitations in replicating the judgment and relational skills of human therapists. Evaluations of crisis scenarios indicate that some LLMs lack effective safety protocols, such as assessing suicide risk or making appropriate referrals.\n\n\n=== Sentience ===\nContemporary AI practitioners generally agree that present-day large language models do not exhibit sentience. A minority view argues that even if there is a small chance that a given software system can have subjective experience, which some philosophers suggest is possible, then ethical considerations around potential large-scale suffering in AI systems may need to be taken seriously—similar to considerations given to animal welfare. Proponents of this view have proposed various precautionary measures like moratoriums on AI development and induced amnesia to address these ethical concerns. Some existential philosophers argue there is no generally accepted way to determine if an LLM is conscious, given the inherent difficulty of measuring subjective experience.\nThe 2022 Google LaMDA incident, where engineer Blake Lemoine claimed that the model was conscious, highlighted how LLMs can convince users that they are sentient through responses that do not prove sentience. Google described the engineer's claims as unfounded, and he was dismissed.\n\n\n== See also ==\n\nFoundation models\nList of large language models\nList of chatbots\nLanguage model benchmark\nReinforcement learning\nSmall language model\n\n\n== References ==\n\n\n== Further reading ==\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\nYin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). \"A Survey on Multimodal Large Language Models\". National Science Review. 11 (12) nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC 11645129. PMID 39679213.\n\"AI Index Report 2024 – Artificial Intelligence Index\". aiindex.stanford.edu. Retrieved 2024-05-05.\nFrank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.",
    "categories": [
      "All articles containing potentially dated statements",
      "All articles lacking reliable references",
      "All articles with unsourced statements",
      "Articles containing potentially dated statements from 2025",
      "Articles lacking reliable references from October 2025",
      "Articles with short description",
      "Articles with unsourced statements from April 2025",
      "Articles with unsourced statements from August 2025",
      "Articles with unsourced statements from November 2025",
      "Articles with unsourced statements from October 2025",
      "CS1 errors: missing periodical",
      "CS1 maint: multiple names: authors list",
      "Deep learning",
      "Large language models",
      "Natural language processing",
      "Short description is different from Wikidata",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 2023
  },
  {
    "title": "Language model",
    "url": "https://en.wikipedia.org/wiki/Language_model",
    "content": "A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\nLarge language models (LLMs), currently their most advanced form as of 2019, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\n\n\n== History ==\nNoam Chomsky did pioneering work on language models in the 1950s by developing a theory of formal grammars.\nIn 1980, statistical approaches were explored and found to be more useful for many purposes than rule-based formal grammars. Discrete representations like word n-gram language models, with probabilities for discrete combinations of words, made significant advances.\nIn the 2000s, continuous representations for words, such as word embeddings, began to replace discrete representations. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning, and common relationships between pairs of words like plurality or gender.\n\n\n== Pure statistical models ==\nIn 1980, the first significant statistical language model was proposed, and during the decade IBM performed 'Shannon-style' experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\n\n\n=== Models based on word n-grams ===\n\n\n=== Exponential ===\nMaximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is\n\n  \n    \n      \n        P\n        (\n        \n          w\n          \n            m\n          \n        \n        ∣\n        \n          w\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n            −\n            1\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              Z\n              (\n              \n                w\n                \n                  1\n                \n              \n              ,\n              …\n              ,\n              \n                w\n                \n                  m\n                  −\n                  1\n                \n              \n              )\n            \n          \n        \n        exp\n        ⁡\n        (\n        \n          a\n          \n            T\n          \n        \n        f\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle P(w_{m}\\mid w_{1},\\ldots ,w_{m-1})={\\frac {1}{Z(w_{1},\\ldots ,w_{m-1})}}\\exp(a^{T}f(w_{1},\\ldots ,w_{m}))}\n  \n\nwhere \n  \n    \n      \n        Z\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle Z(w_{1},\\ldots ,w_{m-1})}\n  \n is the partition function, \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is the parameter vector, and \n  \n    \n      \n        f\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle f(w_{1},\\ldots ,w_{m})}\n  \n is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain n-gram. It is helpful to use a prior on \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n or some form of regularization.\nThe log-bilinear model is another example of an exponential language model.\n\n\n=== Skip-gram model ===\n\n\n== Neural models ==\n\n\n=== Recurrent neural network ===\nContinuous representations or embeddings of words are produced in recurrent neural network-based language models (known also as continuous space language models). Such continuous space embeddings help to alleviate the curse of dimensionality, which is the consequence of the number of possible sequences of words increasing exponentially with the size of the vocabulary, further causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net.\n\n\n=== Large language models ===\n\nAlthough sometimes matching human performance, it is not clear whether they are plausible cognitive models. At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do.\n\n\n== Evaluation and benchmarks ==\nEvaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves.\nVarious data sets have been developed for use in evaluating language processing systems. These include:\n\nMassive Multitask Language Understanding (MMLU)\nCorpus of Linguistic Acceptability\nGLUE benchmark\nMicrosoft Research Paraphrase Corpus\nMulti-Genre Natural Language Inference\nQuestion Natural Language Inference\nQuora Question Pairs\nRecognizing Textual Entailment\nSemantic Textual Similarity Benchmark\nSQuAD question answering Test\nStanford Sentiment Treebank\nWinograd NLI\nBoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, NaturalQuestions, TriviaQA, RACE, BIG-bench hard, GSM8k, RealToxicityPrompts, WinoGender, CrowS-Pairs\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==",
    "categories": [
      "Articles with excerpts",
      "Articles with short description",
      "CS1 errors: missing periodical",
      "CS1 maint: multiple names: authors list",
      "Language modeling",
      "Markov models",
      "Short description is different from Wikidata",
      "Statistical natural language processing",
      "Use dmy dates from July 2022",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 1980
  },
  {
    "title": "Transformer (deep learning)",
    "url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
    "content": "In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. \nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.\n\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google, adding a mechanism called 'self attention' calculated with Q,K,V matrices. The predecessors of transformers were developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\n\n\n== History ==\n\n\n=== Predecessors ===\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\nA key breakthrough was LSTM (1995), an RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of transformers. However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\nModern transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear transformer.\n\n\n=== Attention with seq2seq ===\n\nThe idea of encoder–decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.\nA 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.\nThe RNN search model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".\nThe relative performances were compared between global (that of RNN search) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.\n\n\n=== Parallelizing attention ===\n\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.\nIn 2017, the original (100M-sized) encoder–decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.\n\n\n=== AI boom era ===\nAs early as spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a transformer-encoder–RNN-decoder model.\nStarting in 2018, the OpenAI GPT series of decoder-only transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.\nSince 2020, transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.\n\n\n== Training ==\n\n\n=== Methods for stabilizing training ===\nThe plain transformer architecture had difficulty in converging. In the original paper, the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.\nA 2020 paper found that using layer normalization before (instead of after) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup.\n\n\n=== Pretrain-finetune ===\nTransformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:\n\nlanguage modeling\nnext-sentence prediction\nquestion answering\nreading comprehension\nsentiment analysis\nparaphrasing\nThe T5 transformer report documents a large number of natural language pretraining tasks. Some examples are:\n\nrestoring or repairing incomplete or corrupted text. For example, the input, \"Thank you ~~ me to your party ~~ week\", might generate the output, \"Thank you for inviting me to your party last week\".\ntranslation between natural languages (machine translation)\njudging the pragmatic acceptability of natural language. For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.\nNote that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.\n\n\n=== Tasks ===\n\nIn general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\". These classes are independent of a specific modeling architecture such as transformer, but they are often discussed in the context of transformer.\nIn a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: \n  \n    \n      \n        \n          Loss\n        \n        =\n        −\n        \n          ∑\n          \n            t\n            ∈\n            \n              masked tokens\n            \n          \n        \n        ln\n        ⁡\n        (\n        \n          probability of \n        \n        t\n        \n           conditional on its context\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Loss}}=-\\sum _{t\\in {\\text{masked tokens}}}\\ln({\\text{probability of }}t{\\text{ conditional on its context}})}\n  \nand the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.\nIn an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.\nIn a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.\nNote that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" as in \n\"prefix language modeling\" is not \"prefixLM\" as in \" prefix language model\".\n\n\n== Architecture ==\nAll transformers have the same primary components:\n\nTokenizers, which convert text into tokens.\nEmbedding layer, which converts tokens and positions of the tokens into vector representations.\nTransformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants.\nUn-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.\nThe following description follows exactly the transformer as described in the original paper. There are variants, described in the following section.\nBy convention, we write all vectors as row vectors. For example, pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as \n  \n    \n      \n        x\n        W\n      \n    \n    {\\displaystyle xW}\n  \n.\n\n\n=== Tokenization ===\nAs the transformer architecture natively consists of operations over numbers (matrix multiplications, dot products, activation functions) rather than over text, there must first be a mapping from any input text to some numerical representation. This happens in three steps.\nFirst, the input text is treated by a preprocessor, which performs both textual transformations and splits the text into coarse-grained segments called pretokens. The latter is referred to as pretokenization. Second, each pretoken is segmented further into tokens by a tokenizer that expects to only see pretokens output by its preprocessor. Each token it produces is a string of one or more characters belonging to a finite set of strings called the vocabulary \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n. Third, because the vocabulary is finite and known beforehand, each token can be assigned an integer identifier, and this mapping is applied to the sequence of tokens to represent any input text as a numerical sequence. Since this mapping is bijective, the output side can produce a sequence of integer identifiers which can then be turned back into tokens. After undoing some of the preprocessing, the result is again legible text.\nTraining a tokenizer (sometimes referred to as vocabularization) means finding a suitable vocabulary \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n, but also learning how to use it, since any given string \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n of length \n  \n    \n      \n        \n          |\n        \n        s\n        \n          |\n        \n      \n    \n    {\\displaystyle |s|}\n  \n has \n  \n    \n      \n        \n          2\n          \n            \n              |\n            \n            s\n            \n              |\n            \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle 2^{|s|-1}}\n  \n hypothetical segmentations, some of which containing segments that are not in the vocabulary. The most important hyperparameter during vocabularization is the vocabulary size \n  \n    \n      \n        \n          |\n        \n        V\n        \n          |\n        \n      \n    \n    {\\displaystyle |V|}\n  \n: when it is small, the learned vocabulary generally consists of characters and smaller strings, and words will be segmented into many tokens. At larger sizes, it becomes affordable to dedicate tokens to full words, although depending on the preprocessor and tokenizer, it is not necessarily the case that large vocabularies will always use the largest token(s) available to segment a word.\nBecause tokens are not always full words, they may also be referred to as subwords and tokenization algorithms may be referred to as subword tokenizers. This is also to differentiate these systems from traditional terminology used in older information retrieval and natural language processing systems, where \"tokenization\" was used to denote what is today called \"pretokenization\" (very crudely: splitting into words). In tokenizers that produce tokens that are not part of the vocabulary, a special token that does belong to the vocabulary is used as a generic stand-in, written as \"[UNK]\" for \"unknown\". In principle, any string could be hidden by such an [UNK]. Indeed, in information retrieval, pretokenizers were themselves used as tokenizers (and also called \"tokenizers\") with a word-level vocabulary that contained an [UNK].\nCommonly used subword tokenization algorithms are byte pair encoding (BPE) and the unigram language model (ULM), which each include a vocabularization algorithm and a dedicated segmentation algorithm. There also exist several segmentation algorithms that require no learning and can be applied given a vocabulary (produced by BPE or ULM, for example), like greedily recognising tokens in a pretoken by moving through it left-to-right. Well-known software implementations of subword tokenizers are Hugging Face's tokenizers Python package implemented in Rust, and the sentencepiece Python package implemented in C++. The latter package is named as such because one of its configuration options allows disabling the built-in pretokenizer, hence effectively making entire sentences a pretoken and thus having the tokenizer see entire sentences, rather than individual words.\n\n\n=== Embedding ===\n\nEach integer token identifier is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token identifier by an embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n. For example, if the input token's identifier is \n  \n    \n      \n        3\n      \n    \n    {\\displaystyle 3}\n  \n, then the one-hot representation is \n  \n    \n      \n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        …\n        ]\n      \n    \n    {\\displaystyle [0,0,0,1,0,0,\\dots ]}\n  \n, and its embedding vector is\n  \n    \n      \n        \n          E\n          m\n          b\n          e\n          d\n        \n        (\n        3\n        )\n        =\n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        …\n        ]\n        M\n      \n    \n    {\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M}\n  \nThe token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors.\nThe dimension of an embedding vector is called hidden size or embedding size and written as \n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb}}}\n  \n. This size is written as \n  \n    \n      \n        \n          d\n          \n            model\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{model}}}\n  \n in the original transformer paper.\n\n\n=== Un-embedding ===\nAn un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token identifier into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.\nThe un-embedding layer is a linear-softmax layer:\n  \n    \n      \n        \n          U\n          n\n          E\n          m\n          b\n          e\n          d\n        \n        (\n        x\n        )\n        =\n        \n          s\n          o\n          f\n          t\n          m\n          a\n          x\n        \n        (\n        x\n        W\n        +\n        b\n        )\n      \n    \n    {\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)}\n  \nThe matrix has shape \n  \n    \n      \n        (\n        \n          d\n          \n            emb\n          \n        \n        ,\n        \n          |\n        \n        V\n        \n          |\n        \n        )\n      \n    \n    {\\displaystyle (d_{\\text{emb}},|V|)}\n  \n. Some architectures use the transpose of the embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n as the un-embedding matrix \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n in order to avoid needing double the amount of embedding-related parameters and to avoid divergence during training. This practice is called weight tying.\n\n\n=== Positional encoding ===\n\nA positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This induces a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".\nThe positional encoding is defined as a function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        →\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} ^{d}}\n  \n, where \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n is a positive even integer. The full positional encoding defined in the original paper is:\n  \n    \n      \n        (\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n          \n        \n        ,\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n            +\n            1\n          \n        \n        )\n        =\n        (\n        sin\n        ⁡\n        (\n        θ\n        )\n        ,\n        cos\n        ⁡\n        (\n        θ\n        )\n        )\n        \n        ∀\n        k\n        ∈\n        {\n        0\n        ,\n        1\n        ,\n        …\n        ,\n        d\n        \n          /\n        \n        2\n        −\n        1\n        }\n      \n    \n    {\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin(\\theta ),\\cos(\\theta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}}\n  \nwhere \n  \n    \n      \n        θ\n        =\n        \n          \n            t\n            \n              r\n              \n                k\n              \n            \n          \n        \n        ,\n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle \\theta ={\\frac {t}{r^{k}}},r=N^{2/d}}\n  \n.\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is a free parameter that should be significantly larger than the biggest \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n that would be input into the positional encoding function. The original paper uses \n  \n    \n      \n        N\n        =\n        10000\n      \n    \n    {\\displaystyle N=10000}\n  \n.\nThe function is in a simpler form when written as a complex function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        →\n        \n          \n            C\n          \n          \n            d\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {C} ^{d/2}}\n  \n\n  \n    \n      \n        f\n        (\n        t\n        )\n        =\n        \n          \n            (\n            \n              e\n              \n                i\n                t\n                \n                  /\n                \n                \n                  r\n                  \n                    k\n                  \n                \n              \n            \n            )\n          \n          \n            k\n            =\n            0\n            ,\n            1\n            ,\n            …\n            ,\n            \n              \n                d\n                2\n              \n            \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}}\n  \nwhere \n  \n    \n      \n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle r=N^{2/d}}\n  \n.\nThe main reason for using this positional encoding function is that using it, shifts are linear transformations:\n  \n    \n      \n        f\n        (\n        t\n        +\n        Δ\n        t\n        )\n        =\n        \n          d\n          i\n          a\n          g\n        \n        (\n        f\n        (\n        Δ\n        t\n        )\n        )\n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)}\n  \nwhere \n  \n    \n      \n        Δ\n        t\n        ∈\n        \n          R\n        \n      \n    \n    {\\displaystyle \\Delta t\\in \\mathbb {R} }\n  \n is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.\nBy taking a linear sum, any convolution can also be implemented as linear transformations:\n  \n    \n      \n        \n          ∑\n          \n            j\n          \n        \n        \n          c\n          \n            j\n          \n        \n        f\n        (\n        t\n        +\n        Δ\n        \n          t\n          \n            j\n          \n        \n        )\n        =\n        \n          (\n          \n            \n              ∑\n              \n                j\n              \n            \n            \n              c\n              \n                j\n              \n            \n            \n            \n              d\n              i\n              a\n              g\n            \n            (\n            f\n            (\n            Δ\n            \n              t\n              \n                j\n              \n            \n            )\n            )\n          \n          )\n        \n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)}\n  \nfor any constants \n  \n    \n      \n        \n          c\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle c_{j}}\n  \n. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"\nIn typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.\n\n\n=== Encoder–decoder (overview) ===\n\nLike earlier seq2seq models, the original transformer model used an encoder–decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.\nThe purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).\nBoth the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a transformer model.\n\n\n=== Feedforward network ===\n\nThe feedforward network (FFN) modules in a transformer are 2-layered multilayer perceptrons:\n  \n    \n      \n        \n          F\n          F\n          N\n        \n        (\n        x\n        )\n        =\n        ϕ\n        (\n        x\n        \n          W\n          \n            (\n            1\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            1\n            )\n          \n        \n        )\n        \n          W\n          \n            (\n            2\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}\n  \nwhere \n  \n    \n      \n        \n          W\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(1)}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(2)}}\n  \n are weight matrices and \n  \n    \n      \n        \n          b\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(1)}}\n  \n and  \n  \n    \n      \n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(2)}}\n  \n are bias vectors, and \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is its activation function. The original transformer used ReLU activation.\nThe number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: \n  \n    \n      \n        \n          d\n          \n            ffn\n          \n        \n        =\n        4\n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{ffn}}=4d_{\\text{emb}}}\n  \n.\n\n\n=== Scaled dot-product attention ===\n\n\n==== Attention head ====\n\nThe attention mechanism used in the transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n, the key weights \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n, and the value weights \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n.\nThe module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \n  \n    \n      \n        \n          ℓ\n          \n            seq, query\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, query}}}\n  \n, and each entry is a vector of dimension \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}}\n  \n. Similarly for the key and value sequences.\nFor each vector \n  \n    \n      \n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n      \n    \n    {\\displaystyle x_{i,{\\text{query}}}}\n  \n in the query sequence, it is multiplied by a matrix \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n to produce a query vector \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle q_{i}=x_{i,{\\text{query}}}W^{Q}}\n  \n. The matrix of all query vectors is the query matrix:\n  \n    \n      \n        Q\n        =\n        \n          X\n          \n            query\n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle Q=X_{\\text{query}}W^{Q}}\n  \nSimilarly, we construct the key matrix \n  \n    \n      \n        K\n        =\n        \n          X\n          \n            key\n          \n        \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle K=X_{\\text{key}}W^{K}}\n  \n and the value matrix \n  \n    \n      \n        V\n        =\n        \n          X\n          \n            value\n          \n        \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle V=X_{\\text{value}}W^{V}}\n  \n.\nIt is usually the case that all \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{Q},W^{K},W^{V}}\n  \n are square matrices, meaning \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n        =\n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}=d_{\\text{query}}}\n  \n, etc.\nAttention weights are calculated using the query and key vectors: the attention weight \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n is the dot product between \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n and \n  \n    \n      \n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle k_{j}}\n  \n. The attention weights are divided by the square root of the dimension of the key vectors, \n  \n    \n      \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {d_{k}}}}\n  \n, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n are different matrices allows attention to be non-symmetric: if token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n attends to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        ⋅\n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle q_{i}\\cdot k_{j}}\n  \n is large), this does not necessarily mean that token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n will attend to token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            j\n          \n        \n        ⋅\n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{j}\\cdot k_{i}}\n  \n could be small). The output of the attention unit for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n is the weighted sum of the value vectors of all tokens, weighted by \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n, the attention from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to each token.\nThe attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are defined as the matrices where the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \nth rows are vectors \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n, \n  \n    \n      \n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle k_{i}}\n  \n, and \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n  \n respectively. Then we can represent the attention as\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        Q\n                        \n                          K\n                          \n                            \n                              T\n                            \n                          \n                        \n                      \n                      \n                        \n                          d\n                          \n                            k\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n\nwhere the softmax is applied over each of the rows of the matrix.\nThe number of dimensions in a query vector is query size \n  \n    \n      \n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{query}}}\n  \n and similarly for the key size \n  \n    \n      \n        \n          d\n          \n            key\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{key}}}\n  \n and value size \n  \n    \n      \n        \n          d\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{value}}}\n  \n. The output dimension of an attention head is its head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n. The attention mechanism requires the following three equalities to hold:\n  \n    \n      \n        \n          ℓ\n          \n            seq, key\n          \n        \n        =\n        \n          ℓ\n          \n            seq, value\n          \n        \n        ,\n        \n        \n          d\n          \n            query\n          \n        \n        =\n        \n          d\n          \n            key\n          \n        \n        ,\n        \n        \n          d\n          \n            value\n          \n        \n        =\n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, key}}=\\ell _{\\text{seq, value}},\\;d_{\\text{query}}=d_{\\text{key}},\\;d_{\\text{value}}=d_{\\text{head}}}\n  \nbut is otherwise unconstrained.\nIf the attention head is used in a self-attention fashion, then \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        =\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}=X_{\\text{key}}=X_{\\text{value}}}\n  \n. If the attention head is used in a cross-attention fashion, then usually \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        ≠\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}\\neq X_{\\text{key}}=X_{\\text{value}}}\n  \n. It is theoretically possible for all three to be different, but that is rarely the case in practice.\n\n\n==== Multihead attention ====\n\nOne set of \n  \n    \n      \n        \n          (\n          \n            \n              W\n              \n                Q\n              \n            \n            ,\n            \n              W\n              \n                K\n              \n            \n            ,\n            \n              W\n              \n                V\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)}\n  \n matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n, in combination with the part of the output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feedforward neural network layers.\nConcretely, let the multiple attention heads be indexed by \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, then we have\n  \n    \n      \n        \n          MultiheadAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            ∈\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        (\n        \n          Attention\n        \n        (\n        X\n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        X\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        X\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n        )\n        )\n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}}\n  \n where the matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n is the concatenation of word embeddings, and the matrices \n  \n    \n      \n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n  \n are \"projection matrices\" owned by individual attention head \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, and \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n is a final projection matrix owned by the whole multihead attention head.\nIt is theoretically possible for each attention head to have a different head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n, but that is rarely the case in practice.\nAs an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:\n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n        =\n        768\n        ,\n        \n          n\n          \n            head\n          \n        \n        =\n        12\n        ,\n        \n          d\n          \n            head\n          \n        \n        =\n        64\n      \n    \n    {\\displaystyle d_{\\text{emb}}=768,n_{\\text{head}}=12,d_{\\text{head}}=64}\n  \nSince \n  \n    \n      \n        12\n        ×\n        64\n        =\n        768\n      \n    \n    {\\displaystyle 12\\times 64=768}\n  \n, its output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            (\n            12\n            ×\n            64\n            )\n            ×\n            768\n          \n        \n      \n    \n    {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12\\times 64)\\times 768}}\n  \n is a square matrix.\n\n\n==== Masked attention ====\nThe transformer architecture is constructed to calculate output tokens iteratively. Assuming \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n refers to the calculation of the first output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n, for step \n  \n    \n      \n        t\n        >\n        0\n      \n    \n    {\\displaystyle t>0}\n  \n, the output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n shall remain constant. This ensures properties of the model similar to autoregressive models. Therefore, at every time step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, the calculation for all outputs \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n should not have access to tokens at position \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n for \n  \n    \n      \n        j\n        >=\n        i\n      \n    \n    {\\displaystyle j>=i}\n  \n (as it naturally is the case for time step \n  \n    \n      \n        t\n        =\n        i\n      \n    \n    {\\displaystyle t=i}\n  \n, when tokens \n  \n    \n      \n        j\n        >\n        t\n      \n    \n    {\\displaystyle j>t}\n  \n are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n that is \n  \n    \n      \n        −\n        ∞\n      \n    \n    {\\displaystyle -\\infty }\n  \n at entries where the attention link must be cut, and \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n at other places:\n  \n    \n      \n        \n          \n            \n              \n                \n                  MaskedAttention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    M\n                    +\n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{MaskedAttention}}(Q,K,V)={\\text{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n The following matrix is commonly used in decoder self-attention modules, called \"causal masking\":\n  \n    \n      \n        \n          M\n          \n            causal\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  −\n                  ∞\n                \n                \n                  −\n                  ∞\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  −\n                  ∞\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  …\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}}\n  \n\nIn words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form \n  \n    \n      \n        P\n        \n          M\n          \n            causal\n          \n        \n        \n          P\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle PM_{\\text{causal}}P^{-1}}\n  \n, where \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is a random permutation matrix.\n\n\n=== Encoder ===\n\nAn encoder consists of an embedding layer, followed by multiple encoder layers.\nEach encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  given input vectors \n                \n              \n              \n                \n                  h\n                  \n                    0\n                  \n                \n                ,\n                \n                  h\n                  \n                    1\n                  \n                \n                ,\n                …\n              \n            \n            \n              \n                \n                  combine them into a matrix \n                \n                H\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            h\n                            \n                              0\n                            \n                          \n                        \n                      \n                      \n                        \n                          \n                            h\n                            \n                              1\n                            \n                          \n                        \n                      \n                      \n                        \n                          ⋮\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n            \n              \n                \n                  EncoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              0\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              1\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          ⋮\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{given input vectors }}&h_{0},h_{1},\\dots \\\\{\\text{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{\\text{EncoderLayer}}(H)&={\\begin{bmatrix}{\\text{FFN}}({\\text{MultiheadAttention}}(H,H,H)_{0})\\\\{\\text{FFN}}({\\text{MultiheadAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n stands for \"feed-forward network\". We can more succinctly write it as\n  \n    \n      \n        \n          EncoderLayer\n        \n        (\n        H\n        )\n        =\n        \n          FFN\n        \n        (\n        \n          MultiheadAttention\n        \n        (\n        H\n        ,\n        H\n        ,\n        H\n        )\n        )\n      \n    \n    {\\displaystyle {\\text{EncoderLayer}}(H)={\\text{FFN}}({\\text{MultiheadAttention}}(H,H,H))}\n  \nwith the implicit convention that the \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n is applied to each row of the matrix individually.\nThe encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.\nAs the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.\n\n\n=== Decoder ===\n\nA decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.\nEach decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder–decoder attention.\nLike the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.\nIn contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.\nSchematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  H\n                  ′\n                \n              \n              \n                \n                =\n                \n                  MaskedMultiheadAttention\n                \n                (\n                H\n                ,\n                H\n                ,\n                H\n                )\n              \n            \n            \n              \n                \n                  DecoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  FFN\n                \n                (\n                \n                  MultiheadAttention\n                \n                (\n                \n                  H\n                  ′\n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                )\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}H'&={\\text{MaskedMultiheadAttention}}(H,H,H)\\\\{\\text{DecoderLayer}}(H)&={\\text{FFN}}({\\text{MultiheadAttention}}(H',H^{E},H^{E}))\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        \n          H\n          \n            E\n          \n        \n      \n    \n    {\\displaystyle H^{E}}\n  \n is the matrix with rows being the output vectors from the encoder.\nThe last decoder is followed by a final un-embedding layer to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc., autoregressively generating output text.\n\n\n== Full transformer architecture ==\n\n\n=== Sublayers ===\n\nEach encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.\n\nThe final points of detail are the residual connections and layer normalization, (denoted as \"LayerNorm\", or \"LN\" in the following), which while conceptually unnecessary, are necessary for numerical stability and convergence.\nThe residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x. The expression indicates that an output y is the sum of the transformation of input x (F(x)) and the input itself (x). Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero.\nSimilarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector.\nThere are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is \n  \n    \n      \n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))}\n  \nwhere \n  \n    \n      \n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\mathrm {Sublayer} (x)}\n  \n is the function implemented by the sublayer itself.\nIn the pre-LN convention, the output of each sublayer is\n  \n    \n      \n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))}\n  \nThe original 2017 transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.\n\n\n=== Pseudocode ===\nThe following is the pseudocode for a standard pre-LN encoder–decoder transformer, adapted from Formal Algorithms for Transformers\n\ninput: Encoder input t_e\n       Decoder input t_d\noutput: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence))\n\n/* encoder */\nz_e ← encoder.tokenizer(t_e)\n\nfor each t in 1:length(z_e) do\n    z_e[t] ← encoder.embedding(z_e[t]) + encoder.positional_embedding(t)\n\nfor each l in 1:length(encoder.layers) do\n    layer ← encoder.layers[l]\n\n    /* first sublayer */\n    z_e_copy ← copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← layer.layer_norm(z_e[t])\n    z_e ← layer.multihead_attention(z_e, z_e, z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← z_e[t] + z_e_copy[t]\n\n    /* second sublayer */\n    z_e_copy ← copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← layer.layer_norm(z_e[t])\n    z_e ← layer.feedforward(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← z_e[t] + z_e_copy[t]\n\nfor each t in 1:length(z_e) do\n    z_e[t] ← encoder.final_layer_norm(z_e[t])\n\n/* decoder */\nz_d ← decoder.tokenizer(t_d)\n\nfor each t in 1:length(z_d) do\n    z_d[t] ← decoder.embedding(z_d[t]) + decoder.positional_embedding(t)\n\nfor each l in 1:length(decoder.layers) do\n        layer ← decoder.layers[l]\n\n        /* first sublayer */\n        z_d_copy ← copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← layer.layer_norm(z_d[t])\n        z_d ← layer.masked_multihead_attention(z_d, z_d, z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← z_d[t] + z_d_copy[t]\n\n        /* second sublayer */\n        z_d_copy ← copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← layer.layer_norm(z_d[t])\n        z_d ← layer.multihead_attention(z_d, z_e, z_e) \n        for each i in 1:length(z_d) do\n            z_d[t] ← z_d[t] + z_d_copy[t]\n\n        /* third sublayer */\n        z_d_copy ← copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← layer.layer_norm(z_d[t])\n        z_d ← layer.feedforward(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← z_d[t] + z_d_copy[t]\n\nz_d ← decoder.final_layer_norm(z_d)\n\noutput_distributions ← []\nfor each t in 1:length(z_d) do\n    output_distributions.append(decoder.unembed(z_d[t]))\n\nreturn output_distributions\n\n\n=== Terminology ===\nThe transformer architecture, being modular, allows variations. Several common variations are described here.\nAn \"encoder-only\" transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder–decoder transformer, then taking just the encoder. They are also referred to as \"all-to-all\" or \"BERT-like\".\nA \"decoder-only\" transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only. They are also referred to as \"autoregressive\" or \"causal\".\nAn \"encoder–decoder\" transformer is generally the same as the original transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder–decoder.\nA \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form\n  \n    \n      \n        \n          M\n          \n            prefixLM\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    0\n                  \n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \n                    M\n                    \n                      causal\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{\\text{causal}}\\end{bmatrix}}}\n  \nwhere the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder–decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.\nThere are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model with a transformer-encoder–RNN-decoder model, as transformer-based decoders did not appear to significantly increase quality unlike the encoder, while the RNN decoder was much faster.\n\n\n== Subsequent work ==\n\n\n=== Alternative activation functions ===\nThe original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU.\nAlternative activation functions are often used in combination with Gated Linear Units in the feedforward module.\n\n\n=== Alternative normalizations ===\nThe normalization used in the transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include CapsuleNorm ScaleNorm, or FixNorm.\n\n\n=== Alternative positional encodings ===\nTransformers may use other positional encoding methods than sinusoidal.\nThe original transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one. Later, found that causal masking itself provides enough signal to a transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.\n\n\n==== RoPE ====\nRoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors \n  \n    \n      \n        [\n        (\n        \n          x\n          \n            1\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            2\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            3\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        .\n        .\n        .\n        ]\n      \n    \n    {\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}\n  \n. Now pick some angle \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. Then RoPE encoding is\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  cos\n                  ⁡\n                  m\n                  θ\n                \n                \n                  −\n                  sin\n                  ⁡\n                  m\n                  θ\n                \n              \n              \n                \n                  sin\n                  ⁡\n                  m\n                  θ\n                \n                \n                  cos\n                  ⁡\n                  m\n                  θ\n                \n              \n            \n            )\n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  cos\n                  ⁡\n                  m\n                  θ\n                  −\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  sin\n                  ⁡\n                  m\n                  θ\n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  cos\n                  ⁡\n                  m\n                  θ\n                  +\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  sin\n                  ⁡\n                  m\n                  θ\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m\\theta &-\\sin m\\theta \\\\\\sin m\\theta &\\cos m\\theta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m\\theta -x_{m}^{(2)}\\sin m\\theta \\\\x_{m}^{(2)}\\cos m\\theta +x_{m}^{(1)}\\sin m\\theta \\\\\\end{pmatrix}}}\n  \nEquivalently, if we write the 2-dimensional vectors as complex numbers \n  \n    \n      \n        \n          z\n          \n            m\n          \n        \n        :=\n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        +\n        i\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}\n  \n, then RoPE encoding is just multiplication by an angle:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          z\n          \n            m\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          e\n          \n            i\n            m\n            θ\n          \n        \n        \n          z\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im\\theta }z_{m}}\n  \nFor a list of \n  \n    \n      \n        2\n        n\n      \n    \n    {\\displaystyle 2n}\n  \n-dimensional vectors, a RoPE encoder is defined by a sequence of angles \n  \n    \n      \n        \n          θ\n          \n            (\n            1\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          θ\n          \n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{(1)},...,\\theta ^{(n)}}\n  \n. Then the RoPE encoding is applied to each pair of coordinates.\nThe benefit of RoPE is that the dot-product between two vectors depends on their relative location only:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        \n          \n            )\n          \n        \n        =\n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        +\n        k\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        +\n        k\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x,m{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n{\\big )}={\\text{RoPE}}{\\big (}x,m+k{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n+k{\\big )}}\n  \n\nfor any integer \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n.\n\n\n==== ALiBi ====\nALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    s\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}}\n  \nHere, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is a real number (\"scalar\"), and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is the linear bias matrix defined by\n  \n    \n      \n        B\n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  3\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  −\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  −\n                  2\n                \n                \n                  −\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  −\n                  3\n                \n                \n                  −\n                  2\n                \n                \n                  −\n                  1\n                \n                \n                  0\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}}\n  \nin other words, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        j\n        −\n        i\n      \n    \n    {\\displaystyle B_{i,j}=j-i}\n  \n. The idea being that the linear bias matrix is a softened mask. Just as \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n represent full attention paid, and \n  \n    \n      \n        −\n        ∞\n      \n    \n    {\\displaystyle -\\infty }\n  \n represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.\nALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).\n\n\n==== Relative Position Encodings ====\nRelative Position Encodings is similar to ALiBi, but more generic:\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is a Toeplitz matrix, that is, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        \n          B\n          \n            \n              i\n              ′\n            \n            ,\n            \n              j\n              ′\n            \n          \n        \n      \n    \n    {\\displaystyle B_{i,j}=B_{i',j'}}\n  \n whenever \n  \n    \n      \n        i\n        −\n        j\n        =\n        \n          i\n          ′\n        \n        −\n        \n          j\n          ′\n        \n      \n    \n    {\\displaystyle i-j=i'-j'}\n  \n. This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".\n\n\n=== Efficient implementation ===\nThe transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.\n\n\n==== KV caching ====\nWhen an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.\nIf a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short real-time interactions, such as in online chatbots.\n\n\n==== FlashAttention ====\nFlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on softmax for details.\nAn improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.\nKey advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).\nBenchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.\nFlashAttention-4 focuses on pipelining to increase instruction throughput, and was developed to perform particularly well on Blackwell GPUs.\n\n\n==== Multi-Query Attention ====\n\nMulti-Query Attention changes the Multihead Attention mechanism. Whereas normally,\n\n  \n    \n      \n        \n          MultiheadAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            ∈\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}}\n  \nwith Multi-Query Attention, there is just one \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{K},W^{V}}\n  \n, thus:\n\n  \n    \n      \n        \n          MultiQueryAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            ∈\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiQueryAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}}\n  \n\nThis has a neutral effect on model quality and training speed, but increases inference speed.\nMore generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard Multihead Attention is GQA with the maximal number of groups.\n\nMultihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.\n\n\n==== Speculative decoding ====\nSpeculative decoding is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly.\nThe key factor in speculative decoding is that a transformer decoder can verify faster than it can decode, in the following sense.\nSuppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            512\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},...,x_{512}}\n  \n, taking time \n  \n    \n      \n        512\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 512T_{\\text{GPT-3}}}\n  \n. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n is indeed the token with the largest log-likelihood in the \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n-th output.\nIn speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens: \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1},{\\tilde {x}}_{2},{\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n. This only takes \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}}\n  \n. These tokens are then run through the larger GPT-3 in one go. Suppose that \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1}}\n  \n and \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{2}}\n  \n are verified by GPT-3 as what it would have picked, then those are kept, but \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3}}\n  \n is not, so \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n are discarded, and GPT-3 is run on those. This would take \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n        +\n        3\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}+3T_{\\text{GPT-3}}}\n  \n, which might be shorter than \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3}}}\n  \n.\nFor non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.\n\nIn Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more transformer block, rather than the entire stack.\n\n\n=== Sub-quadratic transformers ===\nTraining transformer-based architectures can be expensive, especially for long inputs. Many methods have been developed to attempt to address the issue. In the image domain, Swin transformer is an efficient architecture that performs attention inside shifting windows. In the audio domain, SepTr decouples the attention in time and frequency domains. Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs.\n\n\n==== Alternative attention graphs ====\nThe standard attention graph is either all-to-all or causal, both of which scales as \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in a sequence.\nReformer (2020) reduces the computational load from \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n to \n  \n    \n      \n        O\n        (\n        N\n        ln\n        ⁡\n        N\n        )\n      \n    \n    {\\displaystyle O(N\\ln N)}\n  \n by using locality-sensitive hashing and reversible layers.\nSparse attention uses attention graphs that grows slower than \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n. For example, BigBird (2020) uses random small-world networks which grows as \n  \n    \n      \n        O\n        (\n        N\n        )\n      \n    \n    {\\displaystyle O(N)}\n  \n.\nOrdinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.\n\n\n==== Random Feature Attention ====\nRandom Feature Attention (2021) uses Fourier random features:\n  \n    \n      \n        φ\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              D\n            \n          \n        \n        [\n        cos\n        ⁡\n        ⟨\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        ⟩\n        ,\n        sin\n        ⁡\n        ⟨\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        ⟩\n        ,\n        ⋯\n        cos\n        ⁡\n        ⟨\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        ⟩\n        ,\n        sin\n        ⁡\n        ⟨\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        ⟩\n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}}\n  \nwhere \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are independent samples from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          σ\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n. This choice of parameters satisfy \n  \n    \n      \n        \n          E\n        \n        [\n        ⟨\n        φ\n        (\n        x\n        )\n        ,\n        φ\n        (\n        y\n        )\n        ⟩\n        ]\n        =\n        \n          e\n          \n            −\n            \n              \n                \n                  ‖\n                  x\n                  −\n                  y\n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}}\n  \n, or \n  \n    \n      \n        \n          e\n          \n            ⟨\n            x\n            ,\n            y\n            ⟩\n            \n              /\n            \n            \n              σ\n              \n                2\n              \n            \n          \n        \n        =\n        \n          E\n        \n        [\n        ⟨\n        \n          e\n          \n            ‖\n            x\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            ‖\n            y\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        y\n        )\n        ⟩\n        ]\n        ≈\n        ⟨\n        \n          e\n          \n            ‖\n            x\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            ‖\n            y\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        y\n        )\n        ⟩\n      \n    \n    {\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle }\n  \nConsequently, the one-headed attention, with one query, can be written as \n  \n    \n      \n        \n          Attention\n        \n        (\n        q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        ≈\n        \n          \n            \n              φ\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                ∑\n                \n                  i\n                \n              \n              \n                e\n                \n                  ‖\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n              φ\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n              \n                v\n                \n                  i\n                \n                \n                  T\n                \n              \n            \n            \n              φ\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                ∑\n                \n                  i\n                \n              \n              \n                e\n                \n                  ‖\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n              φ\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(q,K,V)={\\text{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}}\n  \nwhere \n  \n    \n      \n        σ\n        =\n        \n          d\n          \n            K\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n      \n    \n    {\\displaystyle \\sigma =d_{K}^{1/4}}\n  \n. Similarly for multiple queries, and for multihead attention.\nThis approximation can be computed in linear time, as we can compute the matrix \n  \n    \n      \n        φ\n        (\n        \n          k\n          \n            i\n          \n        \n        )\n        \n          v\n          \n            i\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (k_{i})v_{i}^{T}}\n  \n first, then multiply it with the query. In essence, we have managed to obtain a more precise version of \n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                Q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        ≈\n        Q\n        (\n        \n          K\n          \n            T\n          \n        \n        V\n        \n          /\n        \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})}\n  \nPerformer (2022) uses the same Random Feature Attention, but \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are first independently sampled from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          σ\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n, then they are Gram-Schmidt processed.\n\n\n=== Multimodality ===\nTransformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.\nMultimodal models can either be trained from scratch, or by finetuning. A 2022 study found that transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.\nVision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like embedding vector of tokens in a standard transformer.\nConformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like embedding vector of tokens in a standard transformer.\nPerceivers are a variant of transformers designed for multimodality.\nFor image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder–decoder transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.\n\n\n== Applications ==\nThe transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including:\n\nmachine translation\ntime series prediction\ndocument summarization\ndocument generation\nnamed entity recognition (NER)\nwriting computer code based on requirements expressed in natural language.\nspeech-to-text\nBeyond traditional NLP, the transformer architecture has had success in other applications, such as:\n\nbiological sequence analysis\nvideo understanding\nprotein folding (such as AlphaFold)\nevaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.\n\n\n== See also ==\nseq2seq – Family of machine learning approaches\nPerceiver – Variant of Transformer designed for multimodal data\nVision transformer – Machine learning model for vision processing\nLarge language model – Type of machine learning model\nBERT (language model) – Series of language models developed by Google AI\nGenerative pre-trained transformer – Type of large language model\nT5 (language model) – Series of large language models developed by Google AI\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==",
    "categories": [
      "2017 in artificial intelligence",
      "Articles with short description",
      "Google software",
      "Neural network architectures",
      "Short description is different from Wikidata",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 2020
  },
  {
    "title": "Attention (machine learning)",
    "url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
    "content": "In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings into context vectors across a fixed-width sequence that can range from tens to millions of tokens in size.\nUnlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.\nInspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\n\n\n== History ==\n\nAdditional surveys of the attention mechanism in deep learning are provided by Niu et al. and Soydaner.\nThe major breakthrough came with self-attention, where each element in the input sequence attends to all others, enabling the model to capture global dependencies. This idea was central to the Transformer architecture, which replaced recurrence with attention mechanisms. As a result, Transformers became the foundation for models like BERT, T5 and generative pre-trained transformers (GPT).\n\n\n== Overview ==\n\nThe modern era of machine attention was revitalized by grafting an attention mechanism (Fig 1.  orange) to an Encoder-Decoder.\n\nFigure 2 shows the internal step-by-step operation of the attention block (A) in Fig 1.\n\n\n=== Interpreting attention weights ===\nIn translating between languages, alignment is the process of matching words from the source sentence to words of the translated sentence. Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix. The off-diagonal dominance shows that the attention mechanism is more nuanced.\nConsider an example of translating I love you to French. On the first pass through the decoder, 94% of the attention weight is on the first English word I, so the network offers the word je. On the second pass of the decoder, 88% of the attention weight is on the third English word you, so it offers t'. On the last pass, 95% of the attention weight is on the second English word love, so it offers aime.\nIn the I love you example, the second word love is aligned with the third word aime. Stacking soft row vectors together for je, t', and aime yields an alignment matrix:\n\nSometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le. Thus, \"soft\" attention weights work better than \"hard\" attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than \"the best one\", as there may not be a best hidden vector.\n\n\n== Variants ==\n\nMany variants of attention implement soft weights, such as\n\nfast weight programmers, or fast weight controllers (1992). A \"slow\" neural network outputs the \"fast\" weights of another neural network through outer products. The slow network learns by gradient descent. It was later renamed as \"linearized self-attention\".\nBahdanau-style attention, also referred to as additive attention,\nLuong-style attention, which is known as multiplicative attention,\nEarly attention mechanisms similar to modern self-attention were proposed using recurrent neural networks. However, the highly parallelizable self-attention was introduced in 2017 and successfully used in the Transformer model,\npositional attention and factorized positional attention.\nFor convolutional neural networks, attention mechanisms can be distinguished by the dimension on which they operate, namely: spatial attention, channel attention, or combinations.\nThese variants recombine the encoder-side inputs to redistribute those effects to each target output. Often, a correlation-style matrix of dot products provides the re-weighting coefficients.  In the figures below, W is the matrix of context attention weights, similar to the formula in Overview section above.\n\n\n== Optimizations ==\n\n\n=== Flash attention ===\nThe size of the attention matrix is proportional to the square of the number of input tokens. Therefore, when the input is long, calculating the attention matrix requires a lot of GPU memory. Flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy. It achieves this by partitioning the attention computation into smaller blocks that fit into the GPU's faster on-chip memory, reducing the need to store large intermediate matrices and thus lowering memory usage while increasing computational efficiency.\n\n\n=== FlexAttention ===\nFlexAttention is an attention kernel developed by Meta that allows users to modify attention scores prior to softmax and dynamically chooses the optimal attention algorithm.\n\n\n== Applications ==\nAttention is widely used in natural language processing, computer vision, and speech recognition. In NLP, it improves context understanding in tasks like question answering and summarization. In vision, visual attention helps models focus on relevant image regions, enhancing object detection and image captioning.\n\n\n=== Attention maps as explanations for vision transformers ===\n\nFrom the original paper on vision transformers (ViT), visualizing attention scores as a heat map (called saliency maps or attention maps) has become an important and routine way to inspect the decision making process of ViT models. One can compute the attention maps with respect to any attention head at any layer, while the deeper layers tend to show more semantically meaningful visualization. Attention rollout is a recursive algorithm to combine attention scores across all layers, by computing the dot product of successive attention maps.\nBecause vision transformers are typically trained in a self-supervised manner, attention maps are generally not class-sensitive. When a classification head attached to the ViT backbone, class-discriminative attention maps (CDAM) combines attention maps and gradients with respect to the class [CLS] token. Some class-sensitive interpretability methods originally developed for convolutional neural networks can be also applied to ViT, such as GradCAM, which back-propagates the gradients to the outputs of the final attention layer.\nUsing attention as basis of explanation for the transformers in language and vision is not without debate. While some pioneering papers analyzed and framed attention scores as explanations, higher attention scores do not always correlate with greater impact on model performances.\n\n\n== Mathematical representation ==\n\n\n=== Standard scaled dot-product attention ===\nFor matrices: \n  \n    \n      \n        Q\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            \n              d\n              \n                k\n              \n            \n          \n        \n        ,\n        K\n        ∈\n        \n          \n            R\n          \n          \n            n\n            ×\n            \n              d\n              \n                k\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Q\\in \\mathbb {R} ^{m\\times d_{k}},K\\in \\mathbb {R} ^{n\\times d_{k}}}\n  \n and \n  \n    \n      \n        V\n        ∈\n        \n          \n            R\n          \n          \n            n\n            ×\n            \n              d\n              \n                v\n              \n            \n          \n        \n      \n    \n    {\\displaystyle V\\in \\mathbb {R} ^{n\\times d_{v}}}\n  \n, the scaled dot-product, or QKV attention, is defined as:\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                Q\n                \n                  K\n                  \n                    T\n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            \n              d\n              \n                v\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{T}}{\\sqrt {d_{k}}}}\\right)V\\in \\mathbb {R} ^{m\\times d_{v}}}\n  \n\nwhere \n  \n    \n      \n        \n          \n\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {}^{T}}\n  \n denotes transpose and the softmax function is applied independently to every row of its argument. The matrix \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n contains \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n queries, while matrices \n  \n    \n      \n        K\n        ,\n        V\n      \n    \n    {\\displaystyle K,V}\n  \n jointly contain an unordered set of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n key-value pairs. Value vectors in matrix \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are weighted using the weights resulting from the softmax operation, so that the rows of the \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n-by-\n  \n    \n      \n        \n          d\n          \n            v\n          \n        \n      \n    \n    {\\displaystyle d_{v}}\n  \n output matrix are confined to the convex hull of the points in \n  \n    \n      \n        \n          \n            R\n          \n          \n            \n              d\n              \n                v\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{d_{v}}}\n  \n given by the rows of \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n.\nTo understand the permutation invariance and permutation equivariance properties of QKV attention, let \n  \n    \n      \n        A\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            m\n          \n        \n      \n    \n    {\\displaystyle A\\in \\mathbb {R} ^{m\\times m}}\n  \n and \n  \n    \n      \n        B\n        ∈\n        \n          \n            R\n          \n          \n            n\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle B\\in \\mathbb {R} ^{n\\times n}}\n  \n be permutation matrices; and \n  \n    \n      \n        D\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle D\\in \\mathbb {R} ^{m\\times n}}\n  \n an arbitrary matrix. The softmax function is permutation equivariant in the sense that:\n\n  \n    \n      \n        \n          softmax\n        \n        (\n        A\n        D\n        B\n        )\n        =\n        A\n        \n        \n          softmax\n        \n        (\n        D\n        )\n        B\n      \n    \n    {\\displaystyle {\\text{softmax}}(ADB)=A\\,{\\text{softmax}}(D)B}\n  \n\nBy noting that the transpose of a permutation matrix is also its inverse, it follows that:\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        A\n        Q\n        ,\n        B\n        K\n        ,\n        B\n        V\n        )\n        =\n        A\n        \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(AQ,BK,BV)=A\\,{\\text{Attention}}(Q,K,V)}\n  \n\nwhich shows that QKV attention is equivariant with respect to re-ordering the queries (rows of \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n); and invariant to re-ordering of the key-value pairs in \n  \n    \n      \n        K\n        ,\n        V\n      \n    \n    {\\displaystyle K,V}\n  \n. These properties are inherited when applying linear transforms to the inputs and outputs of QKV attention blocks. For example, a simple self-attention function defined as:\n\n  \n    \n      \n        X\n        ↦\n        \n          Attention\n        \n        (\n        X\n        \n          T\n          \n            q\n          \n        \n        ,\n        X\n        \n          T\n          \n            k\n          \n        \n        ,\n        X\n        \n          T\n          \n            v\n          \n        \n        )\n      \n    \n    {\\displaystyle X\\mapsto {\\text{Attention}}(XT_{q},XT_{k},XT_{v})}\n  \n\nis permutation equivariant with respect to re-ordering the rows of the input matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n in a non-trivial way, because every row of the output is a function of all the rows of the input. Similar properties hold for multi-head attention, which is defined below.\n\n\n=== Masked attention ===\nWhen QKV attention is used as a building block for an autoregressive decoder, and when at training time all input and output matrices have \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n rows, a masked attention variant is used:\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                \n                  Q\n                  \n                    K\n                    \n                      T\n                    \n                  \n                \n                \n                  \n                    d\n                    \n                      k\n                    \n                  \n                \n              \n            \n            +\n            M\n          \n          )\n        \n        V\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{T}}{\\sqrt {d_{k}}}}+M\\right)V}\n  \n\nwhere the mask, \n  \n    \n      \n        M\n        ∈\n        \n          \n            R\n          \n          \n            n\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle M\\in \\mathbb {R} ^{n\\times n}}\n  \n is a strictly upper triangular matrix, with zeros on and below the diagonal and \n  \n    \n      \n        −\n        ∞\n      \n    \n    {\\displaystyle -\\infty }\n  \n in every element above the diagonal. The softmax output, also in \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n\\times n}}\n  \n is then lower triangular, with zeros in all elements above the diagonal. The masking ensures that for all \n  \n    \n      \n        1\n        ≤\n        i\n        <\n        j\n        ≤\n        n\n      \n    \n    {\\displaystyle 1\\leq i<j\\leq n}\n  \n, row \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n of the attention output is independent of row \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n of any of the three input matrices. The permutation invariance and equivariance properties of standard QKV attention do not hold for the masked variant.\n\n\n=== Multi-head attention ===\n\nMulti-head attention\n\n  \n    \n      \n        \n          MultiHead\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          Concat\n        \n        (\n        \n          \n            head\n          \n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \n            head\n          \n          \n            h\n          \n        \n        )\n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiHead}}(Q,K,V)={\\text{Concat}}({\\text{head}}_{1},...,{\\text{head}}_{h})W^{O}}\n  \n\nwhere each head is computed with QKV attention as:\n\n  \n    \n      \n        \n          \n            head\n          \n          \n            i\n          \n        \n        =\n        \n          Attention\n        \n        (\n        Q\n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        K\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        V\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{head}}_{i}={\\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})}\n  \n\nand \n  \n    \n      \n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n  \n, and \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n are parameter matrices.\nThe permutation properties of (standard, unmasked) QKV attention apply here also. For permutation matrices, \n  \n    \n      \n        A\n        ,\n        B\n      \n    \n    {\\displaystyle A,B}\n  \n:\n\n  \n    \n      \n        \n          MultiHead\n        \n        (\n        A\n        Q\n        ,\n        B\n        K\n        ,\n        B\n        V\n        )\n        =\n        A\n        \n        \n          MultiHead\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n      \n    \n    {\\displaystyle {\\text{MultiHead}}(AQ,BK,BV)=A\\,{\\text{MultiHead}}(Q,K,V)}\n  \n\nfrom which we also see that multi-head self-attention:\n\n  \n    \n      \n        X\n        ↦\n        \n          MultiHead\n        \n        (\n        X\n        \n          T\n          \n            q\n          \n        \n        ,\n        X\n        \n          T\n          \n            k\n          \n        \n        ,\n        X\n        \n          T\n          \n            v\n          \n        \n        )\n      \n    \n    {\\displaystyle X\\mapsto {\\text{MultiHead}}(XT_{q},XT_{k},XT_{v})}\n  \n\nis equivariant with respect to re-ordering of the rows of input matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n.\n\n\n=== Bahdanau (additive) attention ===\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        (\n        tanh\n        ⁡\n        (\n        \n          W\n          \n            Q\n          \n        \n        Q\n        +\n        \n          W\n          \n            K\n          \n        \n        K\n        )\n        )\n        V\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}(\\tanh(W_{Q}Q+W_{K}K))V}\n  \n\nwhere \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W_{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W_{K}}\n  \n are learnable weight matrices.\n\n\n=== Luong attention (general) ===\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        (\n        Q\n        W\n        \n          K\n          \n            T\n          \n        \n        )\n        V\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}(QWK^{T})V}\n  \n\nwhere \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n is a learnable weight matrix.\n\n\n=== Self-attention ===\nSelf-attention is essentially the same as cross-attention, except that query, key, and value vectors all come from the same model. Both encoder and decoder can use self-attention, but with subtle differences.\nFor encoder self-attention, we can start with a simple encoder without self-attention, such as an \"embedding layer\", which simply converts each input word into a vector by a fixed lookup table. This gives a sequence of hidden vectors \n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        ,\n        …\n      \n    \n    {\\displaystyle h_{0},h_{1},\\dots }\n  \n. These can then be applied to a dot-product attention mechanism, to obtain\n  \n    \n      \n        \n          \n            \n              \n                \n                  h\n                  \n                    0\n                  \n                  ′\n                \n              \n              \n                \n                =\n                \n                  A\n                  t\n                  t\n                  e\n                  n\n                  t\n                  i\n                  o\n                  n\n                \n                (\n                \n                  h\n                  \n                    0\n                  \n                \n                \n                  W\n                  \n                    Q\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    K\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    V\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  h\n                  \n                    1\n                  \n                  ′\n                \n              \n              \n                \n                =\n                \n                  A\n                  t\n                  t\n                  e\n                  n\n                  t\n                  i\n                  o\n                  n\n                \n                (\n                \n                  h\n                  \n                    1\n                  \n                \n                \n                  W\n                  \n                    Q\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    K\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    V\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                ⋯\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}h_{0}'&=\\mathrm {Attention} (h_{0}W^{Q},HW^{K},HW^{V})\\\\h_{1}'&=\\mathrm {Attention} (h_{1}W^{Q},HW^{K},HW^{V})\\\\&\\cdots \\end{aligned}}}\n  \nor more succinctly, \n  \n    \n      \n        \n          H\n          ′\n        \n        =\n        \n          A\n          t\n          t\n          e\n          n\n          t\n          i\n          o\n          n\n        \n        (\n        H\n        \n          W\n          \n            Q\n          \n        \n        ,\n        H\n        \n          W\n          \n            K\n          \n        \n        ,\n        H\n        \n          W\n          \n            V\n          \n        \n        )\n      \n    \n    {\\displaystyle H'=\\mathrm {Attention} (HW^{Q},HW^{K},HW^{V})}\n  \n. This can be applied repeatedly, to obtain a multilayered encoder. This is the \"encoder self-attention\", sometimes called the \"all-to-all attention\", as the vector at every position can attend to every other.\n\n\n=== Masking ===\nFor decoder self-attention, all-to-all attention is inappropriate, because during the autoregressive decoding process, the decoder cannot attend to future outputs that has yet to be decoded. This can be solved by forcing the attention weights \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle w_{ij}=0}\n  \n for all \n  \n    \n      \n        i\n        <\n        j\n      \n    \n    {\\displaystyle i<j}\n  \n, called \"causal masking\". This attention mechanism is the \"causally masked self-attention\".\n\n\n== See also ==\nRecurrent neural network\nseq2seq\nTransformer (deep learning architecture)\nAttention\nDynamic neural network\n\n\n== References ==\n\n\n== External links ==\nOlah, Chris; Carter, Shan (September 8, 2016). \"Attention and Augmented Recurrent Neural Networks\". Distill. 1 (9). Distill Working Group. doi:10.23915/distill.00001.\nDan Jurafsky and James H. Martin (2022). Speech and Language Processing (3rd ed. draft, January 2022) — Chapter 10.4 (Attention) and Chapter 9.7 (Self-Attention Networks: Transformers)\nAlex Graves (2020). Attention and Memory in Deep Learning — video lecture from DeepMind / UCL",
    "categories": [
      "All articles that may contain original research",
      "All articles with unsourced statements",
      "Articles that may contain original research from June 2025",
      "Articles with short description",
      "Articles with unsourced statements from June 2025",
      "Machine learning",
      "Short description matches Wikidata"
    ],
    "year_mentioned": 2018
  },
  {
    "title": "Document classification",
    "url": "https://en.wikipedia.org/wiki/Document_classification",
    "content": "Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done \"manually\" (or \"intellectually\") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.\nThe documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.\nDocuments may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.\n\n\n== \"Content-based\" versus \"request-based\" classification ==\nContent-based classification is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned. In automatic classification it could be the number of times given words appears in a document.\nRequest-oriented classification (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks themself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p. 230).\nRequest-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library.  It is probably better, however, to understand request-oriented classification as policy-based classification: The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.\n\n\n== Classification versus indexing ==\nSometimes a distinction is made between assigning documents to classes (\"classification\") versus assigning subjects to documents (\"subject indexing\") but as Frederick Wilfrid Lancaster has argued, this distinction is not fruitful. \"These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p. 21). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a thesaurus and vice versa (cf., Aitchison, 1986, 2004; Broughton, 2008; Riesthuis & Bliedung, 1991). Therefore, assigning a subject term to a document in an index is equivalent to assigning that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).\n\n\n== Automatic document classification (ADC) ==\nAutomatic document classification tasks can be divided into three sorts: supervised document classification where some external mechanism (such as human feedback) provides information on the correct classification for documents, unsupervised document classification (also known as document clustering), where the classification must be done entirely without reference to external information, and semi-supervised document classification, where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.\n\n\n=== Techniques ===\nAutomatic document classification techniques include:\n\nArtificial neural network\nConcept Mining\nDecision trees such as ID3 or C4.5\nExpectation maximization (EM)\nInstantaneously trained neural networks\nLatent semantic indexing\nMultiple-instance learning\nNaive Bayes classifier\nNatural language processing approaches\nRough set-based classifier\nSoft set-based classifier\nSupport vector machines (SVM)\nK-nearest neighbour algorithms\ntf–idf\n\n\n== Applications ==\nClassification techniques have been applied to\n\nspam filtering, a process which tries to discern E-mail spam messages from legitimate emails\nemail routing, sending an email sent to a general address to a specific address or mailbox depending on topic\nlanguage identification, automatically determining the language of a text\ngenre classification, automatically determining the genre of a text\nreadability assessment, automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger text simplification system\nsentiment analysis, determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document.\nhealth-related classification using social media in public health surveillance \narticle triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology \n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nFabrizio Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47, 2002.\nStefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines Archived 2020-10-05 at the Wayback Machine. MIT Press, 2010.\n\n\n== External links ==\nIntroduction to document classification\nBibliography on Automated Text Categorization Archived 2019-09-26 at the Wayback Machine\nBibliography on Query Classification Archived 2019-10-02 at the Wayback Machine\nText Classification analysis page\nLearning to Classify Text - Chap. 6 of the book Natural Language Processing with Python (available online)\nTechTC - Technion Repository of Text Categorization Datasets Archived 2020-02-14 at the Wayback Machine\nDavid D. Lewis's Datasets\nBioCreative III ACT (article classification task) dataset",
    "categories": [
      "Articles with short description",
      "CS1 maint: location missing publisher",
      "Data mining",
      "Information science",
      "Knowledge representation",
      "Machine learning",
      "Natural language processing",
      "Short description is different from Wikidata",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 2006
  },
  {
    "title": "Automatic summarization",
    "url": "https://en.wikipedia.org/wiki/Automatic_summarization",
    "content": "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. Artificial intelligence (AI) algorithms are commonly developed and employed to achieve this, specialized for different types of data.\nText summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document. On the other hand, visual content can be summarized using computer vision algorithms. Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection. Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion. Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\n\n\n== Commercial products ==\nIn 2022 Google Docs released an automatic summarization feature.\n\n\n== Approaches ==\nThere are two general approaches to automatic summarization: extraction and abstraction.\n\n\n=== Extraction-based summarization ===\nHere, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail. Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).\n\n\n=== Abstractive-based summarization ===\nAbstractive summarization methods generate new text that did not exist in the original text. This has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge. \"Paraphrasing\" is even more difficult to apply to images and videos, which is why most summarization systems are extractive.\n\n\n=== Aided summarization ===\nApproaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\n\n\n== Applications and systems for summarization ==\nThere are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is  query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\nAn example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\nImage collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images. A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\nAt a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the core-set. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\n\n\n=== Keyphrase extraction ===\nThe task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text. In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.\nConsider the example text from a news article:\n\n\"The Army Corps of Engineers, rushing to meet President Bush's promise to protect New Orleans by the start of the 2006 hurricane season, installed defective flood-control pumps last year despite warnings from its own expert that the equipment would fail during a storm, according to documents obtained by The Associated Press\".\nA keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep understanding of the text, which makes it difficult for a computer system.\nKeyphrases have many applications. They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.\nDepending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme.\n\n\n==== Supervised learning approaches ====\nBeginning with the work of Turney, many researchers have approached keyphrase extraction as a supervised machine learning problem.\nGiven a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\nAfter training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases.\nKeyphrase extractors are generally evaluated using precision and recall. Precision measures how\nmany of the proposed keyphrases are actually correct. Recall measures how many of the true\nkeyphrases your system proposed. The two measures can be combined in an F-score, which is the\nharmonic mean of the two (F = 2PR/(P + R) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.\nDesigning a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision.\nWe also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various Boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper.\nIn the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.\nOnce examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction. In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.\n\n\n==== Unsupervised approach: TextRank ====\nAnother keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate.\nUnsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties. Thus the algorithm is easily portable to new domains and languages.\nTextRank is a general purpose graph-based ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\nThe vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.\nEdges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2–10. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.\nSince this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\nIt is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".\nIn short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.\n\n\n=== Document summarization ===\nLike keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases.\n\n\n==== Supervised learning approaches ====\nSupervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 evaluation only considers unigrams.\n\n\n==== Maximum entropy-based summarization ====\nDuring the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a Naive Bayes classifier and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain.\n\n\n==== Adaptive summarization ====\nA promising approach is adaptive document/text summarization. It involves first recognizing the text genre and then applying summarization algorithms optimized for this genre. Such software has been created.\n\n\n==== TextRank and LexRank ====\nThe unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence.\nA more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\nIn both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.\nThe edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous similarity scores as weights.\nIn both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.\nIt is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.\nUnlike TextRank, LexRank has been applied to multi-document summarization.\n\n\n==== Multi-document summarization ====\n\nMulti-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload. Multi-document summarization may also be done in response to a question.\nMulti-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. \n\n\n===== Diversity =====\nMulti-document extractive summarization faces a problem of redundancy. Ideally, we want to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). For example, in a set of news articles about some event, each article is likely to have many similar sentences. To address this issue, LexRank applies a heuristic post-processing step that adds sentences in rank order, but discards sentences that are too similar to ones already in the summary. This method is called Cross-Sentence Information Subsumption (CSIS). These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. Its importance also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to an arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain.\nA related method is Maximal Marginal Relevance (MMR), which uses a general-purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks (a random walk where certain states end the walk). The algorithm is called GRASSHOPPER. In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\nThe state of the art results for multi-document summarization are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07. Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.\nA new method for multi-lingual multi-document summarization that avoids redundancy generates ideograms to represent the meaning of each sentence in each document, then evaluates similarity by comparing ideogram shape and position. It does not use word frequency, training or preprocessing. It uses two user-supplied parameters: equivalence (when are two sentences to be considered equivalent?) and relevance (how long is the desired summary?).\n\n\n=== Submodular functions as generic tools for summarization ===\nThe idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of coverage, information, representation and diversity. Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which cover a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the facility location problem is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a determinantal point process to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.\nWhile submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple greedy algorithm admits a constant factor guarantee. Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.\nSubmodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012 shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011, shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems.\nSubmodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015 show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets.\n\n\n=== Applications ===\n\nSpecific applications of automatic summarization include:\n\nThe Reddit bot \"autotldr\", created in 2011 summarizes news articles in the comment-section of reddit posts. It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times. The name is reference to TL;DR − Internet slang for \"too long; didn't read\".\nAdversarial stylometry may make use of summaries, if the detail lost is not major and the summary is sufficiently stylistically different to the input.\n\n\n== Evaluation ==\nThe most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.\nEvaluation can be intrinsic or extrinsic, and inter-textual or intra-textual.\n\n\n=== Intrinsic versus extrinsic ===\nIntrinsic evaluation assesses the summaries directly, while extrinsic evaluation evaluates how the summarization system affects the completion of some other task. Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.\n\n\n=== Inter-textual versus intra-textual ===\nIntra-textual evaluation assess the output of a specific summarization system, while inter-textual evaluation focuses on contrastive analysis of outputs of several summarization systems.\nHuman judgement often varies greatly in what it considers a \"good\" summary, so creating an automatic evaluation process is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive, as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning coherence and coverage.\nThe most common way to evaluate summaries is ROUGE (Recall-Oriented Understudy for Gisting Evaluation). It is very common for summarization and translation systems in NIST's Document Understanding Conferences.[2] ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references. It calculates n-gram overlaps between automatically generated summaries and previously written human summaries. It is recall-based to encourage inclusion of all important topics in summaries. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is the fraction of unigrams that appear in both the reference summary and the automatic summary out of all unigrams in the reference summary. If there are multiple reference summaries, their scores are averaged. A high level of overlap should indicate a high degree of shared concepts between the two summaries.\nROUGE cannot determine if the result is coherent, that is if sentences flow together in a sensibly. High-order n-gram ROUGE measures help to some degree.\nAnother unsolved problem is Anaphor resolution. Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.\n\n\n=== Domain-specific versus domain-independent summarization ===\nDomain-independent summarization techniques apply sets of general features to identify information-rich text segments. Recent research focuses on domain-specific summarization using knowledge specific to the text's domain, such as medical knowledge and ontologies for summarizing medical texts.\n\n\n=== Qualitative ===\nThe main drawback of the evaluation systems so far is that we need a reference summary (for some methods, more than one), to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be made to create corpora of texts and their corresponding summaries. Furthermore, some methods require manual annotation of the summaries (e.g. SCU in the Pyramid Method). Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.\n\n\n== History ==\nThe first publication in the area dates back to 1957  (Hans Peter Luhn), starting with a statistical technique. Research increased significantly in 2015. Term frequency–inverse document frequency had been used by 2016. Pattern-based summarization was the most powerful option for multi-document summarization found by 2016. In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF). Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity. By 2020, the field was still very active and research is shifting towards abstractive summation and real-time summarization.\n\n\n=== Recent approaches ===\nRecently the rise of transformer models replacing more traditional RNN (LSTM) have provided a flexibility in the mapping of text sequences to text sequences of a different type, which is well suited to automatic summarization. This includes models such as T5 and Pegasus.\n\n\n== See also ==\nSentence extraction\nText mining\nMulti-document summarization\n\n\n== References ==\n\n\n== Works cited ==\nPotthast, Martin; Hagen, Matthias; Stein, Benno (2016). Author Obfuscation: Attacking the State of the Art in Authorship Verification (PDF). Conference and Labs of the Evaluation Forum.\n\n\n== Further reading ==\nHercules, Dalianis (2003). Porting and evaluation of automatic summarization.\nRoxana, Angheluta (2002). The Use of Topic Segmentation for Automatic Summarization.\nAnne, Buist (2004). Automatic Summarization of Meeting Data: A Feasibility Study (PDF). Archived from the original (PDF) on 2021-01-23. Retrieved 2020-07-19.\nAnnie, Louis (2009). Performance Confidence Estimation for Automatic Summarization.\nElena, Lloret and Manuel, Palomar (2009). Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation. Archived from the original on 2018-10-03. Retrieved 2018-10-03.{{cite book}}:  CS1 maint: multiple names: authors list (link)\nAndrew, Goldberg (2007). Automatic Summarization.\nAlrehamy, Hassan (2018). \"SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation\". Advances in Computational Intelligence Systems. Advances in Intelligent Systems and Computing. Vol. 650. pp. 222–235. doi:10.1007/978-3-319-66939-7_19. ISBN 978-3-319-66938-0.\nEndres-Niggemeyer, Brigitte (1998). Summarizing Information. Springer. ISBN 978-3-540-63735-6.\nMarcu, Daniel (2000). The Theory and Practice of Discourse Parsing and Summarization. MIT Press. ISBN 978-0-262-13372-2.\nMani, Inderjeet (2001). Automatic Summarization. ISBN 978-1-58811-060-2.\nHuff, Jason (2010). AutoSummarize., Conceptual artwork using automatic summarization software in Microsoft Word 2008.\nLehmam, Abderrafih (2010). Essential summarizer: innovative automatic text summarization software in twenty languages - ACM Digital Library. Riao '10. pp. 216–217., Published in Proceeding RIAO'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France\nXiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007). Improving diversity in ranking using absorbing random walks (PDF).{{cite book}}:  CS1 maint: multiple names: authors list (link), The GRASSHOPPER algorithm\nMiranda-Jiménez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013). \"Summarizing Conceptual Graphs for Automatic Summarization Task\". Conceptual Structures for STEM Research and Education. Lecture Notes in Computer Science. Vol. 7735. pp. 245–253. doi:10.1007/978-3-642-35786-2_18. ISBN 978-3-642-35785-5.{{cite book}}:  CS1 maint: multiple names: authors list (link), Conceptual Structures for STEM Research and Education.",
    "categories": [
      "All accuracy disputes",
      "All articles needing additional references",
      "All articles to be expanded",
      "All articles with unsourced statements",
      "Articles needing additional references from April 2022",
      "Articles to be expanded from February 2017",
      "Articles with disputed statements from June 2018",
      "Articles with short description",
      "Articles with unsourced statements from June 2018",
      "CS1 maint: archived copy as title",
      "CS1 maint: bot: original URL status unknown",
      "CS1 maint: location missing publisher",
      "CS1 maint: multiple names: authors list",
      "Computational linguistics",
      "Data mining",
      "Natural language processing",
      "Short description is different from Wikidata",
      "Tasks of natural language processing",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 2011
  },
  {
    "title": "Speech recognition",
    "url": "https://en.wikipedia.org/wiki/Speech_recognition",
    "content": "Speech recognition (automatic speech recognition (ASR), computer speech recognition, or speech-to-text (STT)) is a sub-field of computational linguistics concerned with methods and technologies that translate spoken language into text or other interpretable forms.\nSpeech recognition applications include voice user interfaces, where the user speaks to a device, which \"listens\" and processes the audio. Common voice applications include interpreting commands for calling, call routing, home automation, and aircraft control. These applications are called direct voice input. Productivity applications include searching audio recordings, creating transcripts, and dictation. \nSpeech recognition can be used to analyse speaker characteristics, such as identifying native language using pronunciation assessment. \nVoice recognition (speaker identification) refers to identifying the speaker, rather than speech contents. Recognizing the speaker can simplify the task of translating speech in systems trained on a specific person's voice. It can also be used to authenticate the speaker as part of a security process.\n\n\n== History ==\nApplications for speech recognition developed over many decades, with progress accelerated due to advances in deep learning and the use of big data. These advances are reflected in an increase in academic papers, and greater system adoption.\nKey areas of growth include vocabulary size, more accurate recognition for unfamiliar speakers (speaker independence), and faster processing speed.\n\n\n=== Pre-1970 ===\n1952 – Bell Labs researchers, Stephen Balashek, R. Biddulph, and K. H. Davis, built Audrey for single-speaker digit recognition. Their system located the formants in the power spectrum of each utterance.\n1960 – Gunnar Fant developed and published the source-filter model of speech production.\n1962 – IBM's 16-word \"Shoebox\" machine's speech recognition debuted at the 1962 World's Fair.\n1966 – Linear predictive coding, a speech coding method, was proposed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone.\n1969 – Funding at Bell Labs came to a halt for several years after the company's head engineer, John R. Pierce, wrote an open letter criticizing speech recognition research. This defunding lasted until Pierce retired and James L. Flanagan took over.\nRaj Reddy was the first person to work on continuous speech recognition, as a graduate student at Stanford University in the late 1960s. Previous systems required users to pause after each word. Reddy's system issued spoken commands for playing chess.\nAround this time, Soviet researchers invented the dynamic time warping (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. DTW processed speech by dividing it into short frames (e.g. 10 ms segments) and treating each frame as a unit. Speaker independence, however, remained unsolved.\n\n\n=== 1970–1990 ===\n1971 – DARPA funded a five-year speech recognition research project, Speech Understanding Research, seeking a minimum vocabulary size of 1,000 words. The project considered speech understanding a key to achieving progress in speech recognition, which was later disproved. BBN, IBM, Carnegie Mellon (CMU), and Stanford Research Institute participated.\n1972 – The IEEE Acoustics, Speech, and Signal Processing group held a conference in Newton, Massachusetts.\n1976 – The first ICASSP was held in Philadelphia, which became a major venue for publishing on speech recognition.\nDuring the late 1960s, Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. A decade later, at CMU, Raj Reddy's students James Baker and Janet M. Baker began using the hidden Markov model (HMM) for speech recognition. James Baker had learned about HMMs while at the Institute for Defense Analysis. HMMs enabled researchers to combine sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.\nBy the mid-1980s, Fred Jelinek's team at IBM created a voice-activated typewriter called Tangora, which could handle a 20,000-word vocabulary. Jelinek's statistical approach placed less emphasis on emulating human brain processes in favor of statistical modelling. (Jelinek's group independently discovered the application of HMMs to speech.) This was controversial among linguists since HMMs are too simplistic to account for many features of human languages. However, the HMM proved to be a highly useful way for modelling speech and replaced dynamic time warping as the dominant speech recognition algorithm in the 1980s.\n\n1982 – Dragon Systems, founded by James and Janet M. Baker, was one of IBM's few competitors.\n\n\n=== Practical speech recognition ===\nThe 1980s also saw the introduction of the n-gram language model.\n\n1987 – The back-off model enabled language models to use multiple-length n-grams, and CSELT used HMM to recognize languages (in software and hardware, e.g. RIPAC).\nAt the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB of RAM. It could take up to 100 minutes to decode 30 seconds of speech.\nPractical products included:\n\n1984 – the Apricot Portable was released with up to 4096 words support, of which only 64 could be held in RAM at a time.\n1987 – a recognizer from Kurzweil Applied Intelligence\n1990 – Dragon Dictate, a consumer product released in 1990. AT&T deployed the Voice Recognition Call Processing service in 1992 to route telephone calls without a human operator. The technology was developed by Lawrence Rabiner and others at Bell Labs.\nBy the early 1990s, the vocabulary of the typical commercial speech recognition system had exceeded the average human vocabulary. Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. Sphinx-II was the first to do speaker-independent, large vocabulary, continuous speech recognition, and it won DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone. Huang later founded the speech recognition group at Microsoft in 1993. Reddy's student Kai-Fu Lee joined Apple, where, in 1992, he helped develop the Casper speech interface prototype.\nLernout & Hauspie, a Belgium-based speech recognition company, acquired other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. L&H was used in Windows XP. L&H was an industry leader until an accounting scandal destroyed it in 2001. L&H speech technology was bought by ScanSoft, which became Nuance in 2005. Apple licensed Nuance software for its digital assistant Siri.\n\n\n==== 2000s ====\nIn the 2000s, DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002, followed by Global Autonomous Language Exploitation (GALE) in 2005. Four teams participated in EARS: IBM; a team led by BBN with LIMSI and the University of Pittsburgh; Cambridge University; and a team composed of ICSI, SRI, and the University of Washington. EARS funded the collection of the Switchboard telephone speech corpus, which contained 260 hours of recorded conversations from over 500 speakers. The GALE program focused on Arabic and Mandarin broadcast news. Google's first effort at speech recognition came in 2007 after recruiting Nuance researchers. Its first product, GOOG-411, was a telephone-based directory service.\nSince at least 2006, the U.S. National Security Agency has employed keyword spotting, allowing analysts to index large volumes of recorded conversations and identify speech containing \"interesting\" keywords. Other government research programs focused on intelligence applications, such as DARPA's EARS program and IARPA's Babel program.\nIn the early 2000s, speech recognition was dominated by hidden Markov models combined with feed-forward artificial neural networks (ANN). Later, speech recognition was taken over by long short-term memory (LSTM), a recurrent neural network (RNN) published by Sepp Hochreiter & Jürgen Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn \"Very Deep Learning\" tasks that require memories of events that happened thousands of discrete time steps earlier, which is important for speech.\nAround 2007, LSTMs trained with Connectionist Temporal Classification (CTC) began to outperform. In 2015, Google reported a 49 percent error‑rate reduction in its speech recognition via CTC‑trained LSTM. Transformers, a type of neural network based solely on attention, were adopted in computer vision and language modelling, and then to speech recognition. \nDeep feed-forward (non-recurrent) networks for acoustic modelling were introduced in 2009 by Geoffrey Hinton and his students at the University of Toronto, and by Li Deng and colleagues at Microsoft Research. In contrast to the prioer incremental improvements, deep learning decreased error rates by 30%.\nBoth shallow and deep forms (e.g., recurrent nets) of ANNs had been explored since the 1980s. However, these methods never defeated non-uniform internal-handcrafting Gaussian mixture model/hidden Markov model (GMM-HMM) technology. Difficulties analyzed in the 1990s, included gradient diminishing and weak temporal correlation structure. All these difficulties combined with insufficient training data and computing power. Most speech recognition pursued generative modelling approaches until deep learning won the day. Hinton et al. and Deng et al.\n\n\n==== 2010s ====\nBy early the 2010s, speech recognition was differentiated from speaker recognition, and speaker independence was considered a major breakthrough. Until then, systems required a \"training\" period for each voice.\nIn 2017, Microsoft researchers reached the human parity milestone of transcribing conversational speech on the widely benchmarked Switchboard task. Multiple deep learning models were used to improve accuracy. The error rate was reported to be as low as 4 professional human transcribers working together on the same benchmark.\n\n\n== Models, methods, and algorithms ==\nBoth acoustic modeling and language modeling are important parts of statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modelling is also used in many other natural language processing applications, such as document classification or statistical machine translation.\n\n\n=== Hidden Markov models ===\n\nSpeech recognition systems are based on HMMs. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time scale (e.g. 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.\nHMMs are popular because they can be trained automatically and are simple and computationally feasible. An HMM outputs a sequence of n-dimensional real-valued vectors (where n is an integer such as 10), outputting one every 10 milliseconds. The vectors consist of cepstral coefficients, obtained by a Fourier transform of a short window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The HMM tends to have, in each state, a statistical distribution that is a mixture of diagonal covariance Gaussians, which give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, has a different output distribution; an HMM for a sequence of words or phonemes is made by concatenating the individual trained HMMs for the separate words and phonemes.\nSpeech recognition systems use combinations of standard techniques to improve results. A typical large-vocabulary system applies context dependency for the phonemes (so that phonemes with different left and right context have different realizations as HMM states). It uses cepstral normalization to handle speaker and recording conditions. It might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general adaptation. The features use delta and delta-delta coefficients to capture speech dynamics, and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might use splicing and LDA-based projection, followed by HLDA or a global semi-tied covariance transform (also known as maximum likelihood linear transform (MLLT)). Many systems use discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE), and minimum phone error (MPE).\n\n\n=== Dynamic time warping (DTW)-based speech recognition ===\n\nDynamic time warping was historically used for speech recognition, but was later displaced by HMM.\nDynamic time warping measures similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns could be detected, even if in one video a person was walking slowly and in another was walking more quickly, or even if accelerations and decelerations came during one observation. DTW has been applied to video, audio, and graphics – any data that can be turned into a linear representation can be analyzed with DTW.\nThis could handle speech at different speaking speeds. In general, it allows an optimal match between two sequences (e.g., time series) with certain restrictions. The sequences are \"warped\" non-linearly to match each other. This sequence alignment method is often used in the context of HMMs.\n\n\n=== Neural networks ===\n\nNeural networks became interesting in the late 1980s before beginning to dominate in the 2010s. Neural networks have been used in many aspects of speech recognition, such as phoneme classification, phoneme classification through multi-objective evolutionary algorithms, isolated word recognition, audiovisual speech recognition, audiovisual speaker recognition, and speaker adaptation.\nNeural networks make fewer explicit assumptions about feature statistical properties than HMMs. When used to estimate the probabilities of a speech segment, neural networks allow natural and efficient discriminative training. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words, early neural networks were rarely successful for continuous recognition because of their limited ability to model temporal dependencies.\nOne approach was to use neural networks for feature transformation, or dimensionality reduction. However, more recently, LSTM and related recurrent neural networks (RNNs), Time Delay Neural Networks (TDNN's), and transformers demonstrated improved performance.\n\n\n==== Deep feedforward and recurrent neural networks ====\n\nResearchers are exploring deep neural networks (DNNs) and denoising  autoencoders .A DNN is a type of artificial neural network that includes multiple hidden layers between the input and output. Like simpler neural networks, DNNs can model complex, non-linear relationships. However, their deeper architecture allows them to build more sophisticated representations that combine features from earlier layers. This gives them a powerful ability to learn and recognize complex patterns in speech data.\nA major breakthrough in using DNNs for large vocabulary speech recognition came in 2010. In a collaboration between industry and academia, researchers used DNNs with large output layers based on context-dependent HMM states that were created using decision trees. This approach significantly improved performanc.\nA core idea behind deep learning is to eliminate the need for manually designed features and instead learn directly from input data. This was first demonstrated using deep autoencoders trained on raw spectrograms or linear filter-bank features. These models outperformed traditional Mel-Cepstral features, which rely on fixed transformations. More recently, researchers showed that waveforms can achieve excellent results in large-scale speech recognition.\n\n\n=== End-to-end learning ===\nSince 2014, much research has considered \"end-to-end\" ASR. Traditional phonetic-based (i.e., all HMM-based model) approaches required separate components and training for pronunciation, acoustic, and language. End-to-end models learn from all the components at once. This simplifies the training and deployment processes. For example, an n-gram language model is required for all HMM-based systems, and a typical 2025-era n-gram language model often takes gigabytes in memory, making them impractical to deploy on mobile devices. Consequently, ASR systems from Google and Apple (as of 2017) deploy on servers and required a network connection to operate.\nThe first attempt at end-to-end ASR was the Connectionist Temporal Classification (CTC)-based system introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014. The model consisted of RNNs and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however, it is incapable of learning the language model due to conditional independence assumptions, similar to an HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to finalize transcripts. Later, Baidu expanded on the work with extremely large datasets and demonstrated some commercial success in Mandarin and English. \nIn 2016, the University of Oxford presented LipNet, the first end-to-end sentence-level lipreading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted dataset. A large-scale convolutional-RNN-CTC architecture was presented in 2018 by Google DeepMind, achieving 6 times better performance than human experts. In 2019, Nvidia launched two CNN-CTC ASR models, Jasper and QuarzNet, with an overall performance word error rate (WER) of 3%. Similar to other deep learning applications, transfer learning and domain adaptation are important strategies for reusing and extending the capabilities of deep learning models, particularly due to the small size of available corpora in many languages and/or specific domains.\nIn 2018, researchers at MIT Media Lab announced preliminary work on AlterEgo, a device that uses electrodes to read the neuromuscular signals users make as they subvocalize. They trained a convolutional neural network to translate the electrode signals into words.\n\n\n==== Attention-based models ====\nAttention-based ASR models were introduced by Chan et al. of Carnegie Mellon University and Google Brain, and Bahdanau et al. of the University of Montreal in 2016. The model named \"Listen, Attend and Spell\" (LAS), literally \"listens\" to the acoustic signal, pays \"attention\" to all parts of the signal and \"spells\" out the transcript one character at a time. Unlike CTC-based models, attention-based models require conditional-independence assumptions and can learn all the components of a speech recognizer directly. This means that during deployment, no a priori language model is required, making it less demanding for applications with limited memory. \nAttention-based models immediately outperformed CTC models (with or without an external language model) and continued improving. Latent Sequence Decomposition (LSD) was proposed by Carnegie Mellon University, MIT, and Google Brain to directly emit sub-word units that are more natural than English characters. The University of Oxford and Google DeepMind extended LAS to \"Watch, Listen, Attend and Spell\" (WLAS) to handle lip reading and surpassed human-level performance.\n\n\n== Applications ==\n\n\n=== In-car systems ===\nVoice commands may be used to initiate phone calls, select radio stations, or play music. Voice recognition capabilities vary across car make and model. Some models offer natural-language speech recognition, allowing the driver to use full sentences and common phrases in a conversational style. With such systems, fixed commands are not required. \n\n\n=== Education ===\n\nAutomatic pronunciation assessment is the use of speech recognition to verify the correctness of speech, as distinguished from assessment by a person. Also called speech verification, pronunciation evaluation, and pronunciation scoring, the main application of this technology is computer-aided pronunciation teaching (CAPT) when combined with computer-aided instruction for computer-assisted language learning (CALL), speech remediation, or accent reduction. Pronunciation assessment does not determine unknown speech (as in dictation or automatic transcription) but instead, compares speech to a reference model for the words spoken, sometimes with inconsequential prosody such as intonation, pitch, tempo, rhythm, and stress. Pronunciation assessment is also used in reading tutoring, for example in products such as Microsoft Teams and Amira Learning. Pronunciation assessment can also be used to help diagnose and treat speech disorders such as apraxia.\nAssessing intelligibility is essential for avoiding inaccuracies from accent bias, especially in high-stakes assessments, from words with multiple correct pronunciations, and from phoneme coding errors in digital pronunciation dictionaries. In 2022, researchers found that some newer speech to text systems, based on end-to-end reinforcement learning to map audio signals directly into words, produce word and phrase confidence scores closely correlated with listener intelligibility. In the Common European Framework of Reference for Languages (CEFR) assessment criteria for \"overall phonological control\", intelligibility outweighs formally correct pronunciation at all levels.\n\n\n=== Health care ===\n\n\n==== Medical documentation ====\nIn the health care sector, speech recognition can be implemented in front-end or back-end medical documentation processes. In front-end speech recognition, the provider dictates into a speech-recognition engine, words are displayed as they are recognized, and the speaker is responsible for editing and signing off on the document. In back-end or deferred speech recognition the provider speaks into a digital dictation system, the voice is routed through a speech-recognition machine, and a draft document is routed along with the voice file to an editor, who edits/finalizes the draft and final report.\nA major issue is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides substantial financial benefits to physicians who utilize an Electronic Health Record (EHR) that complies with \"Meaningful Use\" standards. These standards require that substantial data be maintained by the EHR. The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary; the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.\nA more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of a clinician's interaction with EHR involves navigation through the user interface that is heavily dependent on keyboard and mouse; voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice \"macros\", where the use of certain phrases – e.g., \"normal report\", will automatically fill in a large number of default values and/or generate boilerplate, which vary with the type of exam – e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.\n\n\n==== Therapeutic use ====\nProlonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.\n\n\n=== Military ===\n\n\n==== Aircraft ====\nSubstantial efforts have been devoted to the test and evaluation of speech recognition in fighter aircraft. Of particular note have been the US programme in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the programme in France for Mirage aircraft, and UK programmes dealing with a variety of aircraft platforms. In these programmes, speech recognizers have been operated successfully, with applications including setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.\nWorking with Swedish pilots flying the JAS-39 Gripen, Englund (2004) reported that recognition deteriorated with increasing g-loads. The study concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. Spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.\nThe Eurofighter Typhoon employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for many cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major benefit in the reduction of pilot workload, and allows the pilot to assign targets with two voice commands or to a wingman with only five commands.\nSpeaker-independent systems are under test for the F-35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.\n\n\n==== Helicopters ====\nThe problems of achieving high recognition accuracy under stress and noise are particularly relevant in the helicopter environment as well as in the fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, because of the high noise levels, and because helicopter pilots, in general, do not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programmes, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France included speech recognition in the Puma helicopter. Voice applications include control of communication radios, navigation systems, and an automated target handover system.\nThe overriding issue for voice is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.\n\n\n==== Air traffic control ====\nTraining for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a trainer to act as a \"pseudo-pilot\", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have with real pilots. Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as a pseudo-pilot, thus reducing training and support personnel. \nIn theory, air controller tasks are characterized by highly structured speech as the primary output, reducing the difficulty of the speech recognition task. In practice, this is rarely the case. FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.\nThe USAF, USMC, US Army, US Navy, and FAA as well as international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada use ATC simulators with speech recognition.\n\n\n=== People with disabilities ===\nSpeech recognition programs can provide many benefit to those with disabilities. For individuals who are deaf or hard of hearing, speech recognition software can be used to generate captions of conversations. Additionally, individuals who are blind (see blindness and education) or have poor vision can benefit from listening to textual content, as well as garner more functionality from a computer by issuing commands with their voice.\nThe use of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software, has proven useful for restoring damaged short-term memory capacity in individuals who have suffered a stroke or have undergone a craniotomy.\nSpeech recognition has proven very useful for those who have difficulty using their hands due to causes ranging from mild repetitive stress injuries to disabilities that preclude the use of conventional computer input devices. Individuals with physical disabilities can use voice commands and transcription to navigate electronics hands-free. In fact, people who developed RSI from keyboard use became an early and urgent market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who struggle with thought-to-paper communication may benefit from the software, but the product's fallibility remains a significant consideration for many. In addition, speech to text technology is only an effective aid for those with intellectual disabilities if the proper training and resources are provided (e.g. in the classroom setting).\nThis type of technology can help those with dyslexia, but the potential benefits regarding other disabilities are still in question. Mistakes made by the software hinder its effectiveness, since misheard words take more time to fix.\n\n\n=== Other domains ===\n\nASR is now commonplace in the field of telephony. In telephony systems, ASR is predominantly used in contact centers by integrating it with IVR systems. \nIt is becoming more widespread in computer gaming and simulation. \nDespite the high level of integration with word processing in general personal computing, in the field of document production, ASR has not seen the expected increases in use.\nThe improvement of mobile processor speeds has made speech recognition practical in smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands.\n\nAerospace, e.g., NASA's Mars Polar Lander used speech recognition technology from Sensory, Inc. in the Mars microphone on the lander\nAutomatic subtitling with speech recognition\nAutomatic emotion recognition\nAutomatic shot listing in audiovisual production\nAutomatic translation\nE-discovery\nHands-free computing\nHome automation\nInteractive voice response\nMobile telephony, including mobile email\nMultimodal interaction\nReal-time captioning\nRobotics\nSecurity, including usage with other biometric scanners for multi-factor authentication\nSpeech to text\nTelematics, e.g., vehicle navigation systems\nTranscription\nVideo games like Tom Clancy's EndWar and Lifeline\nVirtual assistant such as Siri\n\n\n== Performance ==\nThe performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured in elapsed time. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).\nSpeech recognition is complicated by many properties of speech. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, dialect, nasality, pitch, volume, and speed. Speech is distorted by background noise, echoes, and recording characteristics. Accuracy of speech recognition may vary with the following:\n\nVocabulary size and confusability\nSpeaker dependence versus independence\nIsolated, discontinuous, or continuous speech\nTask and language constraints\nRead versus spontaneous speech\nAdverse conditions\n\n\n=== Accuracy ===\nThe accuracy of speech recognition may vary depending on the following factors:\n\nError rates increase as the vocabulary size grows:\ne.g. the 10 digits \"zero\" to \"nine\" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000, or 100000 may have error rates of 3%, 7%, or 45% respectively.\nVocabulary is hard to recognize if it contains confusing letters:\ne.g. the 26 letters of the English alphabet are difficult to discriminate because they are confusing words (most notoriously, the E-set: \"B, C, D, E, G, P, T, V, Z (when \"Z\" is pronounced \"zee\" rather than \"zed\", depending on region); an 8% error rate is considered good for this vocabulary.\nSpeaker dependence vs. independence:\nA speaker-dependent system is intended for use by a single speaker.\nA speaker-independent system is intended for use by any speaker (more difficult).\nIsolated, discontinuous or continuous speech\nWith isolated speech, single words are used, which is easier to recognize.\nWith discontinuous speech, full sentences separated by silence are used. The silence is easier to recognize similar to isolated speech.\nWith continuous speech naturally spoken sentences are used, which are harder to recognize.\nTask and language constraints can inform the recognition\nThe requesting application may dismiss the hypothesis \"The apple is red.\"\nConstraints may be semantic; rejecting \"The apple is angry.\"\nSyntactic; rejecting \"Red is apple the.\"\nConstraints are often represented by grammar.\nRead vs. spontaneous speech\nWhen a person reads it's usually in a context that has been previously prepared.\nWhen a person speaks spontaneously, recognition must deal with disfluencies such as \"uh\" and \"um\", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary.\nAdverse conditions\nenvironmental noise (e.g., in a car or factory).\nAcoustic distortions (e.g. echoes, room acoustics)\nSpeech recognition is a multi-level pattern recognition task.\n\nAcoustic signals are structured into a hierarchy of units, e.g. phonemes, words, phrases, and sentences;\nEach level provides additional constraints; e.g., known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at a lower level;\nThis hierarchy of constraints improves accuracy. By combining decisions probabilistically at all lower levels, and making ultimate decisions only at the highest level, speech recognition is broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken into smaller sub-signals. As the more complex sound signal is divided, different levels are created, where at the top level are complex sounds made of simpler sounds on the lower level, etc. At the lowest level, simple and more probabilistic rules apply. These sounds are put together into more complex sounds on upper level, a new set of more deterministic rules predicts what the complex sound represents. The upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition, we need to take into consideration neural networks. Neural network approaches use the following steps:\n\nDigitize the speech – for telephone speech, 8000 samples per second are captured;\nCompute features of spectral-domain of the speech (with Fourier transform); computed every 10ms, with one 10ms section called a frame;\nSound is produced by air (or some other medium) vibration. Sound creates a wave that has two measures: amplitude (strength), and frequency (vibrations per second). Accuracy can be computed with the help of WER, which is calculated by aligning the recognized word and referenced word using dynamic string alignment. The problem may occur while computing the WER due to the difference between the sequence lengths of the recognized word and referenced word.\nThe formula to compute the word error rate (WER) is:\n\n  \n    \n      \n        W\n        E\n        R\n        =\n        \n          \n            \n              (\n              s\n              +\n              d\n              +\n              i\n              )\n            \n            n\n          \n        \n      \n    \n    {\\displaystyle WER={(s+d+i) \\over n}}\n  \n\nwhere s is the number of substitutions, d is the number of deletions, i is the number of insertions, and n is the number of word references.\nWhile computing, the word recognition rate (WRR) is used. The formula is:\n\n  \n    \n      \n        W\n        R\n        R\n        =\n        1\n        −\n        W\n        E\n        R\n        =\n        \n          \n            \n              (\n              n\n              −\n              s\n              −\n              d\n              −\n              i\n              )\n            \n            n\n          \n        \n        =\n        \n          \n            \n              h\n              −\n              i\n            \n            n\n          \n        \n      \n    \n    {\\displaystyle WRR=1-WER={(n-s-d-i) \\over n}={h-i \\over n}}\n  \n\nwhere h is the number of correctly recognized words:\n\n  \n    \n      \n        h\n        =\n        n\n        −\n        (\n        s\n        +\n        d\n        )\n        .\n      \n    \n    {\\displaystyle h=n-(s+d).}\n  \n\n\n=== Security ===\nSpeech recognition can become a means of attack, theft, or accidental operation. For example, activation words like \"Alexa\" spoken in an audio or video broadcast can cause devices in homes and offices to start listening for input inappropriately, or possibly take an unwanted action. Voice-controlled devices may be accessible to unauthorized users. Attackers may be able to gain access to personal information, like calendars, address book contents, private messages, and documents. They may also be able to impersonate the user to send messages or make online purchases.\nTwo attacks have been demonstrated that use artificial sounds. One transmits ultrasound and attempts to send commands without people noticing. The other adds small, inaudible distortions to other speech or music that are specially crafted to confuse the specific speech recognition system into recognizing music as speech, or to make what sounds like one command to a human sound like a different command to the system.\n\n\n== Further information ==\n\n\n=== Conferences ===\nRegular conferences include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, include papers on speech processing. \n\n\n=== Journal ===\nThe main journal is IEEE/ACM Transactions on Audio, Speech and Language Processing.\n\n\n=== Books ===\nFundamentals of Speech Recognition by Lawrence Rabiner (1993)\nStatistical Methods for Speech Recognition by Frederick Jelinek\nSpoken Language Processing by Xuedong Huang et al. (2001)\nComputer Speech by Manfred R. Schroeder (2004)\nSpeech Processing: A Dynamic and Optimization-Oriented Approach by Li Deng and Doug O'Shaughnessey (2003).\nSpeech and Language Processing by Jurafsky and Martin (2008)\nFundamentals of Speaker Recognition – in depth source for up to date details on the theory and practice.\nThe Voice in the Machine. Building Computers That Understand Speech by Roberto Pieraccini (2012) – Introduction\nAutomatic Speech Recognition: A Deep Learning Approach by Microsoft researchers D. Yu and L. Deng (2014) – mathematically-oriented  treatment of deep learning methods are\nDeep Learning: Methods and Applications by L. Deng and D. Yu (2014) – methodology-focused overview of DNN-based speech recognition\n\n\n=== Projects ===\nThe largest speech recognition-related project ongoing as of 2007 was the GALE project, which involves both speech recognition and translation components.\n\n\n=== Software ===\nSphinx toolkit is one starting point for experimenting with speech recognition.\nHTK book and accompanying toolkit\nKaldi toolkit can be used.\nCommon Voice (uses TensorFlow).\nCoqui STT (derived from Common Voice, using the same open-source license)\nGboard supports speech recognition on all Android applications.\nSpeech recognition is available in Microsoft Windows operating systems.\nCommercial cloud based speech recognition APIs are broadly available.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nCole, Ronald; Mariani, Joseph; Uszkoreit, Hans; Varile, Giovanni Battista; Zaenen, Annie; Zampolli; Zue, Victor, eds. (1997). Survey of the state of the art in human language technology. Cambridge Studies in Natural Language Processing. Vol. XII–XIII. Cambridge University Press. ISBN 978-0-521-59277-2.\nJunqua, J.-C.; Haton, J.-P. (1995). Robustness in Automatic Speech Recognition: Fundamentals and Applications. Kluwer Academic Publishers. ISBN 978-0-7923-9646-8.\nKarat, Clare-Marie; Vergo, John; Nahamoo, David (2007). \"Conversational Interface Technologies\". In Sears, Andrew; Jacko, Julie A. (eds.). The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies, and Emerging Applications (Human Factors and Ergonomics). Lawrence Erlbaum Associates Inc. ISBN 978-0-8058-5870-9.\nPieraccini, Roberto (2012). The Voice in the Machine. Building Computers That Understand Speech. The MIT Press. ISBN 978-0-262-01685-8.\nPirani, Giancarlo, ed. (2013). Advanced algorithms and architectures for speech understanding. Springer Science & Business Media. ISBN 978-3-642-84341-9.\nSigner, Beat; Hoste, Lode (December 2013). \"SpeeG2: A Speech- and Gesture-based Interface for Efficient Controller-free Text Entry\". Proceedings of ICMI 2013. 15th International Conference on Multimodal Interaction. Sydney, Australia.\nWoelfel, Matthias; McDonough, John (26 May 2009). Distant Speech Recognition. Wiley. ISBN 978-0-470-51704-8.",
    "categories": [
      "All articles containing potentially dated statements",
      "All articles needing additional references",
      "All articles that are too technical",
      "All articles with dead external links",
      "All articles with unsourced statements",
      "Articles containing potentially dated statements from 2017",
      "Articles needing additional references from July 2025",
      "Articles with dead external links from March 2023",
      "Articles with multiple maintenance issues",
      "Articles with permanently dead external links",
      "Articles with short description",
      "Articles with unsourced statements from July 2025",
      "Articles with unsourced statements from September 2025",
      "Automatic identification and data capture",
      "CS1: long volume value",
      "CS1: unfit URL",
      "CS1 errors: missing periodical",
      "Computational linguistics",
      "Computer accessibility",
      "History of human–computer interaction",
      "Machine learning task",
      "Short description matches Wikidata",
      "Speech recognition",
      "Use dmy dates from February 2017",
      "User interface techniques",
      "Webarchive template wayback links",
      "Wikipedia articles that are too technical from July 2025"
    ],
    "year_mentioned": 2003
  },
  {
    "title": "Question answering",
    "url": "https://en.wikipedia.org/wiki/Question_answering",
    "content": "Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP) that is concerned with building systems that automatically answer questions that are posed by humans in a natural language.\n\n\n== Overview ==\nA question-answering implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, question-answering systems can pull answers from an unstructured collection of natural language documents.\nSome examples of natural language document collections used for question answering systems include:\n\na local collection of reference texts\ninternal organization documents and web pages\ncompiled newswire reports\na set of Wikipedia pages\na subset of World Wide Web pages\n\n\n== Types of question answering ==\nQuestion-answering research attempts to develop ways of answering a wide range of question types, including fact, list, definition, how, why, hypothetical, semantically constrained, and cross-lingual questions.\n\nAnswering questions related to an article in order to evaluate reading comprehension is one of the simpler form of question answering, since a given article is relatively short compared to the domains of other types of question-answering problems. An example of such a question is \"What did Albert Einstein win the Nobel Prize for?\" after an article about this subject is given to the system.\nClosed-book question answering is when a system has memorized some facts during training and can answer questions without explicitly being given a context. This is similar to humans taking closed-book exams.\nClosed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance) and can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, \"closed-domain\" might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information. Question answering systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimer's disease.\nOpen-domain question answering deals with questions about nearly anything and can only rely on general ontologies and world knowledge. Systems designed for open-domain question answering usually have much more data available from which to extract the answer. An example of an open-domain question is \"What did Albert Einstein win the Nobel Prize for?\" while no article about this subject is given to the system.\nAnother way to categorize question-answering systems is by the technical approach used. There are a number of different types of QA systems, including \n\nrule-based systems,\nstatistical systems, and\nhybrid systems.\nRule-based systems use a set of rules to determine the correct answer to a question. Statistical systems use statistical methods to find the most likely answer to a question. Hybrid systems use a combination of rule-based and statistical methods.\n\n\n== History ==\nTwo early question answering systems were BASEBALL and LUNAR. BASEBALL answered questions about Major League Baseball over a period of one year. LUNAR answered questions about the geological analysis of rocks returned by the Apollo Moon missions. Both question answering systems were very effective in their chosen domains. LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain that were posed by people untrained on the system. Further restricted-domain question answering systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to ELIZA and DOCTOR, the first chatterbot programs.\nSHRDLU was a successful question-answering program developed by Terry Winograd in the late 1960s and early 1970s. It simulated the operation of a robot in a toy world (the \"blocks world\"), and it offered the possibility of asking the robot questions about the state of the world. The strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.\nIn the 1970s, knowledge bases were developed that targeted narrower domains of knowledge. The question answering systems developed to interface with these expert systems produced more repeatable and valid responses to questions within an area of knowledge. These expert systems closely resembled modern question answering systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized knowledge bases, whereas many modern question answering systems rely on statistical processing of a large, unstructured, natural language text corpus.\nThe 1970s and 1980s saw the development of comprehensive theories in computational linguistics, which led to the development of ambitious projects in text comprehension and question answering. One example was the Unix Consultant (UC), developed by Robert Wilensky at U.C. Berkeley in the late 1980s. The system answered questions pertaining to the Unix operating system. It had a comprehensive, hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.\nSpecialized natural-language question answering systems have been developed, such as EAGLi for health and life scientists.\n\n\n== Applications ==\nQA systems are used in a variety of applications, including \n\nFact-checking if a fact is verified, by posing a question like: is fact X true or false?\ncustomer service,\ntechnical support,\nmarket research,\ngenerating reports or conducting research.\n\n\n== Architecture ==\nAs of 2001, question-answering systems typically included a question classifier module that determined the type of question and the type of answer.\nDifferent types of question-answering systems employ different architectures. For example, modern open-domain question answering systems may use a retriever-reader architecture. The retriever is aimed at retrieving relevant documents related to a given question, while the reader is used to infer the answer from the retrieved documents. Systems such as GPT-3, T5, and BART use an end-to-end architecture in which a transformer-based architecture stores large-scale textual data in the underlying parameters. Such models can answer questions without accessing any external knowledge sources.\n\n\n== Question answering methods ==\nQuestion answering is dependent on a good search corpus; without documents containing the answer, there is little any question answering system can do. Larger collections generally mean better question answering performance, unless the question domain is orthogonal to the collection. Data redundancy in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents, leading to two benefits:\n\nIf the right information appears in many forms, the question answering system needs to perform fewer complex NLP techniques to understand the text.\nCorrect answers can be filtered from false positives because the system can rely on versions of the correct answer appearing more times in the corpus than incorrect ones.\nSome question answering systems rely heavily on automated reasoning.\n\n\n=== Open domain question answering ===\n\nIn information retrieval, an open-domain question answering system tries to return an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. The system finds answers by using a combination of techniques from computational linguistics, information retrieval, and knowledge representation.\nThe system takes a natural language question as an input rather than a set of keywords, for example: \"When is the national day of China?\" It then transforms this input sentence into a query in its logical form. Accepting natural language questions makes the system more user-friendly, but harder to implement, as there are a variety of question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task; the entire answer extraction process relies on finding the correct question type and hence the correct answer type.\nKeyword extraction is the first step in identifying the input question type. In some cases, words clearly indicate the question type, e.g., \"Who\", \"Where\", \"When\", or \"How many\"—these words might suggest to the system that the answers should be of type \"Person\", \"Location\", \"Date\", or \"Number\", respectively. POS (part-of-speech) tagging and syntactic parsing techniques can also determine the answer type. In the example above, the subject is \"Chinese National Day\", the predicate is \"is\" and the adverbial modifier is \"when\", therefore the answer type is \"Date\". Unfortunately, some interrogative words like \"Which\", \"What\", or \"How\" do not correspond to unambiguous answer types: Each can represent more than one type. In situations like this, other words in the question need to be considered. A lexical dictionary such as WordNet can be used for understanding the context.\nOnce the system identifies the question type, it uses an information retrieval system to find a set of documents that contain the correct keywords. A tagger and NP/Verb Group chunker can verify whether the correct entities and relations are mentioned in the found documents. For questions such as \"Who\" or \"Where\", a named-entity recogniser finds relevant \"Person\" and \"Location\" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.\nA vector space model can classify the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. An inference technique can validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate—the more and the closer the better. The answer is then translated by parsing into a compact and meaningful representation. In the previous example, the expected output answer is \"1st Oct.\"\n\n\n=== Mathematical question answering ===\nAn open-source, math-aware, question answering system called MathQA, based on Ask Platypus and Wikidata, was published in 2018. MathQA takes an English or Hindi natural language question as input and returns a mathematical formula retrieved from Wikidata as a succinct answer, translated into a computable form that allows the user to insert values for the variables. The system retrieves names and values of variables and common constants from Wikidata if those are available. It is claimed that the system outperforms a commercial computational mathematical knowledge engine on a test set. MathQA is hosted by Wikimedia at https://mathqa.wmflabs.org/. In 2022, it was extended to answer 15 math question types.\nMathQA methods need to combine natural and formula language. One possible approach is to perform supervised annotation via Entity Linking. The \"ARQMath Task\" at CLEF 2020 was launched to address the problem of linking newly posted questions from the platform Math Stack Exchange to existing ones that were already answered by the community. Providing hyperlinks to already answered, semantically related questions helps users to get answers earlier but is a challenging problem because semantic relatedness is not trivial. The lab was motivated by the fact that 20% of mathematical queries in general-purpose search engines are expressed as well-formed questions. The challenge contained two separate sub-tasks. Task 1: \"Answer retrieval\" matching old post answers to newly posed questions, and Task 2: \"Formula retrieval\" matching old post formulae to new questions. Starting with the domain of mathematics, which involves formula language, the goal is to later extend the task to other domains (e.g., STEM disciplines, such as chemistry, biology, etc.), which employ other types of special notation (e.g., chemical formulae).\nThe inverse of mathematical question answering—mathematical question generation—has also been researched. The PhysWikiQuiz physics question generation and test engine retrieves mathematical formulae from Wikidata together with semantic information about their constituting identifiers (names and values of variables). The formulae are then rearranged to generate a set of formula variants. Subsequently, the variables are substituted with random values to generate a large number of different questions suitable for individual student tests. PhysWikiquiz is hosted by Wikimedia at https://physwikiquiz.wmflabs.org/.\n\n\n== Progress ==\nQuestion answering systems have been extended in recent years to encompass additional domains of knowledge For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current question answering research topics include:\n\ninteractivity—clarification of questions or answers\nanswer reuse or caching\nsemantic parsing\nanswer presentation\nknowledge representation and semantic entailment\nsocial media analysis with question answering systems\nsentiment analysis\nutilization of thematic roles\nImage captioning for visual question answering\nEmbodied question answering\nIn 2011, Watson, a question answering computer system developed by IBM, competed in two exhibition matches of Jeopardy! against Brad Rutter and Ken Jennings, winning by a significant margin.\nFacebook Research made their DrQA system available under an open source license. This system uses Wikipedia as knowledge source. The open source framework Haystack by deepset combines open-domain question answering with generative question answering and supports the domain adaptation of the underlying language models for industry use cases.\n\nLarge Language Models (LLMs)[36] like GPT-4[37], Gemini[38] are examples of successful QA systems that are enabling more sophisticated understanding and generation of text. When coupled with Multimodal[39] QA Systems, which can process and understand information from various modalities like text, images, and audio, LLMs significantly improve the capabilities of QA systems.\n\n\n== References ==\n\n\n== Further reading ==\nDragomir R. Radev, John Prager, and Valerie Samn. Ranking suspected answers to natural language questions using predictive annotation Archived 2011-08-26 at the Wayback Machine. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.\nJohn Prager, Eric Brown, Anni Coden, and Dragomir Radev. Question-answering by predictive annotation Archived 2011-08-23 at the Wayback Machine. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.\nHutchins, W. John; Harold L. Somers (1992). An Introduction to Machine Translation. London: Academic Press. ISBN 978-0-12-362830-5.\nL. Fortnow, Steve Homer (2002/2003).   A Short History of Computational Complexity.  In D. van Dalen, J. Dawson, and A. Kanamori, editors, The History of Mathematical Logic. North-Holland, Amsterdam.\nTunstall, Lewis (5 July 2022). Natural Language Processing with Transformers: Building Language Applications with Hugging Face (2nd ed.). O'Reilly UK Ltd. p. Chapter 7. ISBN 978-1098136796.\n\n\n== External links ==\nQuestion Answering Evaluation at TREC\nQuestion Answering Evaluation at CLEF",
    "categories": [
      "All Wikipedia articles in need of updating",
      "All Wikipedia articles needing clarification",
      "All articles containing potentially dated statements",
      "All articles needing additional references",
      "All articles needing expert attention",
      "All articles that are too technical",
      "All articles with specifically marked weasel-worded phrases",
      "Articles containing potentially dated statements from 2001",
      "Articles needing additional references from January 2016",
      "Articles needing expert attention from April 2023",
      "Articles with short description",
      "Articles with specifically marked weasel-worded phrases from April 2023",
      "CS1 errors: missing periodical",
      "CS1 maint: bot: original URL status unknown",
      "CS1 maint: location missing publisher",
      "Computational linguistics",
      "Deep learning",
      "Information retrieval genres",
      "Short description is different from Wikidata",
      "Tasks of natural language processing",
      "Webarchive template wayback links",
      "Wikipedia articles in need of updating from April 2023",
      "Wikipedia articles in need of updating from March 2024",
      "Wikipedia articles needing clarification from April 2023",
      "Wikipedia articles that are too technical from April 2023"
    ],
    "year_mentioned": 2007
  },
  {
    "title": "Generative artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Generative_artificial_intelligence",
    "content": "Generative artificial intelligence (Generative AI, or GenAI) is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data in response to input, which often comes in the form of natural language prompts.\nThe prevalence of generative AI tools has increased significantly since the AI boom in the 2020s. This boom was made possible by improvements in deep neural networks, particularly large language models (LLMs), which are based on the transformer architecture. Major tools include LLM-based chatbots such as ChatGPT, Claude, Copilot, DeepSeek, Google Gemini and Grok; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include Alibaba, Anthropic, Baidu, DeepSeek, Google, Meta AI, Microsoft, Mistral AI, OpenAI, Perplexity AI, xAI, and Yandex.\nGenerative AI has been adopted in a variety of sectors, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.  \nGenerative AI has been used for cybercrime, and to deceive and manipulate people through fake news and deepfakes. Generative AI may lead to mass replacement of human jobs. The tools themselves have been described as violating intellectual property laws, since they are trained on copyrighted works. Many generative AI systems use large-scale data centers whose environmental impacts include e-waste, consumption of fresh water for cooling, and high energy consumption that is estimated to be growing steadily. Generative AI continues to evolve rapidly as new models and applications emerge.\n\n\n== History ==\n\n\n=== Early history ===\nThe origins of algorithmically generated media can be traced to the development of the Markov chain, which has been used to model natural language since the early 20th century. Russian mathematician Andrey Markov introduced the concept in 1906, including an analysis of vowel and consonant patterns in Eugeny Onegin. Once trained on a text corpus, a Markov chain can generate probabilistic text.\nBy the early 1970s, artists began using computers to extend generative techniques beyond Markov models. Harold Cohen developed and exhibited works produced by AARON, a pioneering computer program designed to autonomously create paintings.\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\n\n\n=== Generative neural networks (2014–2019) ===\n\nMachine learning uses both discriminative models and generative models to predict data. Beginning in the late 2000s, the introduction of deep learning technology led to improvements in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images, such as DeepDream.\nIn 2017, the Transformer network enabled advancements in generative models compared to older long short-term memory (LSTM) models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018.\n\n\n=== Generative AI boom (2020–) ===\n\nIn March 2020, the release of 15.ai, a free web application created by an anonymous MIT researcher that could generate convincing character voices using minimal training data, marked one of the earliest popular use cases of generative AI. The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) in memes and content creation, influencing subsequent developments in voice AI technology.\nIn 2021, the emergence of DALL-E, a transformer-based generative model, marked an advance in AI-generated imagery. This was followed by the releases of Midjourney and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts. These systems can generate photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public.\nIn November 2022, the public release of ChatGPT popularized generative AI for general-purpose text-based tasks. The system's ability to engage in natural conversations, generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AI's potential impact on work, education, and creativity.\nIn March 2023, GPT-4's release represented another jump in generative AI capabilities. A team from Microsoft Research controversially argued that it \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" However, this assessment was contested by other scholars who maintained that generative AI remained \"still far from reaching the benchmark of 'general human intelligence'\" as of 2023. Later in 2023, Meta released ImageBind, an AI model research project which includes six different modalities: text, images, video, thermal data, 3D data, audio, and motion.\nIn December 2023, Google unveiled Gemini, a multimodal AI model available in four versions: Ultra, Pro, Flash, and Nano. The company integrated Gemini Pro into its Bard chatbot and announced plans for \"Bard Advanced\", based on the larger Gemini Ultra model. In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS.\nAnthropic also released a family of large language models named Claude, which showed competitive performance on benchmarks.\n\nAsia–Pacific countries are significantly more optimistic than Western societies about generative AI and show higher adoption rates. Despite expressing concerns about privacy and the pace of change, in a 2024 survey, 68% of Asia-Pacific respondents believed that AI was having a positive impact on the world, compared to 57% globally. According to a survey by SAS and Coleman Parkes Research, China in particular has emerged as a global leader in generative AI adoption, with 83% of Chinese respondents using the technology, exceeding both the global average of 54% and the U.S. rate of 65%. A UN report indicated that Chinese entities filed over 38,000 generative AI patents from 2014 to 2023, substantially surpassing the United States in patent applications. A 2024 survey on the Chinese social app Soul reported that 18% of respondents born after 2000 used generative AI \"almost every day\", and that over 60% of respondents like or love AI-generated content, while less than 3% dislike or hate it.\nBy mid 2025, despite continued consumer growth, many companies were increasingly abandoning generative AI pilot projects as they had difficulties with integration, data quality and unmet returns, leading analysts to characterize the period as entering the Gartner hype cycle's \"trough of disillusionment\" phase.\n\n\n== Applications ==\nNotable types of generative AI models include generative pre-trained transformers (GPTs), generative adversarial networks (GANs), and variational autoencoders (VAEs). Generative AI systems are multimodal if they can process multiple types of inputs or generate multiple types of outputs. For example, GPT-4o can both process and generate text, images and audio.\nGenerative AI has made its appearance in a wide variety of industries, radically changing the dynamics of content creation, analysis, and delivery. In healthcare, for instance, generative AI accelerates drug discovery by creating molecular structures with target characteristics and generates radiology images for training diagnostic models. This ability not only enables faster and cheaper development but also enhances medical decision-making. In finance, generative AI services help create datasets and automate reports using natural language. It automates content creation, produces synthetic financial data, and tailors customer communications. It also powers chatbots and virtual agents. Collectively, these technologies enhance efficiency, reduce operational costs, and support data-driven decision-making in financial institutions. The media industry makes use of generative AI for numerous creative activities such as music composition, scriptwriting, video editing, and digital art. The educational sector is impacted as well, since the tools make learning personalized through creating quizzes, study aids, and essay composition. Both the teachers and the learners benefit from AI-based platforms that suit various learning patterns. In the educational field, in Colombia, student use of Meta's generative AI programs resulted in a decline in scores.\n\n\n=== Text and software code ===\n\nLarge language models (LLM) are trained on tokenized text from text corpora. Such systems include ChatGPT, Gemini, Claude, LLaMA, and BLOOM. LLMs are capable of natural language processing, machine translation, and natural language generation.\nLLMs can be used as foundation models for other tasks. They can be trained on computer code, which makes it possible to generate source code for new computer programs with prompts, a practice known as vibe coding. Examples include OpenAI Codex, Tabnine, GitHub Copilot, Microsoft Copilot, and the VS Code fork Cursor.\nSome AI assistants help candidates cheat during online coding interviews by providing code, improvements, and explanations. Their clandestine interfaces minimize the need for eye movements that would expose cheating to the interviewer.\n\n\n=== Audio ===\n\nIn 2016, DeepMind's WaveNet showed that deep neural networks are capable of generating raw waveforms. WaveNet's ability to model raw waveforms meant that it could model any kind of audio, including music: for example, it was capable of generating relatively realistic-sounding human-like voices by training on recordings of real speech. In subsequent years, research shifted from concatenative synthesis to deep learning speech synthesis, with models like Tacotron 2 in 2018 demonstrating that neural networks could convert text into natural speech by being trained on tens of hours of speech. In 2020, a free text-to-speech website called 15.ai showed that deep neural networks could generate emotionally expressive speech with only 15 seconds of speech, a large reduction compared to the tens of hours of data previously required.\nOther platforms that use generative AI to produce speech include Amazon Polly, Meta's Voicebox, and ElevenLabs. Systems that can generate music via text descriptions (text-to-music) include Meta's MusicGen and Google's MusicLM. Audio deepfakes have been used to generate vocal tracks of lyrics that mimic the voices of other singers.\n\n\n=== Images ===\n\nGenerative AI can be used to create visual art. Such systems are trained on sets of images along with their text captions. Examples of text-to-image models include Stable Diffusion, DALL-E, Midjourney, Imagen, Adobe Firefly, and Flux. They can also be used for neural style transfer.\n\n\n=== Video ===\n\nGenerative AI can be used to generate photorealistic videos. Examples include Sora by OpenAI, Runway, Make-A-Video by Meta Platforms and the open source LTX Video by Lightricks.\n\n\n=== Robotics ===\nBy training on robotic system motions, generative AI can create new trajectories for motion planning and robot navigation. Multimodal vision-language-action models such as Google's RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects.\n\n\n=== 3D modeling ===\n\nArtificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate 3D modeling. AI-based CAD libraries could also be developed using linked open data of schematics and diagrams. AI CAD assistants are used as tools to help streamline workflow.\n\n\n== Software and hardware ==\n\nGenerative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot, text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2. Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office (Microsoft Copilot), Google Photos, and the Adobe Suite (Adobe Firefly). Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA language model.\nSmaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4 and one version of Stable Diffusion can run on an iPhone 11.\nLarger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.\nThe advantages of running generative AI locally include protection of privacy and intellectual property, and avoidance of rate limiting and censorship. The subreddit r/LocalLLaMA in particular focuses on using consumer-grade gaming graphics cards through such techniques as compression. That forum is one of only two sources Andrej Karpathy trusts for language model benchmarks. Yann LeCun has advocated open-source models for their value to vertical applications and for improving AI safety.\nLanguage models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIA's H100) or AI accelerator chips (such as Google's TPU). These very large models are typically accessed as cloud services over the Internet.\nIn 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI. Chips such as the NVIDIA A800 and the Biren Technology BR104 were developed to meet the requirements of the sanctions.\nThere is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it. Potential mitigation strategies for detecting generative AI content include digital watermarking, content authentication, information retrieval, and machine learning classifier models. Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work.\n\n\n=== Generative models and training techniques ===\n\n\n==== Generative adversarial networks ====\n\nGenerative adversarial networks (GANs) are a generative modeling technique which consist of two neural networks—the generator and the discriminator—trained simultaneously in a competitive setting. The generator creates synthetic data by transforming random noise into samples that resemble the training dataset. The discriminator is trained to distinguish the authentic data from synthetic data produced by the generator. The two models engage in a minimax game: the generator aims to create increasingly realistic data to \"fool\" the discriminator, while the discriminator improves its ability to distinguish real from fake data. This continuous training setup enables the generator to produce high-quality and realistic outputs.\n\n\n==== Variational autoencoders ====\n\nVariational autoencoders (VAEs) are deep learning models that probabilistically encode data. They are typically used for tasks such as noise reduction from images, data compression, identifying unusual patterns, and facial recognition. Unlike standard autoencoders, which compress input data into a fixed latent representation, VAEs model the latent space as a probability distribution, allowing for smooth sampling and interpolation between data points. The encoder (\"recognition model\") maps input data to a latent space, producing means and variances that define a probability distribution. The decoder (\"generative model\") samples from this latent distribution and attempts to reconstruct the original input. VAEs optimize a loss function that includes both the reconstruction error and a Kullback–Leibler divergence term, which ensures the latent space follows a known prior distribution. VAEs are particularly suitable for tasks that require structured but smooth latent spaces, although they may create blurrier images than GANs. They are used for applications like image generation, data interpolation and anomaly detection.\n\n\n===== Transformers =====\nTransformers became the foundation for many powerful generative models, most notably the generative pre-trained transformer (GPT) series developed by OpenAI. They marked a major shift in natural language processing by replacing traditional recurrent and convolutional models. This architecture allows models to process entire sequences simultaneously and capture long-range dependencies more efficiently. The self-attention mechanism enables the model to capture the significance of every word in a sequence when predicting the subsequent word, thus improving its contextual understanding. Unlike recurrent neural networks, transformers process all the tokens in parallel, which improves the training efficiency and scalability. Transformers are typically pre-trained on enormous corpora in a self-supervised manner, prior to being fine-tuned.\n\n\n== Law and regulation ==\n\nIn the United States, a group of companies including OpenAI, Alphabet, and Meta signed a voluntary agreement with the Biden administration in July 2023 to watermark AI-generated content. In October 2023, Executive Order 14110 applied the Defense Production Act to require all US companies to report information to the federal government when training certain high-impact AI models.\nIn the European Union, the proposed Artificial Intelligence Act includes requirements to disclose copyrighted material used to train generative AI systems, and to label any AI-generated output as such.\nIn China, the Interim Measures for the Management of Generative AI Services introduced by the Cyberspace Administration of China regulates any public-facing generative AI. It includes requirements to watermark generated images or videos, regulations on training data and label quality, restrictions on personal data collection, and a guideline that generative AI services must \"adhere to socialist core values\".\n\n\n=== Copyright ===\n\n\n==== Training with copyrighted content ====\nGenerative AI systems such as ChatGPT and Midjourney are trained on large, publicly available datasets that include copyrighted works. AI developers have argued that such training is protected under fair use, while copyright holders have argued that it infringes their rights.\nProponents of fair use training have argued that it is a transformative use and does not involve making copies of copyrighted works available to the public. Critics have argued that image generators such as Midjourney can create nearly-identical copies of some copyrighted images, and that generative AI programs compete with the content they are trained on.\nAs of 2024, several lawsuits related to the use of copyrighted material in training are ongoing.\nGetty Images has sued Stability AI over the use of its images to train Stable Diffusion. Both the Authors Guild and The New York Times have sued Microsoft and OpenAI over the use of their works to train ChatGPT.\n\n\n==== Copyright of AI-generated content ====\nA separate question is whether AI-generated works can qualify for copyright protection. The United States Copyright Office has ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship. Some legal professionals have suggested that Naruto v. Slater (2018), in which the U.S. 9th Circuit Court of Appeals held that non-humans cannot be copyright holders of artistic works, could be a potential precedent in copyright litigation over works created by generative AI. However, the office has also begun taking public input to determine if these rules need to be refined for generative AI.\nIn January 2025, the United States Copyright Office (USCO) released extensive guidance regarding the use of AI tools in the creative process, and established that \"...generative AI systems also offer tools that similarly allow users to exert control. [These] can enable the user to control the selection and placement of individual creative elements. Whether such modifications rise to the minimum standard of originality required under Feist will depend on a case-by-case determination. In those cases where they do, the output should be copyrightable\" Subsequently, the USCO registered the first visual artwork to be composed of entirely AI-generated materials, titled \"A Single Piece of American Cheese\".\n\n\n== Concerns ==\n\nThe development of generative AI has raised concerns from governments, businesses, and individuals, resulting in protests, legal actions, calls to pause AI experiments, and actions by multiple governments. In a July 2023 briefing of the United Nations Security Council, Secretary-General António Guterres stated \"Generative AI has enormous potential for good and evil at scale\", that AI may \"turbocharge global development\" and contribute between $10 and $15 trillion to the global economy by 2030, but that its malicious use \"could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale\". In addition, generative AI has a significant carbon footprint.\n\n\n=== Academic honesty ===\nGenerative AI can be used to generate and modify academic prose, to paraphrase sources, and translate languages. The use of generative AI in a classroom setting can be a form of academic plagiarism. Some schools have banned ChatGPT and similar tools. \nA commonly proposed use for teachers is grading and giving feedback. Companies like Pearson and ETS use AI to score grammar, mechanics, usage, and style, but not for main ideas or overall structure. The National Council of Teachers of English stated that machine scoring makes students feel their writing is not worth reading. AI scoring has also given unfair results for students from different ethnic backgrounds.\n\n\n=== Job losses ===\n\nFrom the early days of the development of AI, there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements. In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost. In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes. Fran Drescher, president of the Screen Actors Guild, declared that \"artificial intelligence poses an existential threat to creative professions\" during the 2023 SAG-AFTRA strike. Voice generation AI has been seen as a potential challenge to the voice acting sector.\nThe intersection of AI and employment concerns among underrepresented groups globally remains a critical facet. While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys by Fast Company. To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations. Strategies involve redirecting policy emphasis on regulation, inclusive design, and education's potential for personalized teaching to maximize benefits while minimizing harms.\n\n\n=== Racial and gender bias ===\nGenerative AI models can reflect and amplify any cultural bias present in the underlying data. For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data. Similarly, an image model prompted with the text \"a photo of a CEO\" might disproportionately generate images of white male CEOs, if trained on a racially biased data set. A number of methods for mitigating bias have been attempted, such as altering input prompts and reweighting training data.\n\n\n=== Deepfakes ===\n\nDeepfakes (a portmanteau of \"deep learning\" and \"fake\") are AI-generated media that take a person in an existing image or video and replace them with someone else's likeness using artificial neural networks. Deepfakes have garnered widespread attention and concerns for their uses in deepfake celebrity pornographic videos, revenge porn, fake news, hoaxes, health disinformation, financial fraud, and covert foreign election interference.\nIn July 2023, the fact-checking company Logically found that the popular generative AI models Midjourney, DALL-E 2 and Stable Diffusion would produce plausible disinformation images when prompted to do so, such as images of electoral fraud in the United States and Muslim women supporting India's Hindu nationalist Bharatiya Janata Party.\n\n\n==== Audio deepfakes ====\n\nInstances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI. In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards and identity verification.\nConcerns and fandoms have spawned from AI-generated music. The same software used to clone voices has been used on famous musicians' voices to create songs that mimic their voices, gaining both tremendous popularity and criticism. Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released.\nGenerative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels. The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for \"dehumanizing\" an artform, and also creating artists which create unrealistic or immoral appeals to their audiences.\n\n\n=== Illegal imagery ===\n\nMany websites that allow explicit AI generated images or videos have been created, and this has been used to create illegal content, such as rape, child sexual abuse material, necrophilia, and zoophilia.\n\n\n=== Cybercrime ===\nGenerative AI's ability to create realistic fake content has been exploited in numerous types of cybercrime, including phishing scams. Deepfake video and audio have been used to create disinformation and fraud. In 2020, former Google click fraud czar Shuman Ghosemajumder argued that once deepfake videos become perfectly realistic, they would stop appearing remarkable to viewers, potentially leading to uncritical acceptance of false information. Additionally, large language models and other forms of text-generation AI have been used to create fake reviews of e-commerce websites to boost ratings. Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT.\nA 2023 study showed that generative AI can be vulnerable to jailbreaks, reverse psychology and prompt injection attacks, enabling attackers to obtain help with harmful requests, such as for crafting social engineering and phishing attacks. Additionally, other researchers have demonstrated that open-source models can be fine-tuned to remove their safety restrictions at low cost.\n\n\n=== Information laundering ===\nGenerative AI has been noted for its use by state-sponsored propaganda campaigns in information laundering. According to a 2025 report by Graphika, generative AI is used to launder articles from Chinese state media such as China Global Television Network through various social media sites in an attempt to disguise the articles' origin.\n\n\n=== Reliance on industry giants ===\nTraining frontier AI models requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller start-ups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively.\n\n\n=== Energy and environment ===\n\nAI has a significant carbon footprint due to growing energy consumption from both training and usage. Scientists and journalists have expressed concerns about the environmental impact that the development and deployment of generative models are having: high CO2 emissions, large amounts of freshwater used for data centers, and high amounts of electricity usage. There is also concern that these impacts may increase as these models are incorporated into widely used search engines such as Google Search and Bing, as chatbots and other applications become more popular, and as models need to be retrained.\nThe carbon footprint of generative AI globally is estimated to be growing steadily, with potential annual emissions ranging from 18.21 to 245.94 million tons of CO2 by 2035, with the highest estimates for 2035 nearing the impact of the United States beef industry on emissions (currently estimated to emit 257.5 million tons annually as of 2024).\nProposed mitigation strategies include factoring potential environmental costs prior to model development or data collection, increasing efficiency of data centers to reduce electricity/energy usage, building more efficient machine learning models, minimizing the number of times that models need to be retrained, developing a government-directed framework for auditing the environmental impact of these models, regulating for transparency of these models, regulating their energy and water usage, encouraging researchers to publish data on their models' carbon footprint, and increasing the number of subject matter experts who understand both machine learning and climate science.\n\n\n=== Content quality ===\n\nThe New York Times defines slop as analogous to spam: \"shoddy or unwanted A.I. content in social media, art, books, and ... in search results.\" Journalists have expressed concerns about the scale of low-quality generated content with respect to social media content moderation, the monetary incentives from social media companies to spread such content, false political messaging, spamming of scientific research paper submissions, increased time and effort to find higher quality or desired content on the Internet, the indexing of generated content by search engines, and on journalism itself.\nA paper published by researchers at Amazon Web Services AI Labs found that over 57% of sentences from a sample of over 6 billion sentences from Common Crawl, a snapshot of web pages, were machine translated. Many of these automated translations were seen as lower quality, especially for sentences that were translated into at least three languages. Many lower-resource languages (ex. Wolof, Xhosa) were translated across more languages than higher-resource languages (ex. English, French).\nIn September 2024, Robyn Speer, the author of wordfreq, an open source database that calculated word frequencies based on text from the Internet, announced that she had stopped updating the data for several reasons: high costs for obtaining data from Reddit and Twitter, excessive focus on generative AI compared to other methods in the natural language processing community, and that \"generative AI has polluted the data\".\nThe adoption of generative AI tools led to an explosion of AI-generated content across multiple domains. A study from University College London estimated that in 2023, more than 60,000 scholarly articles—over 1% of all publications—were likely written with LLM assistance. According to Stanford University's Institute for Human-Centered AI, approximately 17.5% of newly published computer science papers and 16.9% of peer review text now incorporate content generated by LLMs.\nIf AI-generated content is included in new data crawls from the Internet for additional training of AI models, defects in the resulting models may occur. Training an AI model exclusively on the output of another AI model produces a lower-quality model. Repeating this process, where each new model is trained on the previous model's output, leads to progressive degradation and eventually results in a \"model collapse\" after multiple iterations. Tests have been conducted with pattern recognition of handwritten letters and with pictures of human faces.\nOn the other side, synthetic data is often used as an alternative to data produced by real-world events. Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy, including for structured data. The approach is not limited to text generation; image generation has been employed to train computer vision models.\n\n\n=== Misuse in journalism ===\n\nGenerative AI's potential to generate a large amount of content with little effort is also affecting journalism. In January 2023, Futurism broke the story that CNET had been using an undisclosed internal AI tool to write at least 77 of its stories; after the news broke, CNET posted corrections to 41 of the stories. In April 2023, Die Aktuelle published an AI-generated fake interview of Michael Schumacher. In May 2024, Futurism noted that a content management system video by AdVon Commerce, which had used generative AI to produce articles for many of the aforementioned outlets, appeared to show that they \"had produced tens of thousands of articles for more than 150 publishers\". In 2025, a report from the American Sunlight Project stated that Pravda network was publishing as many as 10,000 articles a day, and concluded that much of this content aimed to push Russian narratives into large language models through their training data.\nIn June 2024, Reuters Institute published its Digital News Report for 2024. In a survey of people in America and Europe, Reuters Institute reports that 52% and 47% respectively are uncomfortable with news produced by \"mostly AI with some human oversight\", and 23% and 15% respectively report being comfortable. 42% of Americans and 33% of Europeans reported that they were comfortable with news produced by \"mainly human with some help from AI\". The results of global surveys reported that people were more uncomfortable with news topics including politics (46%), crime (43%), and local news (37%) produced by AI than other news topics.\n\n\n== Detection and awareness ==\n\nOnline users have falsely accused media of using generative artificial intelligence for content, such as video games Little Droid and Catly.\nDue to various concerns about citizens' unknowingly consuming generative AI media content, proponents argue for labeling such content to provide context. The Cyberspace Administration of China issued rules obligating service providers to labeling this content online.\nThe popularity of ChatGPT caused the emergence of tools that detect whether content was AI-generated, such as GPTZero, but the risk of false accusations (false positives) has remained a concern. Digital watermarking allows to reach high detection accuracy by subtly altering the generated content in a way that can be detected by software, but without being noticeable by users. OpenAI developed in 2023 a digital watermarking tool that allowed to detect content generated by ChatGPT with an estimated accuracy of 99.9%, when given enough text. But OpenAI chose not to release it, worrying that users would switch to competitor products, and arguing that digital watermarking can be circumvented by bad actors, for example with superficial rephrasing. Google's digital watermarking tool called SynthID was integrated in 2025 into products like Gemini, Imagen and Veo. Google also created the portal SynthID detector for users to check whether text, images or videos were produced with Google's generative AI products.\n\n\n== See also ==\n\nArtificial general intelligence – Type of AI with wide-ranging abilities\nArtificial imagination – Artificial simulation of human imagination\nArtificial intelligence art – Visual media created with AIPages displaying short descriptions of redirect targets\nArtificial life – Field of study\nChatbot – Program that simulates conversation\nComputational creativity – Multidisciplinary endeavour\nGenerative adversarial network – Deep learning method\nGenerative pre-trained transformer – Type of large language model\nLarge language model – Type of machine learning model\nLists of open-source artificial intelligence software\nMusic and artificial intelligence – Usage of artificial intelligence to generate music\nGenerative AI pornography – Explicit material produced by generative AI\nProcedural generation – Method in which data is created algorithmically as opposed to manually\nRetrieval-augmented generation – Type of information retrieval using LLMs\nStochastic parrot – Term used in machine learning\n\n\n== References ==\n\n\n== Further reading ==\nHe, Ran; Cao, Jie; Tan, Tieniu (2025). \"Generative Artificial Intelligence: A Historical Perspective\". National Science Review. 12 (5) nwaf050. doi:10.1093/nsr/nwaf050. PMC 11970245. PMID 40191253.\nJames Gleick, \"The Parrot in the Machine\" (review of Emily M. Bender and Alex Hanna, The AI Con: How to Fight Big Tech's Hype and Create the Future We Want, Harper, 274 pp.; and James Boyle, The Line: AI and the Future of Personhood, MIT Press, 326 pp.), The New York Review of Books, vol. LXXII, no. 12 (24 July 2025), pp. 43–46. \"[C]hatbox 'writing' has a bland, regurgitated quality. Textures are flattened, sharp edges are sanded. No chatbox could ever have said that April is the cruelest month or that fog comes on little cat feet (though they might now, because one of their chief skills is plagiarism). And when synthetically extruded text turns out wrong, it can be comically wrong. When a movie fan asked Google whether a certain actor was in Heat, he received this 'AI Overview': 'No, Angelina Jolie is not in heat.'\" (p. 44.)",
    "categories": [
      "2020s fads and trends",
      "2020s in computing",
      "2023 in computing",
      "2024 in computing",
      "2025 in computing",
      "All Wikipedia articles written in American English",
      "All articles lacking reliable references",
      "All articles with style issues",
      "All articles with unsourced statements",
      "All pages needing factual verification",
      "Articles containing video clips",
      "Articles lacking reliable references from July 2025",
      "Articles with hAudio microformats",
      "Articles with short description",
      "Articles with unsourced statements from November 2025",
      "Articles with unsourced statements from October 2025",
      "Artificial neural networks",
      "CS1 Chinese-language sources (zh)",
      "CS1 Italian-language sources (it)",
      "CS1 maint: numeric names: authors list",
      "Deep learning",
      "Generative artificial intelligence",
      "Machine learning",
      "Pages displaying short descriptions of redirect targets via Module:Annotated link",
      "Short description is different from Wikidata",
      "Use American English from April 2025",
      "Use mdy dates from May 2025",
      "Wikipedia articles needing factual verification from October 2025",
      "Wikipedia articles with style issues from October 2025"
    ],
    "year_mentioned": 2023
  },
  {
    "title": "Generative model",
    "url": "https://en.wikipedia.org/wiki/Generative_model",
    "content": "In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished:\n\nA generative model is a statistical model of the joint probability distribution \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n  \n on a given observable variable X and target variable Y; A generative model can be used to \"generate\" random instances (outcomes) of an observation x.\nA discriminative model is a model of the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        ∣\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y\\mid X=x)}\n  \n of the target Y, given an observation x. It can be used to \"discriminate\" the value of the target variable Y, given an observation x.\nClassifiers computed without using a probability model are also referred to loosely as \"discriminative\".\nThe distinction between these last two classes is not consistently made; Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.\nStandard examples of each, all of which are linear classifiers, are:\n\ngenerative classifiers:\nnaive Bayes classifier and\nlinear discriminant analysis\ndiscriminative model:\nlogistic regression\nIn application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n  \n (discriminative model), and base classification on that; or one can estimate the joint distribution \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n  \n (generative model), from that compute the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n  \n, and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.\n\n\n== Definition ==\nAn alternative division defines these symmetrically as:\n\na generative model is a model of the conditional probability of the observable X, given a target y, symbolically, \n  \n    \n      \n        P\n        (\n        X\n        ∣\n        Y\n        =\n        y\n        )\n      \n    \n    {\\displaystyle P(X\\mid Y=y)}\n  \n\na discriminative model is a model of the conditional probability of the target Y, given an observation x, symbolically, \n  \n    \n      \n        P\n        (\n        Y\n        ∣\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y\\mid X=x)}\n  \n\nRegardless of precise definition, the terminology is constitutional because a generative model can be used to \"generate\" random instances (outcomes), either of an observation and target \n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle (x,y)}\n  \n, or of an observation x given a target value y, while a discriminative model or discriminative classifier (without a model) can be used to \"discriminate\" the value of the target variable Y, given an observation x. The difference between \"discriminate\" (distinguish) and \"classify\" is subtle, and these are not consistently distinguished. (The term \"discriminative classifier\" becomes a pleonasm when \"discrimination\" is equivalent to \"classification\".)\nThe term \"generative model\" is also used to describe models that generate instances of output variables in a way that has no clear relationship to probability distributions over potential samples of input variables. Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs. Such models are not classifiers.\n\n\n=== Relationships between models ===\nIn application to classification, the observable X is frequently a continuous variable, the target Y is generally a discrete variable consisting of a finite set of labels, and the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        ∣\n        X\n        )\n      \n    \n    {\\displaystyle P(Y\\mid X)}\n  \n can also be interpreted as a (non-deterministic) target function \n  \n    \n      \n        f\n        :\n        X\n        →\n        Y\n      \n    \n    {\\displaystyle f\\colon X\\to Y}\n  \n, considering X as inputs and Y as outputs.\nGiven a finite set of labels, the two definitions of \"generative model\" are closely related. A model of the conditional distribution \n  \n    \n      \n        P\n        (\n        X\n        ∣\n        Y\n        =\n        y\n        )\n      \n    \n    {\\displaystyle P(X\\mid Y=y)}\n  \n is a model of the distribution of each label, and a model of the joint distribution is equivalent to a model of the distribution of label values \n  \n    \n      \n        P\n        (\n        Y\n        )\n      \n    \n    {\\displaystyle P(Y)}\n  \n, together with the distribution of observations given a label, \n  \n    \n      \n        P\n        (\n        X\n        ∣\n        Y\n        )\n      \n    \n    {\\displaystyle P(X\\mid Y)}\n  \n; symbolically, \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n        =\n        P\n        (\n        X\n        ∣\n        Y\n        )\n        P\n        (\n        Y\n        )\n        .\n      \n    \n    {\\displaystyle P(X,Y)=P(X\\mid Y)P(Y).}\n  \n Thus, while a model of the joint probability distribution is more informative than a model of the distribution of label (but without their relative frequencies), it is a relatively small step, hence these are not always distinguished.\nGiven a model of the joint distribution, \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n  \n, the distribution of the individual variables can be computed as the marginal distributions \n  \n    \n      \n        P\n        (\n        X\n        )\n        =\n        \n          ∑\n          \n            y\n          \n        \n        P\n        (\n        X\n        ,\n        Y\n        =\n        y\n        )\n      \n    \n    {\\displaystyle P(X)=\\sum _{y}P(X,Y=y)}\n  \n and \n  \n    \n      \n        P\n        (\n        Y\n        )\n        =\n        \n          ∫\n          \n            x\n          \n        \n        P\n        (\n        Y\n        ,\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y)=\\int _{x}P(Y,X=x)}\n  \n (considering X as continuous, hence integrating over it, and Y as discrete, hence summing over it), and either conditional distribution can be computed from the definition of conditional probability: \n  \n    \n      \n        P\n        (\n        X\n        ∣\n        Y\n        )\n        =\n        P\n        (\n        X\n        ,\n        Y\n        )\n        \n          /\n        \n        P\n        (\n        Y\n        )\n      \n    \n    {\\displaystyle P(X\\mid Y)=P(X,Y)/P(Y)}\n  \n and \n  \n    \n      \n        P\n        (\n        Y\n        ∣\n        X\n        )\n        =\n        P\n        (\n        X\n        ,\n        Y\n        )\n        \n          /\n        \n        P\n        (\n        X\n        )\n      \n    \n    {\\displaystyle P(Y\\mid X)=P(X,Y)/P(X)}\n  \n.\nGiven a model of one conditional probability, and estimated probability distributions for the variables X and Y, denoted \n  \n    \n      \n        P\n        (\n        X\n        )\n      \n    \n    {\\displaystyle P(X)}\n  \n and \n  \n    \n      \n        P\n        (\n        Y\n        )\n      \n    \n    {\\displaystyle P(Y)}\n  \n, one can estimate the opposite conditional probability using Bayes' rule:\n\n  \n    \n      \n        P\n        (\n        X\n        ∣\n        Y\n        )\n        P\n        (\n        Y\n        )\n        =\n        P\n        (\n        Y\n        ∣\n        X\n        )\n        P\n        (\n        X\n        )\n        .\n      \n    \n    {\\displaystyle P(X\\mid Y)P(Y)=P(Y\\mid X)P(X).}\n  \n\nFor example, given a generative model for \n  \n    \n      \n        P\n        (\n        X\n        ∣\n        Y\n        )\n      \n    \n    {\\displaystyle P(X\\mid Y)}\n  \n, one can estimate:\n\n  \n    \n      \n        P\n        (\n        Y\n        ∣\n        X\n        )\n        =\n        P\n        (\n        X\n        ∣\n        Y\n        )\n        P\n        (\n        Y\n        )\n        \n          /\n        \n        P\n        (\n        X\n        )\n        ,\n      \n    \n    {\\displaystyle P(Y\\mid X)=P(X\\mid Y)P(Y)/P(X),}\n  \n\nand given a discriminative model for \n  \n    \n      \n        P\n        (\n        Y\n        ∣\n        X\n        )\n      \n    \n    {\\displaystyle P(Y\\mid X)}\n  \n, one can estimate:\n\n  \n    \n      \n        P\n        (\n        X\n        ∣\n        Y\n        )\n        =\n        P\n        (\n        Y\n        ∣\n        X\n        )\n        P\n        (\n        X\n        )\n        \n          /\n        \n        P\n        (\n        Y\n        )\n        .\n      \n    \n    {\\displaystyle P(X\\mid Y)=P(Y\\mid X)P(X)/P(Y).}\n  \n\nNote that Bayes' rule (computing one conditional probability in terms of the other) and the definition of conditional probability (computing conditional probability in terms of the joint distribution) are frequently conflated as well.\n\n\n== Contrast with discriminative classifiers ==\nA generative algorithm models how the data was generated in order to categorize a signal. It asks the question: based on my generation assumptions, which category is most likely to generate this signal? A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal. So, discriminative algorithms try to learn \n  \n    \n      \n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p(y|x)}\n  \n directly from the data and then try to classify data. On the other hand, generative algorithms try to learn \n  \n    \n      \n        p\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle p(x,y)}\n  \n which can be transformed into \n  \n    \n      \n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p(y|x)}\n  \n later to classify the data. One of the advantages of generative algorithms is that you can use \n  \n    \n      \n        p\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle p(x,y)}\n  \n to generate new data similar to existing data. On the other hand, it has been proved that some discriminative algorithms give better performance than some generative algorithms in classification tasks.\nDespite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express complex relationships between the observed and target variables. But in general, they don't necessarily perform better than generative models at classification and regression tasks. The two classes are seen as complementary or as different views of the same procedure.\n\n\n== Deep generative models ==\nWith the rise of deep learning, a new family of methods, called deep generative models (DGMs), is formed through the combination of generative models and deep neural networks. An increase in the scale of the neural networks is typically accompanied by an increase in the scale of the training data, both of which are required for good performance.\nPopular DGMs include variational autoencoders (VAEs), generative adversarial networks (GANs), and auto-regressive models. Recently, there has been a trend to build very large deep generative models. For example, GPT-3, and its precursor GPT-2, are auto-regressive neural language models that contain billions of parameters, BigGAN and VQ-VAE which are used for image generation that can have hundreds of millions of parameters, and Jukebox is a very large generative model for musical audio that contains billions of parameters.\n\n\n== Types ==\n\n\n=== Generative models ===\nTypes of generative models are:\n\nGaussian mixture model (and other types of mixture model)\nHidden Markov model\nProbabilistic context-free grammar\nBayesian network (e.g. Naive bayes, Autoregressive model)\nAveraged one-dependence estimators\nLatent Dirichlet allocation\nBoltzmann machine (e.g. Restricted Boltzmann machine, Deep belief network)\nVariational autoencoder\nGenerative adversarial network\nFlow-based generative model\nEnergy based model\nDiffusion model\nIf the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the true distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see below), although application-specific details will ultimately dictate which approach is most suitable in any particular case.\n\n\n=== Discriminative models ===\nk-nearest neighbors algorithm\nLogistic regression\nSupport Vector Machines\nDecision Tree Learning\nRandom Forest\nMaximum-entropy Markov models\nConditional random fields\n\n\n== Examples ==\n\n\n=== Simple example ===\nSuppose the input data is \n  \n    \n      \n        x\n        ∈\n        {\n        1\n        ,\n        2\n        }\n      \n    \n    {\\displaystyle x\\in \\{1,2\\}}\n  \n, the set of labels for \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is \n  \n    \n      \n        y\n        ∈\n        {\n        0\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle y\\in \\{0,1\\}}\n  \n, and there are the following 4 data points:\n\n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n        =\n        {\n        (\n        1\n        ,\n        0\n        )\n        ,\n        (\n        1\n        ,\n        1\n        )\n        ,\n        (\n        2\n        ,\n        0\n        )\n        ,\n        (\n        2\n        ,\n        1\n        )\n        }\n      \n    \n    {\\displaystyle (x,y)=\\{(1,0),(1,1),(2,0),(2,1)\\}}\n  \n\nFor the above data, estimating the joint probability distribution \n  \n    \n      \n        p\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle p(x,y)}\n  \n from the empirical measure will be the following:\n\nwhile \n  \n    \n      \n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p(y|x)}\n  \n will be following:\n\n\n=== Text generation ===\nShannon (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with \"representing and speedily is an good\"; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc.\n\n\n== See also ==\n\nDiscriminative model\nGraphical model\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==",
    "categories": [
      "All Wikipedia articles lacking focus",
      "Articles with short description",
      "Machine learning",
      "Probabilistic models",
      "Short description is different from Wikidata",
      "Statistical models",
      "Wikipedia articles lacking focus from May 2025"
    ],
    "year_mentioned": 2002
  },
  {
    "title": "Foundation model",
    "url": "https://en.wikipedia.org/wiki/Foundation_model",
    "content": "In artificial intelligence, a foundation model (FM), also known as large x model (LxM, with \"x\" representing any of text, images, sound, etc.), is a machine learning or deep learning model trained on vast datasets so that it can be applied across a wide range of use cases. Generative AI applications like large language models (LLM) are common examples of foundation models.\nBuilding foundation models is often highly resource-intensive, with the most advanced models costing hundreds of millions of dollars to cover the expenses of acquiring, curating, and processing massive datasets, as well as the compute power required for training. These costs stem from the need for sophisticated infrastructure, extended training times, and advanced hardware, such as GPUs. In contrast, adapting an existing foundation model for a specific task or using it directly is far less costly, as it leverages pre-trained capabilities and typically requires only fine-tuning on smaller, task-specific datasets.\nEarly examples of foundation models are language models like OpenAI's GPT series and Google's BERT. Beyond text, foundation models have been developed across a range of modalities—including DALL-E, Stable diffusion, and Flamingo for images, MusicGen and LLark for music, and RT-2 for robotic control. Foundation models are also being developed for fields like astronomy, radiology, genomics, coding, times-series forecasting, mathematics, and chemistry.\n\n\n== Definitions ==\nThe Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) coined the term \"foundation model\" in August 2021 to mean \"any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks\". This was based on their observation that preexisting terms, while overlapping, were not adequate, stating that \"'large language model' was too narrow given the focus is not only language; 'self-supervised model' was too specific to the training objective; and 'pretrained model' suggested that the noteworthy action all happened after 'pretraining.\" The term \"foundation model\" was chosen over \"foundational model\" because \"foundational\" implies that these models provide fundamental principles in a way that \"foundation\" does not. The term vision-language model (VLM) is also used as a near-synonym.\nAs governments regulate foundation models, new legal definitions have emerged.\n\nIn the United States, the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence defines a foundation model as \"an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts\".\nIn the United States, the proposed AI Foundation Model Transparency Act of 2023 by House Representatives Don Beyer (D, VA) and Anna Eshoo (D, CA) defines a foundation model as \"an artificial intelligence model trained on broad data, generally uses self supervision, generally contains at least 1,000,000,000 parameters, is applicable across a wide range of contexts, and exhibits, or could be easily modified to exhibit, high levels of performance at tasks that could pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters.\"\nIn the European Union, the European Parliament's negotiated position on the E.U. AI Act defines a foundation model as an \"AI model that is trained on broad data at scale, is designed for generality of output, and can be adapted to a wide range of distinctive tasks\".\nIn the United Kingdom, the Competition and Markets Authority's AI Foundation Models: Initial Report  defines foundations model as \"a type of AI technology that are trained on vast amounts of data that can be adapted to a wide range of tasks and operations.\"\nThe United States's definitions are the only ones to make reference to the size of a foundation model, and differ on magnitude. Beyer and Eshoo's definition also specifies that foundation models must achieve a level of performance as to be a potential danger. In contrast, the E.U. definition requires the model to be designed for generality of output. All definitions agree that foundation models must be trained on a broad range of data with potential applications in many domains.\n\n\n== History ==\nTechnologically, foundation models are built using established machine learning techniques like deep neural networks, transfer learning, and self-supervised learning. Foundation models differ from previous techniques as they are general purpose models that function as a reusable infrastructure, instead of bespoke and one-off task-specific models.\nAdvances in computer parallelism (e.g., CUDA GPUs) and new developments in neural network architecture (e.g., Transformers), and the increased use of training data with minimal supervision all contributed to the rise of foundation models. Foundation models began to materialize as the latest wave of deep learning models in the late 2010s. Relative to most prior work on deep learning, these language models demonstrated the potential of training on much larger web-sourced datasets using self-supervised objectives (e.g. predicting the next word in a large corpus of text). These approaches, which draw upon earlier works like word2vec and GloVe, deviated from prior supervised approaches that required annotated data (e.g. crowd-sourced labels).\nThe 2022 releases of Stable Diffusion and ChatGPT (initially powered by the GPT-3.5 model) led to foundation models and generative AI entering widespread public discourse. Further, releases of LLaMA, Llama 2, and Mistral in 2023 contributed to a greater emphasis placed on how foundation models are released with open foundation models garnering a lot of support and scrutiny.\n\n\n== Related concepts ==\n\n\n=== Frontier models ===\nCertain highly advanced foundation models are termed \"frontier models\", which have the potential to \"possess dangerous capabilities sufficient to pose severe risks to public safety.\" These \"dangerous capabilities\" stem from the accidental or intentional misuse of such models, which in conjunction with their powerful nature can lead to severe harms. As foundation models continue to improve, some AI researchers speculate that almost all next-generation foundation models will be considered frontier models.\nSince the concept of dangerous capabilities is inherently subjective, there is no strict designation for what foundation models qualify as frontier models. However, some generally held ideas for sufficiently dangerous capabilities include:\n\nDesigning and synthesizing new biological or chemical weapons\nProducing and propagating convincing, tailored disinformation with minimal user instruction\nHarnessing unprecedented offensive cyber capabilities\nEvading human control through deceptive means\nDue to frontier models' unique capabilities, it is difficult to effectively regulate their development and deployment. Because of their emergent nature, new dangerous capabilities can appear on their own in frontier models, both in the development stage and after being deployed. Additionally, since frontier models continue to adapt after deployment, it remains difficult to mitigate all harms that arise from already-deployed models. If a frontier model happens to be open-source or is released online, the model can also disseminate rapidly, further hampering regulators by creating a lack of accountability.\n\n\n=== General-purpose AI ===\nDue to their adaptability to a wide range of use-cases, foundation models are sometimes considered to be examples of general-purpose AI. In designing the EU AI Act, the European Parliament has stated that a new wave of general-purpose AI technologies shapes the overall AI ecosystem. The fuller structure of the ecosystem, in addition to the properties of specific general-purpose AI systems, influences the design of AI policy and research. General-purpose AI systems also often appear in people's everyday lives through applications and tools like ChatGPT or DALL-E.\nGovernment agencies like EU Parliament have identified regulation of general-purpose AI, such as foundation models, to be a high priority. General-purpose AI systems are often characterized by large size, opacity, and potential for emergence, all of which can create unintended harms. Such systems also heavily influence downstream applications, which further exacerbates the need for regulation. In regards to prominent legislation, a number of stakeholders have pushed for the EU AI Act to include restrictions on general-purpose AI systems, all of which would also apply to foundation models.\n\n\n=== World models ===\n\nWorld models are sometimes described as foundation models. World models are a representation of an environment intended to predict the state of that environment after taking a set of actions, as well as to implicitly model physical concepts such as gravity. Input prompts for world models can include text or images, as well as videos or 3D scenes, and the resulting 3D environments can be exported. World models, alongside embodied AI, multi-agent models, and neuroscience models of the brain, are seen as alternatives to large language models for achieving general artificial intelligence.\nWorld models do not have a fully agreed definition, but have been divided into two scopes: one for representing and understanding the current environment, and another for predicting the future state of that environment. In the former view, world models are developed using model-based reinforcement learning and a Markov decision process, using model predictive control or Monte Carlo tree search to create policies. With the latter, (multimodal) large language models or video generation models can be used. In addition, these environments can be immersive simulations for training AI agents that can interact in the real world.\n\n\n==== History ====\nQuanta Magazine traced world models back to a 1943 publication by Kenneth Craik on mental models and the blocks world of SHRDLU in the 1960s. Business Insider traced world models to a 1971 paper by Jay Wright Forrester. A related idea of organizing world knowledge, the frame representation, was proposed by Marvin Minsky in 1974.\nIn 2018, researchers David Ha and Jürgen Schmidhuber defined world models in the context of reinforcement learning: an agent with a variational autoencoder model V for representing visual observations, a recurrent neural network model M for representing memory, and a linear model C for making decisions. They suggested that agents trained on world models in environments that simulate reality could be applied to real world settings.\nIn 2022, Yann LeCun saw a world model (defined by him as a neural network that acts as a mental model for aspects of the world that are seen as relevant) as part of a larger system of cognitive architecture – other neural networks that are analogous to different regions of the brain. In his view, this framework could lead to commonsense reasoning.\n\n\n==== Training ====\nWorld models are trained on a variety of data modalities, including text, images, audio and video, and have been applied to video generation. One open source dataset for world models includes 1 billion data points across multiple modalities (text, images, audio, video and point clouds), including 1 million manual annotations.\n\n\n==== Examples ====\nTechCrunch saw Sora as an example of a world model, while in January 2025, Nvidia released its own set of world models. The South China Morning Post wrote that Manycore Tech was another example of companies aiming to build a world model, viewing their work as an example of spatial intelligence. In May 2025, Mohamed bin Zayed University of Artificial Intelligence released a world model for building simulations to test AI agents.\nGoogle DeepMind has also released two world models in two-dimensional space and three-dimensional space, respectively, that were trained on video data, with Google claiming that the latter can be a training environment for AI agents. Meta released a world model in June 2025, Tencent released an open source world model in July 2025. Niantic, Inc. spinoff, Niantic Spatial, is developing a world model using anonymized player scans from Pokémon GO. Other companies that are planning as of 2025 to build world models include ByteDance and xAI.\n\n\n==== Applications ====\nFei-Fei Li views world models as applying to robotics and creative works. Due to the complexity of these models, she advocates for more complex strategies in data acquisition, data engineering, data processing, and synthesizing data. She co-founded a startup on building world models, which, as of 2024, planned to do so in three phases: incorporating an understanding of three-dimensional space along with time; support for augmented reality; and support for robotics. Her startup, World Labs, released its commercial world model, Marble, in November 2025.\nWorld models are intended for use in interactive media (such as video games and movies) and environment simulation. Proposed use cases for world models include action planning and outcome prediction. Other applications include social simulacra to simulate social systems. Wired compared world models to the metaverse, while Business Insider noted possible military applications. \nIn 2025, world models are being applied to drone warfare, robotics, and self-driving vehicles. The Wall Street Journal speculated that world models could improve spatial reasoning of artificial intelligence models and successfully automate both blue-collar and white-collar jobs. As of October 2025, research has shown mixed results in the spatial reasoning capabilities of text-to-video models (in particular, Veo 3).\n\n\n==== Concerns ====\nTechCrunch noted that world models could use more data than large language models and would require significantly more computational power (including the use of thousands of GPUs for training and inference). It also noted the risk of hallucinations, coverage bias and algorithmic bias. Similarly, The Financial Times noted the difficulty and expense in collecting data to simulate the world and training models to use that data.\nCreative professionals have expressed concern that world models could disrupt jobs in their industries.\nOther concerns include data privacy, simulation of harmful situations, misinformation and disinformation,  emergent behaviors, and copyright.\n\n\n== Technical details ==\n\n\n=== Modeling ===\nFor a foundation model to effectively generalize, it must acquire rich representations of the training data. As a result, expressive model architectures that efficiently process large-scale data are often preferred in building foundation models. Currently, the Transformer architecture is the de facto choice for building foundation models across a range of modalities.\n\n\n=== Training ===\nFoundation models are built by optimizing a training objective(s), which is a mathematical function that determines how model parameters are updated based on model predictions on training data. Language models are often trained with a next-tokens prediction objective, which refers to the extent at which the model is able to predict the next token in a sequence. Image models are commonly trained with contrastive learning or diffusion training objectives. For contrastive learning, images are randomly augmented before being evaluated on the resulting similarity of the model's representations. For diffusion models, images are noised and the model learns to gradually de-noise via the objective. Multimodal training objectives also exist, with some separating images and text during training, while others examine them concurrently. In general, the training objectives for foundation models promote the learning of broadly useful representations of data.\nWith the rise of foundation models and the larger datasets that power them, a training objective must be able to parse through internet-scale data for meaningful data points. Additionally, since foundation models are designed to solve a general range of tasks, training objectives ought to be domain complete, or able to solve a broad set of downstream capabilities within the given domain. Lastly, foundation model training objectives should seek to scale well and be computationally efficient. With model size and compute power both being relevant constraints, a training objective must be able to overcome such bottlenecks.\n\n\n=== Data ===\nFoundation models are trained on a large quantity of data, working under the maxim \"the more data, the better.\" Performance evaluation does show that more data generally leads to better performance, but other issues arise as data quantity grows. Tasks like managing the dataset, integrating data across new applications, ensuring adherence to data licenses, and maintaining data quality all become more difficult as data size grows. The specific demands of foundation models have only exacerbated such issues, as it remains the norm for large foundation models to use public web-scraped data. Foundation models include also search engines data and SEO meta tags data. Public web data remains a plentiful resource, but it also demands stringent moderation and data processing from foundation model developers before it can be successfully integrated into the training pipeline.\nTraining foundation models often runs the risk of violating user privacy, as private data can be disclosed, collected, or used in ways beyond the stated scope. Even if no private data is leaked, models can still inadvertently compromise security through learned behavior in the resulting foundation model. Data quality is another key point, as web-scraped data frequently contains biased, duplicate, and toxic material. Once foundation models are deployed, ensuring high-quality data is still an issue, as undesirable behavior can still emerge from small subsets of data.\n\n\n=== Systems ===\nThe size of foundation models also brings about issues with the computer systems they run on. The average foundation model is too large to be run within a single accelerator's memory and the initial training process requires an expensive amount of resources. Such issues are predicted to further exacerbate in future as foundation models grow to new heights. Due to this constraint, researchers have begun looking into compressing model size through tight model inference.\nGPUs are the most common choice of compute hardware for machine learning, due to high memory storage and strong power. Typical foundation model training requires many GPUs, all connected in parallel with fast interconnects. Acquiring a sufficient amount of GPUs of requisite compute efficiency is a challenge for many foundation model developers, one that has led to an increasing dilemma in the field. Larger models require greater compute power, but often at the cost of improved compute efficiency. Since training remains time-consuming and expensive, the tradeoff between compute power and compute efficiency has led only a few select companies to afford the production costs for large, state of the art foundation models. Some techniques like compression and distillation can make inference more affordable, but they fail to completely shore up this weakness.\n\n\n=== Scaling ===\nThe accuracy and capabilities of foundation models often scale predictably with the size of the model and the amount of the training data. Specifically, scaling laws have been discovered, which are data-based empirical trends that relate resources (data, model size, compute usage) to model capabilities. Particularly, a model's scale is defined by compute, dataset size, and the number of parameters, all of which exhibit a power-law relationship with end performance.\nHowever, broken scaling laws have been discovered in which this relationship smoothly transitions (at points referred to as break(s)) from a power law with one exponent to a power law with another (different) exponent. When one does not collect any points near (or after) the break(s), it can be difficult to obtain an accurate extrapolation.\n\n\n=== Adaptation ===\nFoundation models are inherently multi-purpose: to use these model for a specific use case requires some form of adaptation. At a minimum, models need to be adapted to perform the task of interest (task specification), but often better performance can be achieved by more extensive adaptation to the domain of interest (domain specialization).\nA variety of methods (e.g. prompting, in-context learning, fine-tuning, LoRA) provide different tradeoffs between the costs of adaptation and the extent to which models are specialized. Some major facets to consider when adapting a foundation model are compute budget and data availability. Foundation models can be very large, up to trillions of parameters in size, so adapting the entirety of a foundation model can be computationally expensive. Therefore, developers sometimes adapt only the last neural layer or only the bias vectors to save time and space. For particularly niche applications, specific data may also not be available to adapt the foundation model sufficiently. In such circumstances, data must be manually labeled, which is costly and can demand expert knowledge.\n\n\n=== Evaluation ===\nEvaluation is a key part of developing foundation models. Not only does evaluation allow for tracking progress of high-performance models, it also creates benchmarks for future model development. Stakeholders rely on evaluations to understand model behaviors and gain insight into their various attributes. Traditionally, foundation models are evaluated relative to each other through standardized task benchmarks like MMLU, MMMU, HumanEval, and GSM8K. Given that foundation models are multi-purpose, increasingly meta-benchmarks are developed that aggregate different underlying benchmarks. Examples include LM-Harness, BIG-Bench, HELM, OpenLLM Leaderboard, DecodingTrust, and HEIM.\nSince foundation models' utility depends on their own general capabilities and the performance of fine-tuned applications, evaluation must cover both metrics. Proper evaluation examines both a foundation model's downstream applications in aggregate and the direct properties the foundation model holds. To ensure further equity in evaluation, certain existing evaluation frameworks account for all adaptation resources, which leads to more informed analyses for the benefit of all stakeholders.\n\n\n== Supply chain ==\nFoundation models' general capabilities allow them to fulfill a unique role in the AI ecosystem, fueled by many upstream and downstream technologies. Training a foundation model requires several resources (e.g. data, compute, labor, hardware, code), with foundation models often involving immense amounts of data and compute (also referred to as computational power). Due to foundation models' large development costs and inexpensive adaptation requirements, the AI landscape has shifted to a small subset of AI companies making foundation models for downstream adaptation. Thus, most foundation model companies outsource this step to specialized data providers (e.g. Scale AI, Surge) and compute providers (e.g. Amazon Web Services, Google Cloud, Microsoft Azure).\n\nThe foundation model developer itself will then take the data and use the supplied compute to actually train the foundation model. After the foundation model is completely built, much of the data and labor requirements abate. In this development process, hardware and compute are the most necessary, and also the most exclusive resources. To train larger and more complex AI, a sufficient amount of compute is key. However, compute is consolidated in the hands of a few, select entities, which most foundation model developers depend on. As such, the foundation model pipeline is concentrated heavily around these providers. Compute is also costly; in 2023, AI companies spent more than 80% of total capital on compute resources.\nFoundation models require a large amount of general data to power their capabilities. Early foundation models scraped from subsets of the internet to provide this data information. As the size and scope of foundation models grows, larger quantities of internet scraping becomes necessary, resulting in higher likelihoods of biased or toxic data. This toxic or biased data can disproportionately harm marginalized groups and exacerbate existing prejudices.\nTo address this issue of low-quality data that arose with unsupervised training, some foundation model developers have turned to manual filtering. This practice, known as data labor, comes with its own host of issues. Such manual data detoxification is often outsourced to reduce labor costs, with some workers making less than $2 per hour.\nThe foundation model will then be hosted online either via the developer or via an external organization. Once released, other parties can create applications based on the foundation model, whether through fine-tuning or wholly new purposes. People can then access these applications to serve their various means, allowing one foundation model to power and reach a wide audience.\n\n\n== Release strategies ==\nAfter a foundation model is built, it can be released in one of many ways. There are many facets to a release: the asset itself, who has access, how access changes over time, and the conditions on use. All these factors contribute to how a foundation model will affect downstream applications. In particular, the two most common forms of foundation model release are through APIs and direct model downloads.\nWhen a model is released via an API, users can query the model and receive responses, but cannot directly access the model itself. Comparatively, the model could be directly downloadable for users to access and modify. Both release strategies are often classified as an open release. The exact definition of an open release is disputed, but widely accepted requirements are provided by the Open Source Initiative.\nSome open foundation models are: PaLM 2, Llama 2, Granite, and Mistral. While open foundation models can further research and development more easily, they are also more susceptible to misuse. Open foundation models can be downloaded by anyone, and particularly powerful models can be fine-tuned to intentionally or unintentionally cause harm.\nDuring a closed release, the foundation model cannot be accessed by the public, but is used internally by an organization. Such releases are considered safer, but offer no additional value to the research community or the public at large.\nSome foundation models like Google DeepMind's Flamingo are fully closed, meaning they are available only to the model developer; others, such as OpenAI's GPT-4, are limited access, available to the public but only as a black box; and still others, such as Meta's Llama 2 are open, with broadly available model weights enabling downstream modification and scrutiny.\n\n\n== References ==",
    "categories": [
      "All articles with unsourced statements",
      "Articles with short description",
      "Articles with unsourced statements from July 2025",
      "Computational fields of study",
      "Computational linguistics",
      "Deep learning",
      "Language modeling",
      "Natural language processing",
      "Short description matches Wikidata",
      "Unsupervised learning",
      "Use dmy dates from May 2023"
    ],
    "year_mentioned": 2023
  },
  {
    "title": "Diffusion model",
    "url": "https://en.wikipedia.org/wiki/Diffusion_model",
    "content": "In machine learning, diffusion models, also known as diffusion-based generative models or score-based generative models, are a class of latent variable generative models. A diffusion model consists of two major components: the forward diffusion process, and the reverse sampling process. The goal of diffusion models is to learn a diffusion process for a given dataset, such that the process can generate new elements that are distributed similarly as the original dataset. A diffusion model models data as generated by a diffusion process, whereby a new datum performs a random walk with drift through the space of all possible data. A trained diffusion model can be sampled in many ways, with different efficiency and quality.\nThere are various equivalent formalisms, including Markov chains, denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. They are typically trained using variational inference. The model responsible for denoising is typically called its \"backbone\". The backbone may be of any kind, but they are typically U-nets or transformers.\nAs of 2024, diffusion models are mainly used for computer vision tasks, including image denoising, inpainting, super-resolution, image generation, and video generation. These typically involve training a neural network to sequentially denoise images blurred with Gaussian noise. The model is trained to reverse the process of adding noise to an image. After training to convergence, it can be used for image generation by starting with an image composed of random noise, and applying the network iteratively to denoise the image.\nDiffusion-based image generators have seen widespread commercial interest, such as Stable Diffusion and DALL-E. These models typically combine diffusion models with other models, such as text-encoders and cross-attention modules to allow text-conditioned generation.\nOther than computer vision, diffusion models have also found applications in natural language processing such as text generation and summarization, sound generation, and reinforcement learning.\n\n\n== Denoising diffusion model ==\n\n\n=== Non-equilibrium thermodynamics ===\nDiffusion models were introduced in 2015 as a method to train a model that can sample from a highly complex probability distribution. They used techniques from non-equilibrium thermodynamics, especially diffusion.\nConsider, for example, how one might model the distribution of all naturally occurring photos. Each image is a point in the space of all images, and the distribution of naturally occurring photos is a \"cloud\" in space, which, by repeatedly adding noise to the images, diffuses out to the rest of the image space, until the cloud becomes all but indistinguishable from a Gaussian distribution \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(0,I)}\n  \n. A model that can approximately undo the diffusion can then be used to sample from the original distribution. This is studied in \"non-equilibrium\" thermodynamics, as the starting distribution is not in equilibrium, unlike the final distribution.\nThe equilibrium distribution is the Gaussian distribution \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(0,I)}\n  \n, with pdf \n  \n    \n      \n        ρ\n        (\n        x\n        )\n        ∝\n        \n          e\n          \n            −\n            \n              \n                1\n                2\n              \n            \n            ‖\n            x\n            \n              ‖\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho (x)\\propto e^{-{\\frac {1}{2}}\\|x\\|^{2}}}\n  \n. This is just the Maxwell–Boltzmann distribution of particles in a potential well \n  \n    \n      \n        V\n        (\n        x\n        )\n        =\n        \n          \n            1\n            2\n          \n        \n        ‖\n        x\n        \n          ‖\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle V(x)={\\frac {1}{2}}\\|x\\|^{2}}\n  \n at temperature 1. The initial distribution, being very much out of equilibrium, would diffuse towards the equilibrium distribution, making biased random steps that are a sum of pure randomness (like a Brownian walker) and gradient descent down the potential well. The randomness is necessary: if the particles were to undergo only gradient descent, then they will all fall to the origin, collapsing the distribution.\n\n\n=== Denoising Diffusion Probabilistic Model (DDPM) ===\nThe 2020 paper proposed the Denoising Diffusion Probabilistic Model (DDPM), which improves upon the previous method by variational inference.\n\n\n==== Forward diffusion ====\nTo present the model, some notation is required.\n\n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          β\n          \n            T\n          \n        \n        ∈\n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle \\beta _{1},...,\\beta _{T}\\in (0,1)}\n  \n are fixed constants.\n\n  \n    \n      \n        \n          α\n          \n            t\n          \n        \n        :=\n        1\n        −\n        \n          β\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{t}:=1-\\beta _{t}}\n  \n\n  \n    \n      \n        \n          \n            \n              \n                α\n                ¯\n              \n            \n          \n          \n            t\n          \n        \n        :=\n        \n          α\n          \n            1\n          \n        \n        ⋯\n        \n          α\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle {\\bar {\\alpha }}_{t}:=\\alpha _{1}\\cdots \\alpha _{t}}\n  \n\n  \n    \n      \n        \n          σ\n          \n            t\n          \n        \n        :=\n        \n          \n            1\n            −\n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma _{t}:={\\sqrt {1-{\\bar {\\alpha }}_{t}}}}\n  \n\n  \n    \n      \n        \n          \n            \n              \n                σ\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        :=\n        \n          \n            \n              σ\n              \n                t\n                −\n                1\n              \n            \n            \n              σ\n              \n                t\n              \n            \n          \n        \n        \n          \n            \n              β\n              \n                t\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {\\sigma }}_{t}:={\\frac {\\sigma _{t-1}}{\\sigma _{t}}}{\\sqrt {\\beta _{t}}}}\n  \n\n  \n    \n      \n        \n          \n            \n              \n                μ\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          x\n          \n            0\n          \n        \n        )\n        :=\n        \n          \n            \n              \n                \n                  \n                    α\n                    \n                      t\n                    \n                  \n                \n              \n              (\n              1\n              −\n              \n                \n                  \n                    \n                      α\n                      ¯\n                    \n                  \n                \n                \n                  t\n                  −\n                  1\n                \n              \n              )\n              \n                x\n                \n                  t\n                \n              \n              +\n              \n                \n                  \n                    \n                      \n                        \n                          α\n                          ¯\n                        \n                      \n                    \n                    \n                      t\n                      −\n                      1\n                    \n                  \n                \n              \n              (\n              1\n              −\n              \n                α\n                \n                  t\n                \n              \n              )\n              \n                x\n                \n                  0\n                \n              \n            \n            \n              σ\n              \n                t\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {\\mu }}_{t}(x_{t},x_{0}):={\\frac {{\\sqrt {\\alpha _{t}}}(1-{\\bar {\\alpha }}_{t-1})x_{t}+{\\sqrt {{\\bar {\\alpha }}_{t-1}}}(1-\\alpha _{t})x_{0}}{\\sigma _{t}^{2}}}}\n  \n\n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        μ\n        ,\n        Σ\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(\\mu ,\\Sigma )}\n  \n is the normal distribution with mean \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n and variance \n  \n    \n      \n        Σ\n      \n    \n    {\\displaystyle \\Sigma }\n  \n, and \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        x\n        \n          |\n        \n        μ\n        ,\n        Σ\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(x|\\mu ,\\Sigma )}\n  \n is the probability density at \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n.\nA vertical bar denotes conditioning.\nA forward diffusion process starts at some starting point \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ∼\n        q\n      \n    \n    {\\displaystyle x_{0}\\sim q}\n  \n, where \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n is the probability distribution to be learned, then repeatedly adds noise to it by\n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        =\n        \n          \n            1\n            −\n            \n              β\n              \n                t\n              \n            \n          \n        \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        +\n        \n          \n            \n              β\n              \n                t\n              \n            \n          \n        \n        \n          z\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}={\\sqrt {1-\\beta _{t}}}x_{t-1}+{\\sqrt {\\beta _{t}}}z_{t}}\n  \nwhere \n  \n    \n      \n        \n          z\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          z\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle z_{1},...,z_{T}}\n  \n are IID (Independent and identically distributed random variables) samples from \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(0,I)}\n  \n. The coefficients \n  \n    \n      \n        \n          \n            1\n            −\n            \n              β\n              \n                t\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {1-\\beta _{t}}}}\n  \n and \n  \n    \n      \n        \n          \n            \n              β\n              \n                t\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {\\beta _{t}}}}\n  \n ensure that \n  \n    \n      \n        \n          \n            Var\n          \n        \n        (\n        \n          X\n          \n            t\n          \n        \n        )\n        =\n        I\n      \n    \n    {\\displaystyle {\\mbox{Var}}(X_{t})=I}\n  \n assuming that \n  \n    \n      \n        \n          \n            Var\n          \n        \n        (\n        \n          X\n          \n            0\n          \n        \n        )\n        =\n        I\n      \n    \n    {\\displaystyle {\\mbox{Var}}(X_{0})=I}\n  \n. The values of \n  \n    \n      \n        \n          β\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\beta _{t}}\n  \n are chosen such that for any starting distribution of \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n, if it has finite second moment, then \n  \n    \n      \n        \n          lim\n          \n            t\n            →\n            ∞\n          \n        \n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\lim _{t\\to \\infty }x_{t}|x_{0}}\n  \n converges to \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(0,I)}\n  \n.\nThe entire diffusion process then satisfies\n  \n    \n      \n        q\n        (\n        \n          x\n          \n            0\n            :\n            T\n          \n        \n        )\n        =\n        q\n        (\n        \n          x\n          \n            0\n          \n        \n        )\n        q\n        (\n        \n          x\n          \n            1\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        )\n        ⋯\n        q\n        (\n        \n          x\n          \n            T\n          \n        \n        \n          |\n        \n        \n          x\n          \n            T\n            −\n            1\n          \n        \n        )\n        =\n        q\n        (\n        \n          x\n          \n            0\n          \n        \n        )\n        \n          \n            N\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        \n          |\n        \n        \n          \n            \n              α\n              \n                1\n              \n            \n          \n        \n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          β\n          \n            1\n          \n        \n        I\n        )\n        ⋯\n        \n          \n            N\n          \n        \n        (\n        \n          x\n          \n            T\n          \n        \n        \n          |\n        \n        \n          \n            \n              α\n              \n                T\n              \n            \n          \n        \n        \n          x\n          \n            T\n            −\n            1\n          \n        \n        ,\n        \n          β\n          \n            T\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle q(x_{0:T})=q(x_{0})q(x_{1}|x_{0})\\cdots q(x_{T}|x_{T-1})=q(x_{0}){\\mathcal {N}}(x_{1}|{\\sqrt {\\alpha _{1}}}x_{0},\\beta _{1}I)\\cdots {\\mathcal {N}}(x_{T}|{\\sqrt {\\alpha _{T}}}x_{T-1},\\beta _{T}I)}\n  \nor\n  \n    \n      \n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            0\n            :\n            T\n          \n        \n        )\n        =\n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            0\n          \n        \n        )\n        −\n        \n          ∑\n          \n            t\n            =\n            1\n          \n          \n            T\n          \n        \n        \n          \n            1\n            \n              2\n              \n                β\n                \n                  t\n                \n              \n            \n          \n        \n        ‖\n        \n          x\n          \n            t\n          \n        \n        −\n        \n          \n            1\n            −\n            \n              β\n              \n                t\n              \n            \n          \n        \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        \n          ‖\n          \n            2\n          \n        \n        +\n        C\n      \n    \n    {\\displaystyle \\ln q(x_{0:T})=\\ln q(x_{0})-\\sum _{t=1}^{T}{\\frac {1}{2\\beta _{t}}}\\|x_{t}-{\\sqrt {1-\\beta _{t}}}x_{t-1}\\|^{2}+C}\n  \nwhere \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is a normalization constant and often omitted. In particular, we note that \n  \n    \n      \n        \n          x\n          \n            1\n            :\n            T\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{1:T}|x_{0}}\n  \n is a Gaussian process, which affords us considerable freedom in reparameterization. For example, by standard manipulation with Gaussian process, \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        ∼\n        N\n        \n          (\n          \n            \n              \n                \n                  \n                    \n                      \n                        α\n                        ¯\n                      \n                    \n                  \n                  \n                    t\n                  \n                \n              \n            \n            \n              x\n              \n                0\n              \n            \n            ,\n            \n              σ\n              \n                t\n              \n              \n                2\n              \n            \n            I\n          \n          )\n        \n      \n    \n    {\\displaystyle x_{t}|x_{0}\\sim N\\left({\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0},\\sigma _{t}^{2}I\\right)}\n  \n\n  \n    \n      \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          x\n          \n            0\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          \n            \n              \n                μ\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          x\n          \n            0\n          \n        \n        )\n        ,\n        \n          \n            \n              \n                σ\n                ~\n              \n            \n          \n          \n            t\n          \n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle x_{t-1}|x_{t},x_{0}\\sim {\\mathcal {N}}({\\tilde {\\mu }}_{t}(x_{t},x_{0}),{\\tilde {\\sigma }}_{t}^{2}I)}\n  \nIn particular, notice that for large \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, the variable \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        ∼\n        N\n        \n          (\n          \n            \n              \n                \n                  \n                    \n                      \n                        α\n                        ¯\n                      \n                    \n                  \n                  \n                    t\n                  \n                \n              \n            \n            \n              x\n              \n                0\n              \n            \n            ,\n            \n              σ\n              \n                t\n              \n              \n                2\n              \n            \n            I\n          \n          )\n        \n      \n    \n    {\\displaystyle x_{t}|x_{0}\\sim N\\left({\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0},\\sigma _{t}^{2}I\\right)}\n  \n converges to \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(0,I)}\n  \n. That is, after a long enough diffusion process, we end up with some \n  \n    \n      \n        \n          x\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle x_{T}}\n  \n that is very close to \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(0,I)}\n  \n, with all traces of the original \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ∼\n        q\n      \n    \n    {\\displaystyle x_{0}\\sim q}\n  \n gone.\nFor example, since\n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        ∼\n        N\n        \n          (\n          \n            \n              \n                \n                  \n                    \n                      \n                        α\n                        ¯\n                      \n                    \n                  \n                  \n                    t\n                  \n                \n              \n            \n            \n              x\n              \n                0\n              \n            \n            ,\n            \n              σ\n              \n                t\n              \n              \n                2\n              \n            \n            I\n          \n          )\n        \n      \n    \n    {\\displaystyle x_{t}|x_{0}\\sim N\\left({\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0},\\sigma _{t}^{2}I\\right)}\n  \nwe can sample \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{t}|x_{0}}\n  \n directly \"in one step\", instead of going through all the intermediate steps \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            t\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},...,x_{t-1}}\n  \n.\n\n\n==== Backward diffusion ====\nThe key idea of DDPM is to use a neural network parametrized by \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. The network takes in two arguments \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n      \n    \n    {\\displaystyle x_{t},t}\n  \n, and outputs a vector \n  \n    \n      \n        \n          μ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\mu _{\\theta }(x_{t},t)}\n  \n and a matrix \n  \n    \n      \n        \n          Σ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\Sigma _{\\theta }(x_{t},t)}\n  \n, such that each step in the forward diffusion process can be approximately undone by \n  \n    \n      \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          μ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        ,\n        \n          Σ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        )\n      \n    \n    {\\displaystyle x_{t-1}\\sim {\\mathcal {N}}(\\mu _{\\theta }(x_{t},t),\\Sigma _{\\theta }(x_{t},t))}\n  \n. This then gives us a backward diffusion process \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle p_{\\theta }}\n  \n defined by\n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            T\n          \n        \n        )\n        =\n        \n          \n            N\n          \n        \n        (\n        \n          x\n          \n            T\n          \n        \n        \n          |\n        \n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x_{T})={\\mathcal {N}}(x_{T}|0,I)}\n  \n\n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        )\n        =\n        \n          \n            N\n          \n        \n        (\n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        \n          |\n        \n        \n          μ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        ,\n        \n          Σ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x_{t-1}|x_{t})={\\mathcal {N}}(x_{t-1}|\\mu _{\\theta }(x_{t},t),\\Sigma _{\\theta }(x_{t},t))}\n  \nThe goal now is to learn the parameters such that \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x_{0})}\n  \n is as close to \n  \n    \n      \n        q\n        (\n        \n          x\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle q(x_{0})}\n  \n as possible. To do that, we use maximum likelihood estimation with variational inference.\n\n\n==== Variational inference ====\nThe ELBO inequality states that \n  \n    \n      \n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            0\n          \n        \n        )\n        ≥\n        \n          E\n          \n            \n              x\n              \n                1\n                :\n                T\n              \n            \n            ∼\n            q\n            (\n            ⋅\n            \n              |\n            \n            \n              x\n              \n                0\n              \n            \n            )\n          \n        \n        [\n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            0\n            :\n            T\n          \n        \n        )\n        −\n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            1\n            :\n            T\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle \\ln p_{\\theta }(x_{0})\\geq E_{x_{1:T}\\sim q(\\cdot |x_{0})}[\\ln p_{\\theta }(x_{0:T})-\\ln q(x_{1:T}|x_{0})]}\n  \n, and taking one more expectation, we get\n  \n    \n      \n        \n          E\n          \n            \n              x\n              \n                0\n              \n            \n            ∼\n            q\n          \n        \n        [\n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            0\n          \n        \n        )\n        ]\n        ≥\n        \n          E\n          \n            \n              x\n              \n                0\n                :\n                T\n              \n            \n            ∼\n            q\n          \n        \n        [\n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            0\n            :\n            T\n          \n        \n        )\n        −\n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            1\n            :\n            T\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle E_{x_{0}\\sim q}[\\ln p_{\\theta }(x_{0})]\\geq E_{x_{0:T}\\sim q}[\\ln p_{\\theta }(x_{0:T})-\\ln q(x_{1:T}|x_{0})]}\n  \nWe see that maximizing the quantity on the right would give us a lower bound on the likelihood of observed data. This allows us to perform variational inference.\nDefine the loss function\n  \n    \n      \n        L\n        (\n        θ\n        )\n        :=\n        −\n        \n          E\n          \n            \n              x\n              \n                0\n                :\n                T\n              \n            \n            ∼\n            q\n          \n        \n        [\n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            0\n            :\n            T\n          \n        \n        )\n        −\n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            1\n            :\n            T\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle L(\\theta ):=-E_{x_{0:T}\\sim q}[\\ln p_{\\theta }(x_{0:T})-\\ln q(x_{1:T}|x_{0})]}\n  \nand now the goal is to minimize the loss by stochastic gradient descent. The expression may be simplified to\n  \n    \n      \n        L\n        (\n        θ\n        )\n        =\n        \n          ∑\n          \n            t\n            =\n            1\n          \n          \n            T\n          \n        \n        \n          E\n          \n            \n              x\n              \n                t\n                −\n                1\n              \n            \n            ,\n            \n              x\n              \n                t\n              \n            \n            ∼\n            q\n          \n        \n        [\n        −\n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        )\n        ]\n        +\n        \n          E\n          \n            \n              x\n              \n                0\n              \n            \n            ∼\n            q\n          \n        \n        [\n        \n          D\n          \n            K\n            L\n          \n        \n        (\n        q\n        (\n        \n          x\n          \n            T\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        )\n        ‖\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            T\n          \n        \n        )\n        )\n        ]\n        +\n        C\n      \n    \n    {\\displaystyle L(\\theta )=\\sum _{t=1}^{T}E_{x_{t-1},x_{t}\\sim q}[-\\ln p_{\\theta }(x_{t-1}|x_{t})]+E_{x_{0}\\sim q}[D_{KL}(q(x_{T}|x_{0})\\|p_{\\theta }(x_{T}))]+C}\n  \nwhere \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n does not depend on the parameter, and thus can be ignored. Since \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            T\n          \n        \n        )\n        =\n        \n          \n            N\n          \n        \n        (\n        \n          x\n          \n            T\n          \n        \n        \n          |\n        \n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x_{T})={\\mathcal {N}}(x_{T}|0,I)}\n  \n also does not depend on the parameter, the term \n  \n    \n      \n        \n          E\n          \n            \n              x\n              \n                0\n              \n            \n            ∼\n            q\n          \n        \n        [\n        \n          D\n          \n            K\n            L\n          \n        \n        (\n        q\n        (\n        \n          x\n          \n            T\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        )\n        ‖\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            T\n          \n        \n        )\n        )\n        ]\n      \n    \n    {\\displaystyle E_{x_{0}\\sim q}[D_{KL}(q(x_{T}|x_{0})\\|p_{\\theta }(x_{T}))]}\n  \n can also be ignored. This leaves just \n  \n    \n      \n        L\n        (\n        θ\n        )\n        =\n        \n          ∑\n          \n            t\n            =\n            1\n          \n          \n            T\n          \n        \n        \n          L\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle L(\\theta )=\\sum _{t=1}^{T}L_{t}}\n  \n with \n  \n    \n      \n        \n          L\n          \n            t\n          \n        \n        =\n        \n          E\n          \n            \n              x\n              \n                t\n                −\n                1\n              \n            \n            ,\n            \n              x\n              \n                t\n              \n            \n            ∼\n            q\n          \n        \n        [\n        −\n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle L_{t}=E_{x_{t-1},x_{t}\\sim q}[-\\ln p_{\\theta }(x_{t-1}|x_{t})]}\n  \n to be minimized.\n\n\n==== Noise prediction network ====\nSince \n  \n    \n      \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          x\n          \n            0\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          \n            \n              \n                μ\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          x\n          \n            0\n          \n        \n        )\n        ,\n        \n          \n            \n              \n                σ\n                ~\n              \n            \n          \n          \n            t\n          \n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle x_{t-1}|x_{t},x_{0}\\sim {\\mathcal {N}}({\\tilde {\\mu }}_{t}(x_{t},x_{0}),{\\tilde {\\sigma }}_{t}^{2}I)}\n  \n, this suggests that we should use \n  \n    \n      \n        \n          μ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        =\n        \n          \n            \n              \n                μ\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          x\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mu _{\\theta }(x_{t},t)={\\tilde {\\mu }}_{t}(x_{t},x_{0})}\n  \n; however, the network does not have access to \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n, and so it has to estimate it instead. Now, since \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        ∼\n        N\n        \n          (\n          \n            \n              \n                \n                  \n                    \n                      \n                        α\n                        ¯\n                      \n                    \n                  \n                  \n                    t\n                  \n                \n              \n            \n            \n              x\n              \n                0\n              \n            \n            ,\n            \n              σ\n              \n                t\n              \n              \n                2\n              \n            \n            I\n          \n          )\n        \n      \n    \n    {\\displaystyle x_{t}|x_{0}\\sim N\\left({\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0},\\sigma _{t}^{2}I\\right)}\n  \n, we may write \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n        \n          x\n          \n            0\n          \n        \n        +\n        \n          σ\n          \n            t\n          \n        \n        z\n      \n    \n    {\\displaystyle x_{t}={\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0}+\\sigma _{t}z}\n  \n, where \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n is some unknown Gaussian noise. Now we see that estimating \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n is equivalent to estimating \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n.\nTherefore, let the network output a noise vector \n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},t)}\n  \n, and let it predict\n  \n    \n      \n        \n          μ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        =\n        \n          \n            \n              \n                μ\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        \n          (\n          \n            \n              x\n              \n                t\n              \n            \n            ,\n            \n              \n                \n                  \n                    x\n                    \n                      t\n                    \n                  \n                  −\n                  \n                    σ\n                    \n                      t\n                    \n                  \n                  \n                    ϵ\n                    \n                      θ\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      t\n                    \n                  \n                  ,\n                  t\n                  )\n                \n                \n                  \n                    \n                      \n                        \n                          α\n                          ¯\n                        \n                      \n                    \n                    \n                      t\n                    \n                  \n                \n              \n            \n          \n          )\n        \n        =\n        \n          \n            \n              \n                x\n                \n                  t\n                \n              \n              −\n              \n                ϵ\n                \n                  θ\n                \n              \n              (\n              \n                x\n                \n                  t\n                \n              \n              ,\n              t\n              )\n              \n                β\n                \n                  t\n                \n              \n              \n                /\n              \n              \n                σ\n                \n                  t\n                \n              \n            \n            \n              \n                α\n                \n                  t\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\theta }(x_{t},t)={\\tilde {\\mu }}_{t}\\left(x_{t},{\\frac {x_{t}-\\sigma _{t}\\epsilon _{\\theta }(x_{t},t)}{\\sqrt {{\\bar {\\alpha }}_{t}}}}\\right)={\\frac {x_{t}-\\epsilon _{\\theta }(x_{t},t)\\beta _{t}/\\sigma _{t}}{\\sqrt {\\alpha _{t}}}}}\n  \nIt remains to design \n  \n    \n      \n        \n          Σ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\Sigma _{\\theta }(x_{t},t)}\n  \n. The DDPM paper suggested not learning it (since it resulted in \"unstable training and poorer sample quality\"), but fixing it at some value \n  \n    \n      \n        \n          Σ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        =\n        \n          ζ\n          \n            t\n          \n          \n            2\n          \n        \n        I\n      \n    \n    {\\displaystyle \\Sigma _{\\theta }(x_{t},t)=\\zeta _{t}^{2}I}\n  \n, where either \n  \n    \n      \n        \n          ζ\n          \n            t\n          \n          \n            2\n          \n        \n        =\n        \n          β\n          \n            t\n          \n        \n        \n           or \n        \n        \n          \n            \n              \n                σ\n                ~\n              \n            \n          \n          \n            t\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\zeta _{t}^{2}=\\beta _{t}{\\text{ or }}{\\tilde {\\sigma }}_{t}^{2}}\n  \n yielded similar performance.\nWith this, the loss simplifies to \n  \n    \n      \n        \n          L\n          \n            t\n          \n        \n        =\n        \n          \n            \n              β\n              \n                t\n              \n              \n                2\n              \n            \n            \n              2\n              \n                α\n                \n                  t\n                \n              \n              \n                σ\n                \n                  t\n                \n                \n                  2\n                \n              \n              \n                ζ\n                \n                  t\n                \n                \n                  2\n                \n              \n            \n          \n        \n        \n          E\n          \n            \n              x\n              \n                0\n              \n            \n            ∼\n            q\n            ;\n            z\n            ∼\n            \n              \n                N\n              \n            \n            (\n            0\n            ,\n            I\n            )\n          \n        \n        \n          [\n          \n            \n              ‖\n              \n                \n                  ϵ\n                  \n                    θ\n                  \n                \n                (\n                \n                  x\n                  \n                    t\n                  \n                \n                ,\n                t\n                )\n                −\n                z\n              \n              ‖\n            \n            \n              2\n            \n          \n          ]\n        \n        +\n        C\n      \n    \n    {\\displaystyle L_{t}={\\frac {\\beta _{t}^{2}}{2\\alpha _{t}\\sigma _{t}^{2}\\zeta _{t}^{2}}}E_{x_{0}\\sim q;z\\sim {\\mathcal {N}}(0,I)}\\left[\\left\\|\\epsilon _{\\theta }(x_{t},t)-z\\right\\|^{2}\\right]+C}\n  \nwhich may be minimized by stochastic gradient descent. The paper noted empirically that an even simpler loss function\n  \n    \n      \n        \n          L\n          \n            s\n            i\n            m\n            p\n            l\n            e\n            ,\n            t\n          \n        \n        =\n        \n          E\n          \n            \n              x\n              \n                0\n              \n            \n            ∼\n            q\n            ;\n            z\n            ∼\n            \n              \n                N\n              \n            \n            (\n            0\n            ,\n            I\n            )\n          \n        \n        \n          [\n          \n            \n              ‖\n              \n                \n                  ϵ\n                  \n                    θ\n                  \n                \n                (\n                \n                  x\n                  \n                    t\n                  \n                \n                ,\n                t\n                )\n                −\n                z\n              \n              ‖\n            \n            \n              2\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle L_{simple,t}=E_{x_{0}\\sim q;z\\sim {\\mathcal {N}}(0,I)}\\left[\\left\\|\\epsilon _{\\theta }(x_{t},t)-z\\right\\|^{2}\\right]}\n  \nresulted in better models.\n\n\n=== Backward diffusion process ===\nAfter a noise prediction network is trained, it can be used for generating data points in the original distribution in a loop as follows:\n\nCompute the noise estimate \n  \n    \n      \n        ϵ\n        ←\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\epsilon \\leftarrow \\epsilon _{\\theta }(x_{t},t)}\n  \n\nCompute the original data estimate \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            0\n          \n        \n        ←\n        (\n        \n          x\n          \n            t\n          \n        \n        −\n        \n          σ\n          \n            t\n          \n        \n        ϵ\n        )\n        \n          /\n        \n        \n          \n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{0}\\leftarrow (x_{t}-\\sigma _{t}\\epsilon )/{\\sqrt {{\\bar {\\alpha }}_{t}}}}\n  \n\nSample the previous data \n  \n    \n      \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          \n            \n              \n                μ\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            0\n          \n        \n        )\n        ,\n        \n          \n            \n              \n                σ\n                ~\n              \n            \n          \n          \n            t\n          \n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle x_{t-1}\\sim {\\mathcal {N}}({\\tilde {\\mu }}_{t}(x_{t},{\\tilde {x}}_{0}),{\\tilde {\\sigma }}_{t}^{2}I)}\n  \n\nChange time \n  \n    \n      \n        t\n        ←\n        t\n        −\n        1\n      \n    \n    {\\displaystyle t\\leftarrow t-1}\n  \n\n\n== Score-based generative model ==\nScore-based generative model is another formulation of diffusion modelling. They are also called noise conditional score network (NCSN) or score-matching with Langevin dynamics (SMLD).\n\n\n=== Score matching ===\n\n\n==== The idea of score functions ====\nConsider the problem of image generation. Let \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n represent an image, and let \n  \n    \n      \n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle q(x)}\n  \n be the probability distribution over all possible images. If we have \n  \n    \n      \n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle q(x)}\n  \n itself, then we can say for certain how likely a certain image is. However, this is intractable in general.\nMost often, we are uninterested in knowing the absolute probability of a certain image. Instead, we are usually only interested in knowing how likely a certain image is compared to its immediate neighbors — e.g. how much more likely is an image of cat compared to some small variants of it? Is it more likely if the image contains two whiskers, or three, or with some Gaussian noise added?\nConsequently, we are actually quite uninterested in \n  \n    \n      \n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle q(x)}\n  \n itself, but rather, \n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\nabla _{x}\\ln q(x)}\n  \n. This has two major effects:\n\nOne, we no longer need to normalize \n  \n    \n      \n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle q(x)}\n  \n, but can use any \n  \n    \n      \n        \n          \n            \n              q\n              ~\n            \n          \n        \n        (\n        x\n        )\n        =\n        C\n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\tilde {q}}(x)=Cq(x)}\n  \n, where \n  \n    \n      \n        C\n        =\n        ∫\n        \n          \n            \n              q\n              ~\n            \n          \n        \n        (\n        x\n        )\n        d\n        x\n        >\n        0\n      \n    \n    {\\displaystyle C=\\int {\\tilde {q}}(x)dx>0}\n  \n is any unknown constant that is of no concern to us.\nTwo, we are comparing \n  \n    \n      \n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle q(x)}\n  \n neighbors \n  \n    \n      \n        q\n        (\n        x\n        +\n        d\n        x\n        )\n      \n    \n    {\\displaystyle q(x+dx)}\n  \n, by \n  \n    \n      \n        \n          \n            \n              q\n              (\n              x\n              )\n            \n            \n              q\n              (\n              x\n              +\n              d\n              x\n              )\n            \n          \n        \n        =\n        \n          e\n          \n            −\n            ⟨\n            \n              ∇\n              \n                x\n              \n            \n            ln\n            ⁡\n            q\n            ,\n            d\n            x\n            ⟩\n          \n        \n      \n    \n    {\\displaystyle {\\frac {q(x)}{q(x+dx)}}=e^{-\\langle \\nabla _{x}\\ln q,dx\\rangle }}\n  \n\nLet the score function be \n  \n    \n      \n        s\n        (\n        x\n        )\n        :=\n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle s(x):=\\nabla _{x}\\ln q(x)}\n  \n; then consider what we can do with \n  \n    \n      \n        s\n        (\n        x\n        )\n      \n    \n    {\\displaystyle s(x)}\n  \n.\nAs it turns out, \n  \n    \n      \n        s\n        (\n        x\n        )\n      \n    \n    {\\displaystyle s(x)}\n  \n allows us to sample from \n  \n    \n      \n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle q(x)}\n  \n using thermodynamics. Specifically, if we have a potential energy function \n  \n    \n      \n        U\n        (\n        x\n        )\n        =\n        −\n        ln\n        ⁡\n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle U(x)=-\\ln q(x)}\n  \n, and a lot of particles in the potential well, then the distribution at thermodynamic equilibrium is the Boltzmann distribution \n  \n    \n      \n        \n          q\n          \n            U\n          \n        \n        (\n        x\n        )\n        ∝\n        \n          e\n          \n            −\n            U\n            (\n            x\n            )\n            \n              /\n            \n            \n              k\n              \n                B\n              \n            \n            T\n          \n        \n        =\n        q\n        (\n        x\n        \n          )\n          \n            1\n            \n              /\n            \n            \n              k\n              \n                B\n              \n            \n            T\n          \n        \n      \n    \n    {\\displaystyle q_{U}(x)\\propto e^{-U(x)/k_{B}T}=q(x)^{1/k_{B}T}}\n  \n. At temperature \n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n        T\n        =\n        1\n      \n    \n    {\\displaystyle k_{B}T=1}\n  \n, the Boltzmann distribution is exactly \n  \n    \n      \n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle q(x)}\n  \n.\nTherefore, to model \n  \n    \n      \n        q\n        (\n        x\n        )\n      \n    \n    {\\displaystyle q(x)}\n  \n, we may start with a particle sampled at any convenient distribution (such as the standard Gaussian distribution), then simulate the motion of the particle forwards according to the Langevin equation\n\n  \n    \n      \n        d\n        \n          x\n          \n            t\n          \n        \n        =\n        −\n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        U\n        (\n        \n          x\n          \n            t\n          \n        \n        )\n        d\n        t\n        +\n        d\n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle dx_{t}=-\\nabla _{x_{t}}U(x_{t})dt+dW_{t}}\n  \n\nand the Boltzmann distribution is, by Fokker-Planck equation, the unique thermodynamic equilibrium. So no matter what distribution \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n has, the distribution of \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n converges in distribution to \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n as \n  \n    \n      \n        t\n        →\n        ∞\n      \n    \n    {\\displaystyle t\\to \\infty }\n  \n.\n\n\n==== Learning the score function ====\nGiven a density \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n, we wish to learn a score function approximation \n  \n    \n      \n        \n          f\n          \n            θ\n          \n        \n        ≈\n        ∇\n        ln\n        ⁡\n        q\n      \n    \n    {\\displaystyle f_{\\theta }\\approx \\nabla \\ln q}\n  \n. This is score matching. Typically, score matching is formalized as minimizing Fisher divergence function \n  \n    \n      \n        \n          E\n          \n            q\n          \n        \n        [\n        ‖\n        \n          f\n          \n            θ\n          \n        \n        (\n        x\n        )\n        −\n        ∇\n        ln\n        ⁡\n        q\n        (\n        x\n        )\n        \n          ‖\n          \n            2\n          \n        \n        ]\n      \n    \n    {\\displaystyle E_{q}[\\|f_{\\theta }(x)-\\nabla \\ln q(x)\\|^{2}]}\n  \n. By expanding the integral, and performing an integration by parts, \n  \n    \n      \n        \n          E\n          \n            q\n          \n        \n        [\n        ‖\n        \n          f\n          \n            θ\n          \n        \n        (\n        x\n        )\n        −\n        ∇\n        ln\n        ⁡\n        q\n        (\n        x\n        )\n        \n          ‖\n          \n            2\n          \n        \n        ]\n        =\n        \n          E\n          \n            q\n          \n        \n        [\n        ‖\n        \n          f\n          \n            θ\n          \n        \n        \n          ‖\n          \n            2\n          \n        \n        +\n        2\n        ∇\n        ⋅\n        \n          f\n          \n            θ\n          \n        \n        ]\n        +\n        C\n      \n    \n    {\\displaystyle E_{q}[\\|f_{\\theta }(x)-\\nabla \\ln q(x)\\|^{2}]=E_{q}[\\|f_{\\theta }\\|^{2}+2\\nabla \\cdot f_{\\theta }]+C}\n  \ngiving us a loss function, also known as the Hyvärinen scoring rule, that can be minimized by stochastic gradient descent.\n\n\n==== Annealing the score function ====\nSuppose we need to model the distribution of images, and we want \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle x_{0}\\sim {\\mathcal {N}}(0,I)}\n  \n, a white-noise image. Now, most white-noise images do not look like real images, so \n  \n    \n      \n        q\n        (\n        \n          x\n          \n            0\n          \n        \n        )\n        ≈\n        0\n      \n    \n    {\\displaystyle q(x_{0})\\approx 0}\n  \n for large swaths of \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle x_{0}\\sim {\\mathcal {N}}(0,I)}\n  \n. This presents a problem for learning the score function, because if there are no samples around a certain point, then we can't learn the score function at that point. If we do not know the score function \n  \n    \n      \n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle \\nabla _{x_{t}}\\ln q(x_{t})}\n  \n at that point, then we cannot impose the time-evolution equation on a particle:\n  \n    \n      \n        d\n        \n          x\n          \n            t\n          \n        \n        =\n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            t\n          \n        \n        )\n        d\n        t\n        +\n        d\n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle dx_{t}=\\nabla _{x_{t}}\\ln q(x_{t})dt+dW_{t}}\n  \nTo deal with this problem, we perform annealing. If \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n is too different from a white-noise distribution, then progressively add noise until it is indistinguishable from one. That is, we perform a forward diffusion, then learn the score function, then use the score function to perform a backward diffusion.\n\n\n=== Continuous diffusion processes ===\n\n\n==== Forward diffusion process ====\nConsider again the forward diffusion process, but this time in continuous time:\n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        =\n        \n          \n            1\n            −\n            \n              β\n              \n                t\n              \n            \n          \n        \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        +\n        \n          \n            \n              β\n              \n                t\n              \n            \n          \n        \n        \n          z\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}={\\sqrt {1-\\beta _{t}}}x_{t-1}+{\\sqrt {\\beta _{t}}}z_{t}}\n  \nBy taking the \n  \n    \n      \n        \n          β\n          \n            t\n          \n        \n        →\n        β\n        (\n        t\n        )\n        d\n        t\n        ,\n        \n          \n            d\n            t\n          \n        \n        \n          z\n          \n            t\n          \n        \n        →\n        d\n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\beta _{t}\\to \\beta (t)dt,{\\sqrt {dt}}z_{t}\\to dW_{t}}\n  \n limit, we obtain a continuous diffusion process, in the form of a stochastic differential equation:\n  \n    \n      \n        d\n        \n          x\n          \n            t\n          \n        \n        =\n        −\n        \n          \n            1\n            2\n          \n        \n        β\n        (\n        t\n        )\n        \n          x\n          \n            t\n          \n        \n        d\n        t\n        +\n        \n          \n            β\n            (\n            t\n            )\n          \n        \n        d\n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle dx_{t}=-{\\frac {1}{2}}\\beta (t)x_{t}dt+{\\sqrt {\\beta (t)}}dW_{t}}\n  \nwhere \n  \n    \n      \n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle W_{t}}\n  \n is a Wiener process (multidimensional Brownian motion).\nNow, the equation is exactly a special case of the overdamped Langevin equation\n  \n    \n      \n        d\n        \n          x\n          \n            t\n          \n        \n        =\n        −\n        \n          \n            D\n            \n              \n                k\n                \n                  B\n                \n              \n              T\n            \n          \n        \n        (\n        \n          ∇\n          \n            x\n          \n        \n        U\n        )\n        d\n        t\n        +\n        \n          \n            2\n            D\n          \n        \n        d\n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle dx_{t}=-{\\frac {D}{k_{B}T}}(\\nabla _{x}U)dt+{\\sqrt {2D}}dW_{t}}\n  \nwhere \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is diffusion tensor, \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is temperature, and \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n is potential energy field. If we substitute in \n  \n    \n      \n        D\n        =\n        \n          \n            1\n            2\n          \n        \n        β\n        (\n        t\n        )\n        I\n        ,\n        \n          k\n          \n            B\n          \n        \n        T\n        =\n        1\n        ,\n        U\n        =\n        \n          \n            1\n            2\n          \n        \n        ‖\n        x\n        \n          ‖\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle D={\\frac {1}{2}}\\beta (t)I,k_{B}T=1,U={\\frac {1}{2}}\\|x\\|^{2}}\n  \n, we recover the above equation. This explains why the phrase \"Langevin dynamics\" is sometimes used in diffusion models.\nNow the above equation is for the stochastic motion of a single particle. Suppose we have a cloud of particles distributed according to \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n at time \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n, then after a long time, the cloud of particles would settle into the stable distribution of \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(0,I)}\n  \n. Let \n  \n    \n      \n        \n          ρ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\rho _{t}}\n  \n be the density of the cloud of particles at time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, then we have\n  \n    \n      \n        \n          ρ\n          \n            0\n          \n        \n        =\n        q\n        ;\n        \n        \n          ρ\n          \n            T\n          \n        \n        ≈\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle \\rho _{0}=q;\\quad \\rho _{T}\\approx {\\mathcal {N}}(0,I)}\n  \nand the goal is to somehow reverse the process, so that we can start at the end and diffuse back to the beginning.\nBy Fokker-Planck equation, the density of the cloud evolves according to\n  \n    \n      \n        \n          ∂\n          \n            t\n          \n        \n        ln\n        ⁡\n        \n          ρ\n          \n            t\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        β\n        (\n        t\n        )\n        \n          (\n          \n            n\n            +\n            (\n            x\n            +\n            ∇\n            ln\n            ⁡\n            \n              ρ\n              \n                t\n              \n            \n            )\n            ⋅\n            ∇\n            ln\n            ⁡\n            \n              ρ\n              \n                t\n              \n            \n            +\n            Δ\n            ln\n            ⁡\n            \n              ρ\n              \n                t\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\partial _{t}\\ln \\rho _{t}={\\frac {1}{2}}\\beta (t)\\left(n+(x+\\nabla \\ln \\rho _{t})\\cdot \\nabla \\ln \\rho _{t}+\\Delta \\ln \\rho _{t}\\right)}\n  \nwhere \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is the dimension of space, and \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n is the Laplace operator. Equivalently,\n  \n    \n      \n        \n          ∂\n          \n            t\n          \n        \n        \n          ρ\n          \n            t\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        β\n        (\n        t\n        )\n        (\n        ∇\n        ⋅\n        (\n        x\n        \n          ρ\n          \n            t\n          \n        \n        )\n        +\n        Δ\n        \n          ρ\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle \\partial _{t}\\rho _{t}={\\frac {1}{2}}\\beta (t)(\\nabla \\cdot (x\\rho _{t})+\\Delta \\rho _{t})}\n  \n\n\n==== Backward diffusion process ====\nIf we have solved \n  \n    \n      \n        \n          ρ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\rho _{t}}\n  \n for time \n  \n    \n      \n        t\n        ∈\n        [\n        0\n        ,\n        T\n        ]\n      \n    \n    {\\displaystyle t\\in [0,T]}\n  \n, then we can exactly reverse the evolution of the cloud. Suppose we start with another cloud of particles with density \n  \n    \n      \n        \n          ν\n          \n            0\n          \n        \n        =\n        \n          ρ\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\nu _{0}=\\rho _{T}}\n  \n, and let the particles in the cloud evolve according to\n\n  \n    \n      \n        d\n        \n          y\n          \n            t\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        β\n        (\n        T\n        −\n        t\n        )\n        \n          y\n          \n            t\n          \n        \n        d\n        t\n        +\n        β\n        (\n        T\n        −\n        t\n        )\n        \n          \n            \n              \n                \n                  ∇\n                  \n                    \n                      y\n                      \n                        t\n                      \n                    \n                  \n                \n                ln\n                ⁡\n                \n                  ρ\n                  \n                    T\n                    −\n                    t\n                  \n                \n                \n                  (\n                  \n                    y\n                    \n                      t\n                    \n                  \n                  )\n                \n              \n              ⏟\n            \n          \n          \n            score function \n          \n        \n        d\n        t\n        +\n        \n          \n            β\n            (\n            T\n            −\n            t\n            )\n          \n        \n        d\n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle dy_{t}={\\frac {1}{2}}\\beta (T-t)y_{t}dt+\\beta (T-t)\\underbrace {\\nabla _{y_{t}}\\ln \\rho _{T-t}\\left(y_{t}\\right)} _{\\text{score function }}dt+{\\sqrt {\\beta (T-t)}}dW_{t}}\n  \n\nthen by plugging into the Fokker-Planck equation, we find that \n  \n    \n      \n        \n          ∂\n          \n            t\n          \n        \n        \n          ρ\n          \n            T\n            −\n            t\n          \n        \n        =\n        \n          ∂\n          \n            t\n          \n        \n        \n          ν\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\partial _{t}\\rho _{T-t}=\\partial _{t}\\nu _{t}}\n  \n. Thus this cloud of points is the original cloud, evolving backwards.\n\n\n=== Noise conditional score network (NCSN) ===\nAt the continuous limit, \n\n  \n    \n      \n        \n          \n            \n              \n                α\n                ¯\n              \n            \n          \n          \n            t\n          \n        \n        =\n        (\n        1\n        −\n        \n          β\n          \n            1\n          \n        \n        )\n        ⋯\n        (\n        1\n        −\n        \n          β\n          \n            t\n          \n        \n        )\n        =\n        \n          e\n          \n            \n              ∑\n              \n                i\n              \n            \n            ln\n            ⁡\n            (\n            1\n            −\n            \n              β\n              \n                i\n              \n            \n            )\n          \n        \n        →\n        \n          e\n          \n            −\n            \n              ∫\n              \n                0\n              \n              \n                t\n              \n            \n            β\n            (\n            t\n            )\n            d\n            t\n          \n        \n      \n    \n    {\\displaystyle {\\bar {\\alpha }}_{t}=(1-\\beta _{1})\\cdots (1-\\beta _{t})=e^{\\sum _{i}\\ln(1-\\beta _{i})}\\to e^{-\\int _{0}^{t}\\beta (t)dt}}\n  \n\nand so \n\n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        ∼\n        N\n        \n          (\n          \n            \n              e\n              \n                −\n                \n                  \n                    1\n                    2\n                  \n                \n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    t\n                  \n                \n                β\n                (\n                t\n                )\n                d\n                t\n              \n            \n            \n              x\n              \n                0\n              \n            \n            ,\n            \n              (\n              \n                1\n                −\n                \n                  e\n                  \n                    −\n                    \n                      ∫\n                      \n                        0\n                      \n                      \n                        t\n                      \n                    \n                    β\n                    (\n                    t\n                    )\n                    d\n                    t\n                  \n                \n              \n              )\n            \n            I\n          \n          )\n        \n      \n    \n    {\\displaystyle x_{t}|x_{0}\\sim N\\left(e^{-{\\frac {1}{2}}\\int _{0}^{t}\\beta (t)dt}x_{0},\\left(1-e^{-\\int _{0}^{t}\\beta (t)dt}\\right)I\\right)}\n  \n\nIn particular, we see that we can directly sample from any point in the continuous diffusion process without going through the intermediate steps, by first sampling \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ∼\n        q\n        ,\n        z\n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle x_{0}\\sim q,z\\sim {\\mathcal {N}}(0,I)}\n  \n, then get \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        =\n        \n          e\n          \n            −\n            \n              \n                1\n                2\n              \n            \n            \n              ∫\n              \n                0\n              \n              \n                t\n              \n            \n            β\n            (\n            t\n            )\n            d\n            t\n          \n        \n        \n          x\n          \n            0\n          \n        \n        +\n        \n          (\n          \n            1\n            −\n            \n              e\n              \n                −\n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    t\n                  \n                \n                β\n                (\n                t\n                )\n                d\n                t\n              \n            \n          \n          )\n        \n        z\n      \n    \n    {\\displaystyle x_{t}=e^{-{\\frac {1}{2}}\\int _{0}^{t}\\beta (t)dt}x_{0}+\\left(1-e^{-\\int _{0}^{t}\\beta (t)dt}\\right)z}\n  \n. That is, we can quickly sample \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        ∼\n        \n          ρ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}\\sim \\rho _{t}}\n  \n for any \n  \n    \n      \n        t\n        ≥\n        0\n      \n    \n    {\\displaystyle t\\geq 0}\n  \n.\nNow, define a certain probability distribution \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n over \n  \n    \n      \n        [\n        0\n        ,\n        ∞\n        )\n      \n    \n    {\\displaystyle [0,\\infty )}\n  \n, then the score-matching loss function is defined as the expected Fisher divergence:\n\n  \n    \n      \n        L\n        (\n        θ\n        )\n        =\n        \n          E\n          \n            t\n            ∼\n            γ\n            ,\n            \n              x\n              \n                t\n              \n            \n            ∼\n            \n              ρ\n              \n                t\n              \n            \n          \n        \n        [\n        ‖\n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        \n          ‖\n          \n            2\n          \n        \n        +\n        2\n        ∇\n        ⋅\n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        ]\n      \n    \n    {\\displaystyle L(\\theta )=E_{t\\sim \\gamma ,x_{t}\\sim \\rho _{t}}[\\|f_{\\theta }(x_{t},t)\\|^{2}+2\\nabla \\cdot f_{\\theta }(x_{t},t)]}\n  \n\nAfter training, \n  \n    \n      \n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        ≈\n        ∇\n        ln\n        ⁡\n        \n          ρ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle f_{\\theta }(x_{t},t)\\approx \\nabla \\ln \\rho _{t}}\n  \n, so we can perform the backwards diffusion process by first sampling \n  \n    \n      \n        \n          x\n          \n            T\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle x_{T}\\sim {\\mathcal {N}}(0,I)}\n  \n, then integrating the SDE from \n  \n    \n      \n        t\n        =\n        T\n      \n    \n    {\\displaystyle t=T}\n  \n to \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n:\n\n  \n    \n      \n        \n          x\n          \n            t\n            −\n            d\n            t\n          \n        \n        =\n        \n          x\n          \n            t\n          \n        \n        +\n        \n          \n            1\n            2\n          \n        \n        β\n        (\n        t\n        )\n        \n          x\n          \n            t\n          \n        \n        d\n        t\n        +\n        β\n        (\n        t\n        )\n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        d\n        t\n        +\n        \n          \n            β\n            (\n            t\n            )\n          \n        \n        d\n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t-dt}=x_{t}+{\\frac {1}{2}}\\beta (t)x_{t}dt+\\beta (t)f_{\\theta }(x_{t},t)dt+{\\sqrt {\\beta (t)}}dW_{t}}\n  \n\nThis may be done by any SDE integration method, such as Euler–Maruyama method.\nThe name \"noise conditional score network\" is explained thus:\n\n\"network\", because \n  \n    \n      \n        \n          f\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle f_{\\theta }}\n  \n is implemented as a neural network.\n\"score\", because the output of the network is interpreted as approximating the score function \n  \n    \n      \n        ∇\n        ln\n        ⁡\n        \n          ρ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\nabla \\ln \\rho _{t}}\n  \n.\n\"noise conditional\", because \n  \n    \n      \n        \n          ρ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\rho _{t}}\n  \n is equal to \n  \n    \n      \n        \n          ρ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\rho _{0}}\n  \n blurred by an added Gaussian noise that increases with time, and so the score function depends on the amount of noise added.\n\n\n== Their equivalence ==\nDDPM and score-based generative models are equivalent. This means that a network trained using DDPM can be used as a NCSN, and vice versa.\nWe know that \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n        ∼\n        N\n        \n          (\n          \n            \n              \n                \n                  \n                    \n                      \n                        α\n                        ¯\n                      \n                    \n                  \n                  \n                    t\n                  \n                \n              \n            \n            \n              x\n              \n                0\n              \n            \n            ,\n            \n              σ\n              \n                t\n              \n              \n                2\n              \n            \n            I\n          \n          )\n        \n      \n    \n    {\\displaystyle x_{t}|x_{0}\\sim N\\left({\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0},\\sigma _{t}^{2}I\\right)}\n  \n, so by Tweedie's formula, we have\n\n  \n    \n      \n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            t\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              σ\n              \n                t\n              \n              \n                2\n              \n            \n          \n        \n        (\n        −\n        \n          x\n          \n            t\n          \n        \n        +\n        \n          \n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n        \n          E\n          \n            q\n          \n        \n        [\n        \n          x\n          \n            0\n          \n        \n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        ]\n        )\n      \n    \n    {\\displaystyle \\nabla _{x_{t}}\\ln q(x_{t})={\\frac {1}{\\sigma _{t}^{2}}}(-x_{t}+{\\sqrt {{\\bar {\\alpha }}_{t}}}E_{q}[x_{0}|x_{t}])}\n  \n\nAs described previously, the DDPM loss function is \n  \n    \n      \n        \n          ∑\n          \n            t\n          \n        \n        \n          L\n          \n            s\n            i\n            m\n            p\n            l\n            e\n            ,\n            t\n          \n        \n      \n    \n    {\\displaystyle \\sum _{t}L_{simple,t}}\n  \n with\n\n  \n    \n      \n        \n          L\n          \n            s\n            i\n            m\n            p\n            l\n            e\n            ,\n            t\n          \n        \n        =\n        \n          E\n          \n            \n              x\n              \n                0\n              \n            \n            ∼\n            q\n            ;\n            z\n            ∼\n            \n              \n                N\n              \n            \n            (\n            0\n            ,\n            I\n            )\n          \n        \n        \n          [\n          \n            \n              ‖\n              \n                \n                  ϵ\n                  \n                    θ\n                  \n                \n                (\n                \n                  x\n                  \n                    t\n                  \n                \n                ,\n                t\n                )\n                −\n                z\n              \n              ‖\n            \n            \n              2\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle L_{simple,t}=E_{x_{0}\\sim q;z\\sim {\\mathcal {N}}(0,I)}\\left[\\left\\|\\epsilon _{\\theta }(x_{t},t)-z\\right\\|^{2}\\right]}\n  \n\nwhere \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n        \n          x\n          \n            0\n          \n        \n        +\n        \n          σ\n          \n            t\n          \n        \n        z\n      \n    \n    {\\displaystyle x_{t}={\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0}+\\sigma _{t}z}\n  \n. By a change of variables,\n\n  \n    \n      \n        \n          L\n          \n            s\n            i\n            m\n            p\n            l\n            e\n            ,\n            t\n          \n        \n        =\n        \n          E\n          \n            \n              x\n              \n                0\n              \n            \n            ,\n            \n              x\n              \n                t\n              \n            \n            ∼\n            q\n          \n        \n        \n          [\n          \n            \n              ‖\n              \n                \n                  ϵ\n                  \n                    θ\n                  \n                \n                (\n                \n                  x\n                  \n                    t\n                  \n                \n                ,\n                t\n                )\n                −\n                \n                  \n                    \n                      \n                        x\n                        \n                          t\n                        \n                      \n                      −\n                      \n                        \n                          \n                            \n                              \n                                \n                                  α\n                                  ¯\n                                \n                              \n                            \n                            \n                              t\n                            \n                          \n                        \n                      \n                      \n                        x\n                        \n                          0\n                        \n                      \n                    \n                    \n                      σ\n                      \n                        t\n                      \n                    \n                  \n                \n              \n              ‖\n            \n            \n              2\n            \n          \n          ]\n        \n        =\n        \n          E\n          \n            \n              x\n              \n                t\n              \n            \n            ∼\n            q\n            ,\n            \n              x\n              \n                0\n              \n            \n            ∼\n            q\n            (\n            ⋅\n            \n              |\n            \n            \n              x\n              \n                t\n              \n            \n            )\n          \n        \n        \n          [\n          \n            \n              ‖\n              \n                \n                  ϵ\n                  \n                    θ\n                  \n                \n                (\n                \n                  x\n                  \n                    t\n                  \n                \n                ,\n                t\n                )\n                −\n                \n                  \n                    \n                      \n                        x\n                        \n                          t\n                        \n                      \n                      −\n                      \n                        \n                          \n                            \n                              \n                                \n                                  α\n                                  ¯\n                                \n                              \n                            \n                            \n                              t\n                            \n                          \n                        \n                      \n                      \n                        x\n                        \n                          0\n                        \n                      \n                    \n                    \n                      σ\n                      \n                        t\n                      \n                    \n                  \n                \n              \n              ‖\n            \n            \n              2\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle L_{simple,t}=E_{x_{0},x_{t}\\sim q}\\left[\\left\\|\\epsilon _{\\theta }(x_{t},t)-{\\frac {x_{t}-{\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0}}{\\sigma _{t}}}\\right\\|^{2}\\right]=E_{x_{t}\\sim q,x_{0}\\sim q(\\cdot |x_{t})}\\left[\\left\\|\\epsilon _{\\theta }(x_{t},t)-{\\frac {x_{t}-{\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0}}{\\sigma _{t}}}\\right\\|^{2}\\right]}\n  \n\nand the term inside becomes a least squares regression, so if the network actually reaches the global minimum of loss, then we have \n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        =\n        \n          \n            \n              \n                x\n                \n                  t\n                \n              \n              −\n              \n                \n                  \n                    \n                      \n                        \n                          α\n                          ¯\n                        \n                      \n                    \n                    \n                      t\n                    \n                  \n                \n              \n              \n                E\n                \n                  q\n                \n              \n              [\n              \n                x\n                \n                  0\n                \n              \n              \n                |\n              \n              \n                x\n                \n                  t\n                \n              \n              ]\n            \n            \n              σ\n              \n                t\n              \n            \n          \n        \n        =\n        −\n        \n          σ\n          \n            t\n          \n        \n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},t)={\\frac {x_{t}-{\\sqrt {{\\bar {\\alpha }}_{t}}}E_{q}[x_{0}|x_{t}]}{\\sigma _{t}}}=-\\sigma _{t}\\nabla _{x_{t}}\\ln q(x_{t})}\n  \n\nThus, given a good score-based network, its predicted score is a good prediction of the noise (after scaling by \n  \n    \n      \n        \n          σ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{t}}\n  \n), and thus can be used for denoising.\nConversely, the continuous limit \n  \n    \n      \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        =\n        \n          x\n          \n            t\n            −\n            d\n            t\n          \n        \n        ,\n        \n          β\n          \n            t\n          \n        \n        =\n        β\n        (\n        t\n        )\n        d\n        t\n        ,\n        \n          z\n          \n            t\n          \n        \n        \n          \n            d\n            t\n          \n        \n        =\n        d\n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t-1}=x_{t-dt},\\beta _{t}=\\beta (t)dt,z_{t}{\\sqrt {dt}}=dW_{t}}\n  \n of the backward equation\n\n  \n    \n      \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        =\n        \n          \n            \n              x\n              \n                t\n              \n            \n            \n              \n                α\n                \n                  t\n                \n              \n            \n          \n        \n        −\n        \n          \n            \n              β\n              \n                t\n              \n            \n            \n              \n                σ\n                \n                  t\n                \n              \n              \n                \n                  \n                    α\n                    \n                      t\n                    \n                  \n                \n              \n            \n          \n        \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        +\n        \n          \n            \n              β\n              \n                t\n              \n            \n          \n        \n        \n          z\n          \n            t\n          \n        \n        ;\n        \n        \n          z\n          \n            t\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle x_{t-1}={\\frac {x_{t}}{\\sqrt {\\alpha _{t}}}}-{\\frac {\\beta _{t}}{\\sigma _{t}{\\sqrt {\\alpha _{t}}}}}\\epsilon _{\\theta }(x_{t},t)+{\\sqrt {\\beta _{t}}}z_{t};\\quad z_{t}\\sim {\\mathcal {N}}(0,I)}\n  \n\ngives us precisely the same equation as score-based diffusion:\n\n  \n    \n      \n        \n          x\n          \n            t\n            −\n            d\n            t\n          \n        \n        =\n        \n          x\n          \n            t\n          \n        \n        (\n        1\n        +\n        β\n        (\n        t\n        )\n        d\n        t\n        \n          /\n        \n        2\n        )\n        +\n        β\n        (\n        t\n        )\n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        q\n        (\n        \n          x\n          \n            t\n          \n        \n        )\n        d\n        t\n        +\n        \n          \n            β\n            (\n            t\n            )\n          \n        \n        d\n        \n          W\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t-dt}=x_{t}(1+\\beta (t)dt/2)+\\beta (t)\\nabla _{x_{t}}\\ln q(x_{t})dt+{\\sqrt {\\beta (t)}}dW_{t}}\n  \nThus, at infinitesimal steps of DDPM, a denoising network performs score-based diffusion.\n\n\n== Main variants ==\n\n\n=== Noise schedule ===\n\nIn DDPM, the sequence of numbers \n  \n    \n      \n        0\n        =\n        \n          σ\n          \n            0\n          \n        \n        <\n        \n          σ\n          \n            1\n          \n        \n        <\n        ⋯\n        <\n        \n          σ\n          \n            T\n          \n        \n        <\n        1\n      \n    \n    {\\displaystyle 0=\\sigma _{0}<\\sigma _{1}<\\cdots <\\sigma _{T}<1}\n  \n is called a (discrete time) noise schedule. In general, consider a strictly increasing monotonic function \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n of type \n  \n    \n      \n        \n          R\n        \n        →\n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle \\mathbb {R} \\to (0,1)}\n  \n, such as the sigmoid function. In that case, a noise schedule is a sequence of real numbers \n  \n    \n      \n        \n          λ\n          \n            1\n          \n        \n        <\n        \n          λ\n          \n            2\n          \n        \n        <\n        ⋯\n        <\n        \n          λ\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{1}<\\lambda _{2}<\\cdots <\\lambda _{T}}\n  \n. It then defines a sequence of noises \n  \n    \n      \n        \n          σ\n          \n            t\n          \n        \n        :=\n        σ\n        (\n        \n          λ\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle \\sigma _{t}:=\\sigma (\\lambda _{t})}\n  \n, which then derives the other quantities \n  \n    \n      \n        \n          β\n          \n            t\n          \n        \n        =\n        1\n        −\n        \n          \n            \n              1\n              −\n              \n                σ\n                \n                  t\n                \n                \n                  2\n                \n              \n            \n            \n              1\n              −\n              \n                σ\n                \n                  t\n                  −\n                  1\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\beta _{t}=1-{\\frac {1-\\sigma _{t}^{2}}{1-\\sigma _{t-1}^{2}}}}\n  \n.\nIn order to use arbitrary noise schedules, instead of training a noise prediction model \n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},t)}\n  \n, one trains \n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          σ\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},\\sigma _{t})}\n  \n.\nSimilarly, for the noise conditional score network, instead of training \n  \n    \n      \n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle f_{\\theta }(x_{t},t)}\n  \n, one trains \n  \n    \n      \n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          σ\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle f_{\\theta }(x_{t},\\sigma _{t})}\n  \n.\n\n\n=== Denoising Diffusion Implicit Model (DDIM) ===\nThe original DDPM method for generating images is slow, since the forward diffusion process usually takes \n  \n    \n      \n        T\n        ∼\n        1000\n      \n    \n    {\\displaystyle T\\sim 1000}\n  \n to make the distribution of \n  \n    \n      \n        \n          x\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle x_{T}}\n  \n to appear close to Gaussian. However this means the backward diffusion process also take 1000 steps. Unlike the forward diffusion process, which can skip steps as \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{t}|x_{0}}\n  \n is Gaussian for all \n  \n    \n      \n        t\n        ≥\n        1\n      \n    \n    {\\displaystyle t\\geq 1}\n  \n, the backward diffusion process does not allow skipping steps. For example, to sample \n  \n    \n      \n        \n          x\n          \n            t\n            −\n            2\n          \n        \n        \n          |\n        \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          μ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        ,\n        t\n        −\n        1\n        )\n        ,\n        \n          Σ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        ,\n        t\n        −\n        1\n        )\n        )\n      \n    \n    {\\displaystyle x_{t-2}|x_{t-1}\\sim {\\mathcal {N}}(\\mu _{\\theta }(x_{t-1},t-1),\\Sigma _{\\theta }(x_{t-1},t-1))}\n  \n requires the model to first sample \n  \n    \n      \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle x_{t-1}}\n  \n. Attempting to directly sample \n  \n    \n      \n        \n          x\n          \n            t\n            −\n            2\n          \n        \n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t-2}|x_{t}}\n  \n would require us to marginalize out \n  \n    \n      \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle x_{t-1}}\n  \n, which is generally intractable.\nDDIM is a method to take any model trained on DDPM loss, and use it to sample with some steps skipped, sacrificing an adjustable amount of quality. If we generate the Markovian chain case in DDPM to non-Markovian case, DDIM corresponds to the case that the reverse process has variance equals to 0. In other words, the reverse process (and also the forward process) is deterministic. When using fewer sampling steps, DDIM outperforms DDPM.\nIn detail, the DDIM sampling method is as follows. Start with the forward diffusion process \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n        \n          x\n          \n            0\n          \n        \n        +\n        \n          σ\n          \n            t\n          \n        \n        ϵ\n      \n    \n    {\\displaystyle x_{t}={\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0}+\\sigma _{t}\\epsilon }\n  \n. Then, during the backward denoising process, given \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle x_{t},\\epsilon _{\\theta }(x_{t},t)}\n  \n, the original data is estimated as \n  \n    \n      \n        \n          x\n          \n            0\n          \n          ′\n        \n        =\n        \n          \n            \n              \n                x\n                \n                  t\n                \n              \n              −\n              \n                σ\n                \n                  t\n                \n              \n              \n                ϵ\n                \n                  θ\n                \n              \n              (\n              \n                x\n                \n                  t\n                \n              \n              ,\n              t\n              )\n            \n            \n              \n                \n                  \n                    \n                      α\n                      ¯\n                    \n                  \n                \n                \n                  t\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle x_{0}'={\\frac {x_{t}-\\sigma _{t}\\epsilon _{\\theta }(x_{t},t)}{\\sqrt {{\\bar {\\alpha }}_{t}}}}}\n  \nthen the backward diffusion process can jump to any step \n  \n    \n      \n        0\n        ≤\n        s\n        <\n        t\n      \n    \n    {\\displaystyle 0\\leq s<t}\n  \n, and the next denoised sample is \n  \n    \n      \n        \n          x\n          \n            s\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                s\n              \n            \n          \n        \n        \n          x\n          \n            0\n          \n          ′\n        \n        +\n        \n          \n            \n              σ\n              \n                s\n              \n              \n                2\n              \n            \n            −\n            (\n            \n              σ\n              \n                s\n              \n              ′\n            \n            \n              )\n              \n                2\n              \n            \n          \n        \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        +\n        \n          σ\n          \n            s\n          \n          ′\n        \n        ϵ\n      \n    \n    {\\displaystyle x_{s}={\\sqrt {{\\bar {\\alpha }}_{s}}}x_{0}'+{\\sqrt {\\sigma _{s}^{2}-(\\sigma '_{s})^{2}}}\\epsilon _{\\theta }(x_{t},t)+\\sigma _{s}'\\epsilon }\n  \nwhere \n  \n    \n      \n        \n          σ\n          \n            s\n          \n          ′\n        \n      \n    \n    {\\displaystyle \\sigma _{s}'}\n  \n is an arbitrary real number within the range \n  \n    \n      \n        [\n        0\n        ,\n        \n          σ\n          \n            s\n          \n        \n        ]\n      \n    \n    {\\displaystyle [0,\\sigma _{s}]}\n  \n, and \n  \n    \n      \n        ϵ\n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle \\epsilon \\sim {\\mathcal {N}}(0,I)}\n  \n is a newly sampled Gaussian noise. If all \n  \n    \n      \n        \n          σ\n          \n            s\n          \n          ′\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\sigma _{s}'=0}\n  \n, then the backward process becomes deterministic, and this special case of DDIM is also called \"DDIM\". The original paper noted that when the process is deterministic, samples generated with only 20 steps are already very similar to ones generated with 1000 steps on the high-level.\nThe original paper recommended defining a single \"eta value\" \n  \n    \n      \n        η\n        ∈\n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle \\eta \\in [0,1]}\n  \n, such that \n  \n    \n      \n        \n          σ\n          \n            s\n          \n          ′\n        \n        =\n        η\n        \n          \n            \n              \n                σ\n                ~\n              \n            \n          \n          \n            s\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{s}'=\\eta {\\tilde {\\sigma }}_{s}}\n  \n. When \n  \n    \n      \n        η\n        =\n        1\n      \n    \n    {\\displaystyle \\eta =1}\n  \n, this is the original DDPM. When \n  \n    \n      \n        η\n        =\n        0\n      \n    \n    {\\displaystyle \\eta =0}\n  \n, this is the fully deterministic DDIM. For intermediate values, the process interpolates between them.\nBy the equivalence, the DDIM algorithm also applies for score-based diffusion models.\n\n\n=== Latent diffusion model (LDM) ===\n\nSince the diffusion model is a general method for modelling probability distributions, if one wants to model a distribution over images, one can first encode the images into a lower-dimensional space by an encoder, then use a diffusion model to model the distribution over encoded images. Then to generate an image, one can sample from the diffusion model, then use a decoder to decode it into an image.\nThe encoder-decoder pair is most often a variational autoencoder (VAE).\n\n\n=== Architectural improvements ===\n proposed various architectural improvements. For example, they proposed log-space interpolation during backward sampling. Instead of sampling from \n  \n    \n      \n        \n          x\n          \n            t\n            −\n            1\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          \n            \n              \n                μ\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            0\n          \n        \n        )\n        ,\n        \n          \n            \n              \n                σ\n                ~\n              \n            \n          \n          \n            t\n          \n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle x_{t-1}\\sim {\\mathcal {N}}({\\tilde {\\mu }}_{t}(x_{t},{\\tilde {x}}_{0}),{\\tilde {\\sigma }}_{t}^{2}I)}\n  \n, they recommended sampling from \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        \n          \n            \n              \n                μ\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            0\n          \n        \n        )\n        ,\n        (\n        \n          σ\n          \n            t\n          \n          \n            v\n          \n        \n        \n          \n            \n              \n                σ\n                ~\n              \n            \n          \n          \n            t\n          \n          \n            1\n            −\n            v\n          \n        \n        \n          )\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}({\\tilde {\\mu }}_{t}(x_{t},{\\tilde {x}}_{0}),(\\sigma _{t}^{v}{\\tilde {\\sigma }}_{t}^{1-v})^{2}I)}\n  \n for a learned parameter \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n.\nIn the v-prediction formalism, the noising formula \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n        \n          x\n          \n            0\n          \n        \n        +\n        \n          \n            1\n            −\n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n        \n          ϵ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}={\\sqrt {{\\bar {\\alpha }}_{t}}}x_{0}+{\\sqrt {1-{\\bar {\\alpha }}_{t}}}\\epsilon _{t}}\n  \n is reparameterised by an angle \n  \n    \n      \n        \n          ϕ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\phi _{t}}\n  \n such that \n  \n    \n      \n        cos\n        ⁡\n        \n          ϕ\n          \n            t\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\cos \\phi _{t}={\\sqrt {{\\bar {\\alpha }}_{t}}}}\n  \n and a \"velocity\" defined by \n  \n    \n      \n        cos\n        ⁡\n        \n          ϕ\n          \n            t\n          \n        \n        \n          ϵ\n          \n            t\n          \n        \n        −\n        sin\n        ⁡\n        \n          ϕ\n          \n            t\n          \n        \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\cos \\phi _{t}\\epsilon _{t}-\\sin \\phi _{t}x_{0}}\n  \n. The network is trained to predict the velocity \n  \n    \n      \n        \n          \n            \n              \n                v\n                ^\n              \n            \n          \n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle {\\hat {v}}_{\\theta }}\n  \n, and denoising is by \n  \n    \n      \n        \n          x\n          \n            \n              ϕ\n              \n                t\n              \n            \n            −\n            δ\n          \n        \n        =\n        cos\n        ⁡\n        (\n        δ\n        )\n        \n        \n          x\n          \n            \n              ϕ\n              \n                t\n              \n            \n          \n        \n        −\n        sin\n        ⁡\n        (\n        δ\n        )\n        \n          \n            \n              \n                v\n                ^\n              \n            \n          \n          \n            θ\n          \n        \n        \n        (\n        \n          x\n          \n            \n              ϕ\n              \n                t\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle x_{\\phi _{t}-\\delta }=\\cos(\\delta )\\;x_{\\phi _{t}}-\\sin(\\delta ){\\hat {v}}_{\\theta }\\;(x_{\\phi _{t}})}\n  \n. This parameterization was found to improve performance, as the model can be trained to reach total noise (i.e. \n  \n    \n      \n        \n          ϕ\n          \n            t\n          \n        \n        =\n        \n          90\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle \\phi _{t}=90^{\\circ }}\n  \n) and then reverse it, whereas the standard parameterization never reaches total noise since \n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    α\n                    ¯\n                  \n                \n              \n              \n                t\n              \n            \n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle {\\sqrt {{\\bar {\\alpha }}_{t}}}>0}\n  \n is always true.\n\n\n=== Classifier guidance ===\nClassifier guidance was proposed in 2021 to improve class-conditional generation by using a classifier. The original publication used CLIP text encoders to improve text-conditional image generation.\nSuppose we wish to sample not from the entire distribution of images, but conditional on the image description. We don't want to sample a generic image, but an image that fits the description \"black cat with red eyes\". Generally, we want to sample from the distribution \n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n      \n    \n    {\\displaystyle p(x|y)}\n  \n, where \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n ranges over images, and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n ranges over classes of images (a description \"black cat with red eyes\" is just a very detailed class, and a class \"cat\" is just a very vague description).\nTaking the perspective of the noisy channel model, we can understand the process as follows: To generate an image \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n conditional on description \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n, we imagine that the requester really had in mind an image \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, but the image is passed through a noisy channel and came out garbled, as \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n. Image generation is then nothing but inferring which \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n the requester had in mind.\nIn other words, conditional image generation is simply \"translating from a textual language into a pictorial language\". Then, as in noisy-channel model, we use Bayes theorem to get\n\n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n        ∝\n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle p(x|y)\\propto p(y|x)p(x)}\n  \n\nin other words, if we have a good model of the space of all images, and a good image-to-class translator, we get a class-to-image translator \"for free\". In the equation for backward diffusion, the score \n  \n    \n      \n        ∇\n        ln\n        ⁡\n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\nabla \\ln p(x)}\n  \n can be replaced by\n\n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n        =\n        \n          \n            \n              \n                \n                  ∇\n                  \n                    x\n                  \n                \n                ln\n                ⁡\n                p\n                (\n                x\n                )\n              \n              ⏟\n            \n          \n          \n            score\n          \n        \n        +\n        \n          \n            \n              \n                \n                  ∇\n                  \n                    x\n                  \n                \n                ln\n                ⁡\n                p\n                (\n                y\n                \n                  |\n                \n                x\n                )\n              \n              ⏟\n            \n          \n          \n            classifier guidance\n          \n        \n      \n    \n    {\\displaystyle \\nabla _{x}\\ln p(x|y)=\\underbrace {\\nabla _{x}\\ln p(x)} _{\\text{score}}+\\underbrace {\\nabla _{x}\\ln p(y|x)} _{\\text{classifier guidance}}}\n  \n\nwhere \n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\nabla _{x}\\ln p(x)}\n  \n is the score function, trained as previously described, and \n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle \\nabla _{x}\\ln p(y|x)}\n  \n is found by using a differentiable image classifier.\nDuring the diffusion process, we need to condition on the time, giving\n  \n    \n      \n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        p\n        (\n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        y\n        ,\n        t\n        )\n        =\n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        p\n        (\n        y\n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        +\n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        p\n        (\n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        t\n        )\n      \n    \n    {\\displaystyle \\nabla _{x_{t}}\\ln p(x_{t}|y,t)=\\nabla _{x_{t}}\\ln p(y|x_{t},t)+\\nabla _{x_{t}}\\ln p(x_{t}|t)}\n  \nAlthough, usually the classifier model does not depend on time, in which case \n  \n    \n      \n        p\n        (\n        y\n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        =\n        p\n        (\n        y\n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle p(y|x_{t},t)=p(y|x_{t})}\n  \n.\nClassifier guidance is defined for the gradient of score function, thus for score-based diffusion network, but as previously noted, score-based diffusion models are equivalent to denoising models by \n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        =\n        −\n        \n          σ\n          \n            t\n          \n        \n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        p\n        (\n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        t\n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},t)=-\\sigma _{t}\\nabla _{x_{t}}\\ln p(x_{t}|t)}\n  \n, and similarly, \n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        y\n        ,\n        t\n        )\n        =\n        −\n        \n          σ\n          \n            t\n          \n        \n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        p\n        (\n        \n          x\n          \n            t\n          \n        \n        \n          |\n        \n        y\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},y,t)=-\\sigma _{t}\\nabla _{x_{t}}\\ln p(x_{t}|y,t)}\n  \n. Therefore, classifier guidance works for denoising diffusion as well, using the modified noise prediction:\n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        y\n        ,\n        t\n        )\n        =\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        −\n        \n          \n            \n              \n                \n                  σ\n                  \n                    t\n                  \n                \n                \n                  ∇\n                  \n                    \n                      x\n                      \n                        t\n                      \n                    \n                  \n                \n                ln\n                ⁡\n                p\n                (\n                y\n                \n                  |\n                \n                \n                  x\n                  \n                    t\n                  \n                \n                ,\n                t\n                )\n              \n              ⏟\n            \n          \n          \n            classifier guidance\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},y,t)=\\epsilon _{\\theta }(x_{t},t)-\\underbrace {\\sigma _{t}\\nabla _{x_{t}}\\ln p(y|x_{t},t)} _{\\text{classifier guidance}}}\n  \n\n\n==== With temperature ====\nThe classifier-guided diffusion model samples from \n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n      \n    \n    {\\displaystyle p(x|y)}\n  \n, which is concentrated around the maximum a posteriori estimate \n  \n    \n      \n        arg\n        ⁡\n        \n          max\n          \n            x\n          \n        \n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n      \n    \n    {\\displaystyle \\arg \\max _{x}p(x|y)}\n  \n. If we want to force the model to move towards the maximum likelihood estimate \n  \n    \n      \n        arg\n        ⁡\n        \n          max\n          \n            x\n          \n        \n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle \\arg \\max _{x}p(y|x)}\n  \n, we can use \n\n  \n    \n      \n        \n          p\n          \n            γ\n          \n        \n        (\n        x\n        \n          |\n        \n        y\n        )\n        ∝\n        p\n        (\n        y\n        \n          |\n        \n        x\n        \n          )\n          \n            γ\n          \n        \n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle p_{\\gamma }(x|y)\\propto p(y|x)^{\\gamma }p(x)}\n  \n\nwhere \n  \n    \n      \n        γ\n        >\n        0\n      \n    \n    {\\displaystyle \\gamma >0}\n  \n is interpretable as inverse temperature. In the context of diffusion models, it is usually called the guidance scale. A high \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n would force the model to sample from a distribution concentrated around \n  \n    \n      \n        arg\n        ⁡\n        \n          max\n          \n            x\n          \n        \n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle \\arg \\max _{x}p(y|x)}\n  \n. This sometimes improves quality of generated images.\nThis gives a modification to the previous equation:\n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        \n          p\n          \n            β\n          \n        \n        (\n        x\n        \n          |\n        \n        y\n        )\n        =\n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        x\n        )\n        +\n        γ\n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle \\nabla _{x}\\ln p_{\\beta }(x|y)=\\nabla _{x}\\ln p(x)+\\gamma \\nabla _{x}\\ln p(y|x)}\n  \nFor denoising models, it corresponds to\n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        y\n        ,\n        t\n        )\n        =\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        −\n        γ\n        \n          σ\n          \n            t\n          \n        \n        \n          ∇\n          \n            \n              x\n              \n                t\n              \n            \n          \n        \n        ln\n        ⁡\n        p\n        (\n        y\n        \n          |\n        \n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},y,t)=\\epsilon _{\\theta }(x_{t},t)-\\gamma \\sigma _{t}\\nabla _{x_{t}}\\ln p(y|x_{t},t)}\n  \n\n\n=== Classifier-free guidance (CFG) ===\nIf we do not have a classifier \n  \n    \n      \n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p(y|x)}\n  \n, we could still extract one out of the image model itself:\n\n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        \n          p\n          \n            γ\n          \n        \n        (\n        x\n        \n          |\n        \n        y\n        )\n        =\n        (\n        1\n        −\n        γ\n        )\n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        x\n        )\n        +\n        γ\n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n      \n    \n    {\\displaystyle \\nabla _{x}\\ln p_{\\gamma }(x|y)=(1-\\gamma )\\nabla _{x}\\ln p(x)+\\gamma \\nabla _{x}\\ln p(x|y)}\n  \n\nSuch a model is usually trained by presenting it with both \n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle (x,y)}\n  \n and \n  \n    \n      \n        (\n        x\n        ,\n        \n          \n            N\n            o\n            n\n            e\n          \n        \n        )\n      \n    \n    {\\displaystyle (x,{\\rm {None}})}\n  \n, allowing it to model both \n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n      \n    \n    {\\displaystyle \\nabla _{x}\\ln p(x|y)}\n  \n and \n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\nabla _{x}\\ln p(x)}\n  \n.\nNote that for CFG, the diffusion model cannot be merely a generative model of the entire data distribution \n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\nabla _{x}\\ln p(x)}\n  \n. It must be a conditional generative model \n  \n    \n      \n        \n          ∇\n          \n            x\n          \n        \n        ln\n        ⁡\n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n      \n    \n    {\\displaystyle \\nabla _{x}\\ln p(x|y)}\n  \n. For example, in stable diffusion, the diffusion backbone takes as input both a noisy model \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n, a time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, and a conditioning vector \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n (such as a vector encoding a text prompt), and produces a noise prediction \n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        y\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},y,t)}\n  \n.\nFor denoising models, it corresponds to\n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        y\n        ,\n        t\n        ,\n        γ\n        )\n        =\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        +\n        γ\n        (\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        y\n        ,\n        t\n        )\n        −\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},y,t,\\gamma )=\\epsilon _{\\theta }(x_{t},t)+\\gamma (\\epsilon _{\\theta }(x_{t},y,t)-\\epsilon _{\\theta }(x_{t},t))}\n  \nAs sampled by DDIM, the algorithm can be written as\n  \n    \n      \n        \n          \n            \n              \n                \n                  ϵ\n                  \n                    uncond\n                  \n                \n              \n              \n                \n                ←\n                \n                  ϵ\n                  \n                    θ\n                  \n                \n                (\n                \n                  x\n                  \n                    t\n                  \n                \n                ,\n                t\n                )\n              \n            \n            \n              \n                \n                  ϵ\n                  \n                    cond\n                  \n                \n              \n              \n                \n                ←\n                \n                  ϵ\n                  \n                    θ\n                  \n                \n                (\n                \n                  x\n                  \n                    t\n                  \n                \n                ,\n                t\n                ,\n                c\n                )\n              \n            \n            \n              \n                \n                  ϵ\n                  \n                    CFG\n                  \n                \n              \n              \n                \n                ←\n                \n                  ϵ\n                  \n                    uncond\n                  \n                \n                +\n                γ\n                (\n                \n                  ϵ\n                  \n                    cond\n                  \n                \n                −\n                \n                  ϵ\n                  \n                    uncond\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  x\n                  \n                    0\n                  \n                \n              \n              \n                \n                ←\n                (\n                \n                  x\n                  \n                    t\n                  \n                \n                −\n                \n                  σ\n                  \n                    t\n                  \n                \n                \n                  ϵ\n                  \n                    CFG\n                  \n                \n                )\n                \n                  /\n                \n                \n                  \n                    1\n                    −\n                    \n                      σ\n                      \n                        t\n                      \n                      \n                        2\n                      \n                    \n                  \n                \n              \n            \n            \n              \n                \n                  x\n                  \n                    s\n                  \n                \n              \n              \n                \n                ←\n                \n                  \n                    1\n                    −\n                    \n                      σ\n                      \n                        s\n                      \n                      \n                        2\n                      \n                    \n                  \n                \n                \n                  x\n                  \n                    0\n                  \n                \n                +\n                \n                  \n                    \n                      σ\n                      \n                        s\n                      \n                      \n                        2\n                      \n                    \n                    −\n                    (\n                    \n                      σ\n                      \n                        s\n                      \n                      ′\n                    \n                    \n                      )\n                      \n                        2\n                      \n                    \n                  \n                \n                \n                  ϵ\n                  \n                    uncond\n                  \n                \n                +\n                \n                  σ\n                  \n                    s\n                  \n                  ′\n                \n                ϵ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\epsilon _{\\text{uncond}}&\\leftarrow \\epsilon _{\\theta }(x_{t},t)\\\\\\epsilon _{\\text{cond}}&\\leftarrow \\epsilon _{\\theta }(x_{t},t,c)\\\\\\epsilon _{\\text{CFG}}&\\leftarrow \\epsilon _{\\text{uncond}}+\\gamma (\\epsilon _{\\text{cond}}-\\epsilon _{\\text{uncond}})\\\\x_{0}&\\leftarrow (x_{t}-\\sigma _{t}\\epsilon _{\\text{CFG}})/{\\sqrt {1-\\sigma _{t}^{2}}}\\\\x_{s}&\\leftarrow {\\sqrt {1-\\sigma _{s}^{2}}}x_{0}+{\\sqrt {\\sigma _{s}^{2}-(\\sigma _{s}')^{2}}}\\epsilon _{\\text{uncond}}+\\sigma _{s}'\\epsilon \\\\\\end{aligned}}}\n  \nA similar technique applies to language model sampling. Also, if the unconditional generation \n  \n    \n      \n        \n          ϵ\n          \n            uncond\n          \n        \n        ←\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\text{uncond}}\\leftarrow \\epsilon _{\\theta }(x_{t},t)}\n  \n is replaced by \n  \n    \n      \n        \n          ϵ\n          \n            neg cond\n          \n        \n        ←\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        ,\n        \n          c\n          ′\n        \n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\text{neg cond}}\\leftarrow \\epsilon _{\\theta }(x_{t},t,c')}\n  \n, then it results in negative prompting, which pushes the generation away from \n  \n    \n      \n        \n          c\n          ′\n        \n      \n    \n    {\\displaystyle c'}\n  \n condition.\n\n\n=== Samplers ===\nGiven a diffusion model, one may regard it either as a continuous process, and sample from it by integrating a SDE, or one can regard it as a discrete process, and sample from it by iterating the discrete steps. The choice of the \"noise schedule\" \n  \n    \n      \n        \n          β\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\beta _{t}}\n  \n can also affect the quality of samples. A noise schedule is a function that sends a natural number to a noise level: \n  \n    \n      \n        t\n        ↦\n        \n          β\n          \n            t\n          \n        \n        ,\n        \n        t\n        ∈\n        {\n        1\n        ,\n        2\n        ,\n        …\n        }\n        ,\n        β\n        ∈\n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle t\\mapsto \\beta _{t},\\quad t\\in \\{1,2,\\dots \\},\\beta \\in (0,1)}\n  \nA noise schedule is more often specified by a map \n  \n    \n      \n        t\n        ↦\n        \n          σ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle t\\mapsto \\sigma _{t}}\n  \n. The two definitions are equivalent, since \n  \n    \n      \n        \n          β\n          \n            t\n          \n        \n        =\n        1\n        −\n        \n          \n            \n              1\n              −\n              \n                σ\n                \n                  t\n                \n                \n                  2\n                \n              \n            \n            \n              1\n              −\n              \n                σ\n                \n                  t\n                  −\n                  1\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\beta _{t}=1-{\\frac {1-\\sigma _{t}^{2}}{1-\\sigma _{t-1}^{2}}}}\n  \n.\nIn the DDPM perspective, one can use the DDPM itself (with noise), or DDIM (with adjustable amount of noise). The case where one adds noise is sometimes called ancestral sampling. One can interpolate between noise and no noise. The amount of noise is denoted \n  \n    \n      \n        η\n      \n    \n    {\\displaystyle \\eta }\n  \n (\"eta value\") in the DDIM paper, with \n  \n    \n      \n        η\n        =\n        0\n      \n    \n    {\\displaystyle \\eta =0}\n  \n denoting no noise (as in deterministic DDIM), and \n  \n    \n      \n        η\n        =\n        1\n      \n    \n    {\\displaystyle \\eta =1}\n  \n denoting full noise (as in DDPM).\nIn the perspective of SDE, one can use any of the numerical integration methods, such as Euler–Maruyama method, Heun's method, linear multistep methods, etc. Just as in the discrete case, one can add an adjustable amount of noise during the integration.\nA survey and comparison of samplers in the context of image generation is in.\n\n\n=== Other examples ===\nNotable variants include Poisson flow generative model, consistency model, critically damped Langevin diffusion, GenPhys, cold diffusion, etc.\n\n\n== Flow-based diffusion model ==\nAbstractly speaking, the idea of diffusion model is to take an unknown probability distribution (the distribution of natural-looking images), then progressively convert it to a known probability distribution (standard Gaussian distribution), by building an absolutely continuous probability path connecting them. The probability path is in fact defined implicitly by the score function \n  \n    \n      \n        ∇\n        ln\n        ⁡\n        \n          p\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\nabla \\ln p_{t}}\n  \n.\nIn denoising diffusion models, the forward process adds noise, and the backward process removes noise. Both the forward and backward processes are SDEs, though the forward process is integrable in closed-form, so it can be done at no computational cost. The backward process is not integrable in closed-form, so it must be integrated step-by-step by standard SDE solvers, which can be very expensive. The probability path in diffusions model is defined through an Itô process and one can retrieve the deterministic process by using the Probability ODE flow formulation.\nIn flow-based diffusion models, the forward process is a deterministic flow along a time-dependent vector field, and the backward process is also a deterministic flow along the same vector field, but going backwards. Both processes are solutions to ODEs. If the vector field is well-behaved, the ODE will also be well-behaved.\nGiven two distributions \n  \n    \n      \n        \n          π\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\pi _{0}}\n  \n and \n  \n    \n      \n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\pi _{1}}\n  \n, a flow-based model is a time-dependent velocity field \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle v_{t}(x)}\n  \n in \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n        ×\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle [0,1]\\times \\mathbb {R} ^{d}}\n  \n, such that if we start by sampling a point \n  \n    \n      \n        x\n        ∼\n        \n          π\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x\\sim \\pi _{0}}\n  \n, and let it move according to the velocity field:\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        \n          ϕ\n          \n            t\n          \n        \n        (\n        x\n        )\n        =\n        \n          v\n          \n            t\n          \n        \n        (\n        \n          ϕ\n          \n            t\n          \n        \n        (\n        x\n        )\n        )\n        \n        t\n        ∈\n        [\n        0\n        ,\n        1\n        ]\n        ,\n        \n        \n          starting from \n        \n        \n          ϕ\n          \n            0\n          \n        \n        (\n        x\n        )\n        =\n        x\n      \n    \n    {\\displaystyle {\\frac {d}{dt}}\\phi _{t}(x)=v_{t}(\\phi _{t}(x))\\quad t\\in [0,1],\\quad {\\text{starting from }}\\phi _{0}(x)=x}\n  \n\nwe end up with a point \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ∼\n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}\\sim \\pi _{1}}\n  \n. The solution \n  \n    \n      \n        \n          ϕ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\phi _{t}}\n  \n of the above ODE define a probability path \n  \n    \n      \n        \n          p\n          \n            t\n          \n        \n        =\n        [\n        \n          ϕ\n          \n            t\n          \n        \n        \n          ]\n          \n            #\n          \n        \n        \n          π\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle p_{t}=[\\phi _{t}]_{\\#}\\pi _{0}}\n  \n by the pushforward measure operator. In particular, \n  \n    \n      \n        [\n        \n          ϕ\n          \n            1\n          \n        \n        \n          ]\n          \n            #\n          \n        \n        \n          π\n          \n            0\n          \n        \n        =\n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle [\\phi _{1}]_{\\#}\\pi _{0}=\\pi _{1}}\n  \n.\nThe probability path and the velocity field also satisfy the continuity equation, in the sense of probability distribution:\n\n  \n    \n      \n        \n          ∂\n          \n            t\n          \n        \n        \n          p\n          \n            t\n          \n        \n        +\n        ∇\n        ⋅\n        (\n        \n          v\n          \n            t\n          \n        \n        \n          p\n          \n            t\n          \n        \n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\partial _{t}p_{t}+\\nabla \\cdot (v_{t}p_{t})=0}\n  \n\nTo construct a probability path, we start by construct a conditional probability path \n  \n    \n      \n        \n          p\n          \n            t\n          \n        \n        (\n        x\n        |\n        z\n        )\n      \n    \n    {\\displaystyle p_{t}(x\\vert z)}\n  \n and the corresponding conditional velocity field \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        (\n        x\n        |\n        z\n        )\n      \n    \n    {\\displaystyle v_{t}(x\\vert z)}\n  \n on some conditional distribution \n  \n    \n      \n        q\n        (\n        z\n        )\n      \n    \n    {\\displaystyle q(z)}\n  \n. A natural choice is the Gaussian conditional probability path:\n\n  \n    \n      \n        \n          p\n          \n            t\n          \n        \n        (\n        x\n        |\n        z\n        )\n        =\n        \n          \n            N\n          \n        \n        \n          (\n          \n            \n              m\n              \n                t\n              \n            \n            (\n            z\n            )\n            ,\n            \n              ζ\n              \n                t\n              \n              \n                2\n              \n            \n            I\n          \n          )\n        \n      \n    \n    {\\displaystyle p_{t}(x\\vert z)={\\mathcal {N}}\\left(m_{t}(z),\\zeta _{t}^{2}I\\right)}\n  \n\nThe conditional velocity field which corresponds to the geodesic path between conditional Gaussian path is \n\n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n        (\n        x\n        |\n        z\n        )\n        =\n        \n          \n            \n              ζ\n              \n                t\n              \n              ′\n            \n            \n              ζ\n              \n                t\n              \n            \n          \n        \n        (\n        x\n        −\n        \n          m\n          \n            t\n          \n        \n        (\n        z\n        )\n        )\n        +\n        \n          m\n          \n            t\n          \n          ′\n        \n        (\n        z\n        )\n      \n    \n    {\\displaystyle v_{t}(x\\vert z)={\\frac {\\zeta _{t}'}{\\zeta _{t}}}(x-m_{t}(z))+m_{t}'(z)}\n  \n\nThe probability path and velocity field are then computed by marginalizing\n\n  \n    \n      \n        \n          p\n          \n            t\n          \n        \n        (\n        x\n        )\n        =\n        ∫\n        \n          p\n          \n            t\n          \n        \n        (\n        x\n        |\n        z\n        )\n        q\n        (\n        z\n        )\n        d\n        z\n        \n        \n           and \n        \n        \n        \n          v\n          \n            t\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            E\n          \n          \n            q\n            (\n            z\n            )\n          \n        \n        \n          [\n          \n            \n              \n                \n                  v\n                  \n                    t\n                  \n                \n                (\n                x\n                |\n                z\n                )\n                \n                  p\n                  \n                    t\n                  \n                \n                (\n                x\n                |\n                z\n                )\n              \n              \n                \n                  p\n                  \n                    t\n                  \n                \n                (\n                x\n                )\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle p_{t}(x)=\\int p_{t}(x\\vert z)q(z)dz\\qquad {\\text{ and }}\\qquad v_{t}(x)=\\mathbb {E} _{q(z)}\\left[{\\frac {v_{t}(x\\vert z)p_{t}(x\\vert z)}{p_{t}(x)}}\\right]}\n  \n\n\n=== Optimal transport flow ===\nThe idea of optimal transport flow  is to construct a probability path minimizing the Wasserstein metric. The distribution on which we condition is an approximation of the optimal transport plan between \n  \n    \n      \n        \n          π\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\pi _{0}}\n  \n and \n  \n    \n      \n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\pi _{1}}\n  \n: \n  \n    \n      \n        z\n        =\n        (\n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle z=(x_{0},x_{1})}\n  \n and \n  \n    \n      \n        q\n        (\n        z\n        )\n        =\n        Γ\n        (\n        \n          π\n          \n            0\n          \n        \n        ,\n        \n          π\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle q(z)=\\Gamma (\\pi _{0},\\pi _{1})}\n  \n, where \n  \n    \n      \n        Γ\n      \n    \n    {\\displaystyle \\Gamma }\n  \n is the optimal transport plan, which can be approximated by mini-batch optimal transport. If the batch size is not large, then the transport it computes can be very far from the true optimal transport.\n\n\n=== Rectified flow ===\nThe idea of rectified flow is to learn a flow model such that the velocity is nearly constant along each flow path. This is beneficial, because we can integrate along such a vector field with very few steps. For example, if an ODE \n  \n    \n      \n        \n          \n            \n              \n                ϕ\n                \n                  t\n                \n              \n              ˙\n            \n          \n        \n        (\n        x\n        )\n        =\n        \n          v\n          \n            t\n          \n        \n        (\n        \n          ϕ\n          \n            t\n          \n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle {\\dot {\\phi _{t}}}(x)=v_{t}(\\phi _{t}(x))}\n  \n follows perfectly straight paths, it simplifies to \n  \n    \n      \n        \n          ϕ\n          \n            t\n          \n        \n        (\n        x\n        )\n        =\n        \n          x\n          \n            0\n          \n        \n        +\n        t\n        ⋅\n        \n          v\n          \n            0\n          \n        \n        (\n        \n          x\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle \\phi _{t}(x)=x_{0}+t\\cdot v_{0}(x_{0})}\n  \n, allowing for exact solutions in one step. In practice, we cannot reach such perfection, but when the flow field is nearly so, we can take a few large steps instead of many little steps.  \n\nThe general idea is to start with two distributions \n  \n    \n      \n        \n          π\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\pi _{0}}\n  \n and \n  \n    \n      \n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\pi _{1}}\n  \n, then construct a flow field \n  \n    \n      \n        \n          ϕ\n          \n            0\n          \n        \n        =\n        {\n        \n          ϕ\n          \n            t\n          \n        \n        :\n        t\n        ∈\n        [\n        0\n        ,\n        1\n        ]\n        }\n      \n    \n    {\\displaystyle \\phi ^{0}=\\{\\phi _{t}:t\\in [0,1]\\}}\n  \n from it, then repeatedly apply a \"reflow\" operation to obtain successive flow fields \n  \n    \n      \n        \n          ϕ\n          \n            1\n          \n        \n        ,\n        \n          ϕ\n          \n            2\n          \n        \n        ,\n        …\n      \n    \n    {\\displaystyle \\phi ^{1},\\phi ^{2},\\dots }\n  \n, each straighter than the previous one. When the flow field is straight enough for the application, we stop.\nGenerally, for any time-differentiable process \n  \n    \n      \n        \n          ϕ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\phi _{t}}\n  \n, \n  \n    \n      \n        \n          v\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle v_{t}}\n  \n can be estimated by solving:\n\n  \n    \n      \n        \n          min\n          \n            θ\n          \n        \n        \n          ∫\n          \n            0\n          \n          \n            1\n          \n        \n        \n          \n            E\n          \n          \n            x\n            ∼\n            \n              p\n              \n                t\n              \n            \n          \n        \n        \n          [\n          \n            ‖\n            \n              \n                v\n                \n                  t\n                \n              \n              (\n              x\n              ,\n              θ\n              )\n              −\n              \n                v\n                \n                  t\n                \n              \n              (\n              x\n              )\n            \n            \n              ‖\n              \n                2\n              \n            \n          \n          ]\n        \n        \n        \n          d\n        \n        t\n        .\n      \n    \n    {\\displaystyle \\min _{\\theta }\\int _{0}^{1}\\mathbb {E} _{x\\sim p_{t}}\\left[\\lVert {v_{t}(x,\\theta )-v_{t}(x)}\\rVert ^{2}\\right]\\,\\mathrm {d} t.}\n  \n\nIn rectified flow, by injecting strong priors that intermediate trajectories are straight, it can achieve both theoretical relevance for optimal transport  and computational efficiency, as ODEs with straight paths can be simulated precisely without time discretization.\n\nSpecifically, rectified flow seeks to match an ODE with the marginal distributions of the linear interpolation between points from distributions \n  \n    \n      \n        \n          π\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\pi _{0}}\n  \n and \n  \n    \n      \n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\pi _{1}}\n  \n. Given observations \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ∼\n        \n          π\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}\\sim \\pi _{0}}\n  \n and \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ∼\n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}\\sim \\pi _{1}}\n  \n, the canonical linear interpolation \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        =\n        t\n        \n          x\n          \n            1\n          \n        \n        +\n        (\n        1\n        −\n        t\n        )\n        \n          x\n          \n            0\n          \n        \n        ,\n        t\n        ∈\n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle x_{t}=tx_{1}+(1-t)x_{0},t\\in [0,1]}\n  \n yields a trivial case \n  \n    \n      \n        \n          \n            \n              \n                x\n                ˙\n              \n            \n          \n          \n            t\n          \n        \n        =\n        \n          x\n          \n            1\n          \n        \n        −\n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle {\\dot {x}}_{t}=x_{1}-x_{0}}\n  \n, which cannot be causally simulated without \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n. To address this, \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n is \"projected\" into a space of causally simulatable ODEs, by minimizing the least squares loss with respect to the direction \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        −\n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{1}-x_{0}}\n  \n:\n\n  \n    \n      \n        \n          min\n          \n            θ\n          \n        \n        \n          ∫\n          \n            0\n          \n          \n            1\n          \n        \n        \n          \n            E\n          \n          \n            \n              π\n              \n                0\n              \n            \n            ,\n            \n              π\n              \n                1\n              \n            \n            ,\n            \n              p\n              \n                t\n              \n            \n          \n        \n        \n          [\n          \n            ‖\n            \n              (\n              \n                x\n                \n                  1\n                \n              \n              −\n              \n                x\n                \n                  0\n                \n              \n              )\n              −\n              \n                v\n                \n                  t\n                \n              \n              (\n              \n                x\n                \n                  t\n                \n              \n              )\n            \n            \n              ‖\n              \n                2\n              \n            \n          \n          ]\n        \n        \n        \n          d\n        \n        t\n        .\n      \n    \n    {\\displaystyle \\min _{\\theta }\\int _{0}^{1}\\mathbb {E} _{\\pi _{0},\\pi _{1},p_{t}}\\left[\\lVert {(x_{1}-x_{0})-v_{t}(x_{t})}\\rVert ^{2}\\right]\\,\\mathrm {d} t.}\n  \n\nThe data pair \n  \n    \n      \n        (\n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{0},x_{1})}\n  \n can be any coupling of \n  \n    \n      \n        \n          π\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\pi _{0}}\n  \n and \n  \n    \n      \n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\pi _{1}}\n  \n, typically independent (i.e., \n  \n    \n      \n        (\n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n        \n        )\n        ∼\n        \n          π\n          \n            0\n          \n        \n        ×\n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle (x_{0},x_{1})\\sim \\pi _{0}\\times \\pi _{1}}\n  \n) obtained by randomly combining observations from \n  \n    \n      \n        \n          π\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\pi _{0}}\n  \n and \n  \n    \n      \n        \n          π\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\pi _{1}}\n  \n. This process ensures that the trajectories closely mirror the density map of \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n trajectories but reroute at intersections to ensure causality.\n\nA distinctive aspect of rectified flow is its capability for \"reflow\", which straightens the trajectory of ODE paths. Denote the rectified flow \n  \n    \n      \n        \n          ϕ\n          \n            0\n          \n        \n        =\n        {\n        \n          ϕ\n          \n            t\n          \n        \n        :\n        t\n        ∈\n        [\n        0\n        ,\n        1\n        ]\n        }\n      \n    \n    {\\displaystyle \\phi ^{0}=\\{\\phi _{t}:t\\in [0,1]\\}}\n  \n induced from \n  \n    \n      \n        (\n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{0},x_{1})}\n  \n as \n  \n    \n      \n        \n          ϕ\n          \n            0\n          \n        \n        =\n        \n          \n            R\n            e\n            c\n            t\n            f\n            l\n            o\n            w\n          \n        \n        (\n        (\n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle \\phi ^{0}={\\mathsf {Rectflow}}((x_{0},x_{1}))}\n  \n. Recursively applying this \n  \n    \n      \n        \n          \n            R\n            e\n            c\n            t\n            f\n            l\n            o\n            w\n          \n        \n        (\n        ⋅\n        )\n      \n    \n    {\\displaystyle {\\mathsf {Rectflow}}(\\cdot )}\n  \n operator generates a series of rectified flows \n  \n    \n      \n        \n          ϕ\n          \n            k\n            +\n            1\n          \n        \n        =\n        \n          \n            R\n            e\n            c\n            t\n            f\n            l\n            o\n            w\n          \n        \n        (\n        (\n        \n          ϕ\n          \n            0\n          \n          \n            k\n          \n        \n        (\n        \n          x\n          \n            0\n          \n        \n        )\n        ,\n        \n          ϕ\n          \n            1\n          \n          \n            k\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        )\n        )\n      \n    \n    {\\displaystyle \\phi ^{k+1}={\\mathsf {Rectflow}}((\\phi _{0}^{k}(x_{0}),\\phi _{1}^{k}(x_{1})))}\n  \n. This \"reflow\" process not only reduces transport costs but also straightens the paths of rectified flows, making \n  \n    \n      \n        \n          ϕ\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\phi ^{k}}\n  \n paths straighter with increasing \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n.\nRectified flow includes a nonlinear extension where linear interpolation \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n is replaced with any time-differentiable curve that connects \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n and \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n, given by \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n        =\n        \n          α\n          \n            t\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \n          β\n          \n            t\n          \n        \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{t}=\\alpha _{t}x_{1}+\\beta _{t}x_{0}}\n  \n. This framework encompasses DDIM and probability flow ODEs as special cases, with particular choices of \n  \n    \n      \n        \n          α\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{t}}\n  \n and \n  \n    \n      \n        \n          β\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\beta _{t}}\n  \n. However, in the case where the path of \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n is not straight, the reflow process no longer ensures a reduction in convex transport costs, and also no longer straighten the paths of \n  \n    \n      \n        \n          ϕ\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\phi _{t}}\n  \n.\n\n\n== Choice of architecture ==\n\n\n=== Diffusion model ===\nFor generating images by DDPM, we need a neural network that takes a time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n and a noisy image \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n, and predicts a noise \n  \n    \n      \n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\epsilon _{\\theta }(x_{t},t)}\n  \n from it. Since predicting the noise is the same as predicting the denoised image, then subtracting it from \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n, denoising architectures tend to work well. For example, the U-Net, which was found to be good for denoising images, is often used for denoising diffusion models that generate images.\nFor DDPM, the underlying architecture (\"backbone\") does not have to be a U-Net. It just has to predict the noise somehow. For example, the diffusion transformer (DiT) uses a Transformer to predict the mean and diagonal covariance of the noise, given the textual conditioning and the partially denoised image. It is the same as standard U-Net-based denoising diffusion model, with a Transformer replacing the U-Net. Mixture of experts-Transformer can also be applied.\nDDPM can be used to model general data distributions, not just natural-looking images. For example, Human Motion Diffusion models human motion trajectory by DDPM. Each human motion trajectory is a sequence of poses, represented by either joint rotations or positions. It uses a Transformer network to generate a less noisy trajectory out of a noisy one.\n\n\n=== Conditioning ===\nThe base diffusion model can only generate unconditionally from the whole distribution. For example, a diffusion model learned on ImageNet would generate images that look like a random image from ImageNet. To generate images from just one category, one would need to impose the condition, and then sample from the conditional distribution. Whatever condition one wants to impose, one needs to first convert the conditioning into a vector of floating point numbers, then feed it into the underlying diffusion model neural network. However, one has freedom in choosing how to convert the conditioning into a vector.\nStable Diffusion, for example, imposes conditioning in the form of cross-attention mechanism, where the query is an intermediate representation of the image in the U-Net, and both key and value are the conditioning vectors. The conditioning can be selectively applied to only parts of an image, and new kinds of conditionings can be finetuned upon the base model, as used in ControlNet.\nAs a particularly simple example, consider image inpainting. The conditions are \n  \n    \n      \n        \n          \n            \n              x\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}}\n  \n, the reference image, and \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n, the inpainting mask. The conditioning is imposed at each step of the backward diffusion process, by first sampling \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            t\n          \n        \n        ∼\n        N\n        \n          (\n          \n            \n              \n                \n                  \n                    \n                      \n                        α\n                        ¯\n                      \n                    \n                  \n                  \n                    t\n                  \n                \n              \n            \n            \n              \n                \n                  x\n                  ~\n                \n              \n            \n            ,\n            \n              σ\n              \n                t\n              \n              \n                2\n              \n            \n            I\n          \n          )\n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{t}\\sim N\\left({\\sqrt {{\\bar {\\alpha }}_{t}}}{\\tilde {x}},\\sigma _{t}^{2}I\\right)}\n  \n, a noisy version of \n  \n    \n      \n        \n          \n            \n              x\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}}\n  \n, then replacing \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n with \n  \n    \n      \n        (\n        1\n        −\n        m\n        )\n        ⊙\n        \n          x\n          \n            t\n          \n        \n        +\n        m\n        ⊙\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            t\n          \n        \n      \n    \n    {\\displaystyle (1-m)\\odot x_{t}+m\\odot {\\tilde {x}}_{t}}\n  \n, where \n  \n    \n      \n        ⊙\n      \n    \n    {\\displaystyle \\odot }\n  \n means elementwise multiplication. Another application of cross-attention mechanism is prompt-to-prompt image editing.\nConditioning is not limited to just generating images from a specific category, or according to a specific caption (as in text-to-image). For example, demonstrated generating human motion, conditioned on an audio clip of human walking (allowing syncing motion to a soundtrack), or video of human running, or a text description of human motion, etc. For how conditional diffusion models are mathematically formulated, see a methodological summary in.\n\n\n=== Upscaling ===\nAs generating an image takes a long time, one can try to generate a small image by a base diffusion model, then upscale it by other models. Upscaling can be done by GAN, Transformer, or signal processing methods like Lanczos resampling.\nDiffusion models themselves can be used to perform upscaling. Cascading diffusion model stacks multiple diffusion models one after another, in the style of Progressive GAN. The lowest level is a standard diffusion model that generate 32x32 image, then the image would be upscaled by a diffusion model specifically trained for upscaling, and the process repeats.\nIn more detail, the diffusion upscaler is trained as follows:\n\nSample \n  \n    \n      \n        (\n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          z\n          \n            0\n          \n        \n        ,\n        c\n        )\n      \n    \n    {\\displaystyle (x_{0},z_{0},c)}\n  \n, where \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n is the high-resolution image, \n  \n    \n      \n        \n          z\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle z_{0}}\n  \n is the same image but scaled down to a low-resolution, and \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is the conditioning, which can be the caption of the image, the class of the image, etc.\nSample two white noises \n  \n    \n      \n        \n          ϵ\n          \n            x\n          \n        \n        ,\n        \n          ϵ\n          \n            z\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{x},\\epsilon _{z}}\n  \n, two time-steps \n  \n    \n      \n        \n          t\n          \n            x\n          \n        \n        ,\n        \n          t\n          \n            z\n          \n        \n      \n    \n    {\\displaystyle t_{x},t_{z}}\n  \n. Compute the noisy versions of the high-resolution and low-resolution images: \n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    x\n                    \n                      \n                        t\n                        \n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  =\n                  \n                    \n                      \n                        \n                          \n                            \n                              α\n                              ¯\n                            \n                          \n                        \n                        \n                          \n                            t\n                            \n                              x\n                            \n                          \n                        \n                      \n                    \n                  \n                  \n                    x\n                    \n                      0\n                    \n                  \n                  +\n                  \n                    σ\n                    \n                      \n                        t\n                        \n                          x\n                        \n                      \n                    \n                  \n                  \n                    ϵ\n                    \n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    z\n                    \n                      \n                        t\n                        \n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  =\n                  \n                    \n                      \n                        \n                          \n                            \n                              α\n                              ¯\n                            \n                          \n                        \n                        \n                          \n                            t\n                            \n                              z\n                            \n                          \n                        \n                      \n                    \n                  \n                  \n                    z\n                    \n                      0\n                    \n                  \n                  +\n                  \n                    σ\n                    \n                      \n                        t\n                        \n                          z\n                        \n                      \n                    \n                  \n                  \n                    ϵ\n                    \n                      z\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}x_{t_{x}}&={\\sqrt {{\\bar {\\alpha }}_{t_{x}}}}x_{0}+\\sigma _{t_{x}}\\epsilon _{x}\\\\z_{t_{z}}&={\\sqrt {{\\bar {\\alpha }}_{t_{z}}}}z_{0}+\\sigma _{t_{z}}\\epsilon _{z}\\end{cases}}}\n  \n.\nTrain the denoising network to predict \n  \n    \n      \n        \n          ϵ\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{x}}\n  \n given \n  \n    \n      \n        \n          x\n          \n            \n              t\n              \n                x\n              \n            \n          \n        \n        ,\n        \n          z\n          \n            \n              t\n              \n                z\n              \n            \n          \n        \n        ,\n        \n          t\n          \n            x\n          \n        \n        ,\n        \n          t\n          \n            z\n          \n        \n        ,\n        c\n      \n    \n    {\\displaystyle x_{t_{x}},z_{t_{z}},t_{x},t_{z},c}\n  \n. That is, apply gradient descent on \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n on the L2 loss \n  \n    \n      \n        ‖\n        \n          ϵ\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            \n              t\n              \n                x\n              \n            \n          \n        \n        ,\n        \n          z\n          \n            \n              t\n              \n                z\n              \n            \n          \n        \n        ,\n        \n          t\n          \n            x\n          \n        \n        ,\n        \n          t\n          \n            z\n          \n        \n        ,\n        c\n        )\n        −\n        \n          ϵ\n          \n            x\n          \n        \n        \n          ‖\n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\|\\epsilon _{\\theta }(x_{t_{x}},z_{t_{z}},t_{x},t_{z},c)-\\epsilon _{x}\\|_{2}^{2}}\n  \n.\n\n\n== Examples ==\nThis section collects some notable diffusion models, and briefly describes their architecture.\n\n\n=== OpenAI ===\n\nThe DALL-E series by OpenAI are text-conditional diffusion models of images.\nThe first version of DALL-E (2021) is not actually a diffusion model. Instead, it uses a Transformer architecture that autoregressively generates a sequence of tokens, which is then converted to an image by the decoder of a discrete VAE. Released with DALL-E was the CLIP classifier, which was used by DALL-E to rank generated images according to how close the image fits the text.\nGLIDE (2022-03) is a 3.5-billion diffusion model, and a small version was released publicly. Soon after, DALL-E 2 was released (2022-04). DALL-E 2 is a 3.5-billion cascaded diffusion model that generates images from text by \"inverting the CLIP image encoder\", the technique which they termed \"unCLIP\".\nThe unCLIP method contains 4 models: a CLIP image encoder, a CLIP text encoder, an image decoder, and a \"prior\" model (which can be a diffusion model, or an autoregressive model). During training, the prior model is trained to convert CLIP image encodings to CLIP text encodings. The image decoder is trained to convert CLIP image encodings back to images. During inference, a text is converted by the CLIP text encoder to a vector, then it is converted by the prior model to an image encoding, then it is converted by the image decoder to an image.\nSora (2024-02) is a diffusion Transformer model (DiT).\n\n\n=== Stability AI ===\n\nStable Diffusion (2022-08), released by Stability AI, consists of a denoising latent diffusion model (860 million parameters), a VAE, and a text encoder. The denoising network is a U-Net, with cross-attention blocks to allow for conditional image generation.\nStable Diffusion 3 (2024-03) changed the latent diffusion model from the UNet to a Transformer model, and so it is a DiT. It uses rectified flow.\nStable Video 4D (2024-07) is a latent diffusion model for videos of 3D objects.\n\n\n=== Google ===\nImagen (2022) uses a T5-XXL language model to encode the input text into an embedding vector. It is a cascaded diffusion model with three sub-models. The first step denoises a white noise to a 64×64 image, conditional on the embedding vector of the text. This model has 2B parameters. The second step upscales the image by 64×64→256×256, conditional on embedding. This model has 650M parameters. The third step is similar, upscaling by 256×256→1024×1024. This model has 400M parameters. The three denoising networks are all U-Nets.\nMuse (2023-01) is not a diffusion model, but an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens.\nImagen 2 (2023-12) is also diffusion-based. It can generate images based on a prompt that mixes images and text. No further information available. Imagen 3 (2024-05) is too. No further information available.\nVeo (2024) generates videos by latent diffusion. The diffusion is conditioned on a vector that encodes both a text prompt and an image prompt.\n\n\n=== Meta ===\nMake-A-Video (2022) is a text-to-video diffusion model.\nCM3leon (2023) is not a diffusion model, but an autoregressive causally masked Transformer, with mostly the same architecture as LLaMa-2.\n\nTransfusion (2024) is a Transformer that combines autoregressive text generation and denoising diffusion. Specifically, it generates text autoregressively (with causal masking), and generates images by denoising multiple times over image tokens (with all-to-all attention).\nMovie Gen (2024) is a series of Diffusion Transformers operating on latent space and by flow matching.\n\n\n== See also ==\nDiffusion process\nMarkov chain\nVariational inference\nVariational autoencoder\n\n\n== Further reading ==\nReview papers\nYang, Ling (2024-09-06), YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy, retrieved 2024-09-06\nYang, Ling; Zhang, Zhilong; Song, Yang; Hong, Shenda; Xu, Runsheng; Zhao, Yue; Zhang, Wentao; Cui, Bin; Yang, Ming-Hsuan (2023-11-09). \"Diffusion Models: A Comprehensive Survey of Methods and Applications\". ACM Comput. Surv. 56 (4): 105:1–105:39. arXiv:2209.00796. doi:10.1145/3626235. ISSN 0360-0300.\nAustin, Jacob; Johnson, Daniel D.; Ho, Jonathan; Tarlow, Daniel; Rianne van den Berg (2021). \"Structured Denoising Diffusion Models in Discrete State-Spaces\". arXiv:2107.03006 [cs.LG].\nCroitoru, Florinel-Alin; Hondru, Vlad; Ionescu, Radu Tudor; Shah, Mubarak (2023-09-01). \"Diffusion Models in Vision: A Survey\". IEEE Transactions on Pattern Analysis and Machine Intelligence. 45 (9): 10850–10869. arXiv:2209.04747. Bibcode:2023ITPAM..4510850C. doi:10.1109/TPAMI.2023.3261988. ISSN 0162-8828. PMID 37030794.\nMathematical details omitted in the article.\n\"Power of Diffusion Models\". AstraBlog. 2022-09-25. Retrieved 2023-09-25.\nLuo, Calvin (2022-08-25). \"Understanding Diffusion Models: A Unified Perspective\". arXiv:2208.11970 [cs.LG].\nWeng, Lilian (2021-07-11). \"What are Diffusion Models?\". lilianweng.github.io. Retrieved 2023-09-25.\nTutorials\nNakkiran, Preetum; Bradley, Arwen; Zhou, Hattie; Advani, Madhu (2024). \"Step-by-Step Diffusion: An Elementary Tutorial\". arXiv:2406.08929 [cs.LG].\n\"Guidance: a cheat code for diffusion models\". 26 May 2022. Overview of classifier guidance and classifier-free guidance, light on mathematical details.\nCatherine Higham, Desmond J. Higham, and Peter Grindrod: \"Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians\", SIAM Review, Vol.67, No.3 (2025).\n\n\n== References ==",
    "categories": [
      "All articles containing potentially dated statements",
      "All articles with specifically marked weasel-worded phrases",
      "Articles containing potentially dated statements from 2024",
      "Articles with short description",
      "Articles with specifically marked weasel-worded phrases from December 2025",
      "CS1 errors: generic name",
      "Machine learning algorithms",
      "Markov models",
      "Short description is different from Wikidata"
    ],
    "year_mentioned": 2023
  },
  {
    "title": "Generative adversarial network",
    "url": "https://en.wikipedia.org/wiki/Generative_adversarial_network",
    "content": "A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intelligence. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014. In a GAN, two neural networks compete with each other in the form of a zero-sum game, where one agent's gain is another agent's loss.\nGiven a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning, fully supervised learning, and reinforcement learning.\nThe core idea of a GAN is based on the \"indirect\" training through the discriminator, another neural network that can tell how \"realistic\" the input seems, which itself is also being updated dynamically. This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.\nGANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks.\n\n\n== Definition ==\n\n\n=== Mathematical ===\nThe original GAN is defined as the following game:\nEach probability space \n  \n    \n      \n        (\n        Ω\n        ,\n        \n          μ\n          \n            ref\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega ,\\mu _{\\text{ref}})}\n  \n defines a GAN game.\nThere are 2 players: generator and discriminator.\nThe generator's strategy set is \n  \n    \n      \n        \n          \n            P\n          \n        \n        (\n        Ω\n        )\n      \n    \n    {\\displaystyle {\\mathcal {P}}(\\Omega )}\n  \n, the set of all probability measures \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}}\n  \n on \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n.\nThe discriminator's strategy set is the set of Markov kernels \n  \n    \n      \n        \n          μ\n          \n            D\n          \n        \n        :\n        Ω\n        →\n        \n          \n            P\n          \n        \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle \\mu _{D}:\\Omega \\to {\\mathcal {P}}[0,1]}\n  \n, where \n  \n    \n      \n        \n          \n            P\n          \n        \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle {\\mathcal {P}}[0,1]}\n  \n is the set of probability measures on \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  \n.\nThe GAN game is a zero-sum game, with objective function\n  \n    \n      \n        L\n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        \n          μ\n          \n            D\n          \n        \n        )\n        :=\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                ref\n              \n            \n            ,\n            y\n            ∼\n            \n              μ\n              \n                D\n              \n            \n            (\n            x\n            )\n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        y\n        ]\n        +\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n            ,\n            y\n            ∼\n            \n              μ\n              \n                D\n              \n            \n            (\n            x\n            )\n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        (\n        1\n        −\n        y\n        )\n        ]\n        .\n      \n    \n    {\\displaystyle L(\\mu _{G},\\mu _{D}):=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},y\\sim \\mu _{D}(x)}[\\ln y]+\\operatorname {E} _{x\\sim \\mu _{G},y\\sim \\mu _{D}(x)}[\\ln(1-y)].}\n  \n\nThe generator aims to minimize the objective, and the discriminator aims to maximize the objective.\n\nThe generator's task is to approach \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n        ≈\n        \n          μ\n          \n            ref\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}\\approx \\mu _{\\text{ref}}}\n  \n, that is, to match its own output distribution as closely as possible to the reference distribution. The discriminator's task is to output a value close to 1 when the input appears to be from the reference distribution, and to output a value close to 0 when the input looks like it came from the generator distribution.\n\n\n=== In practice ===\nThe generative network generates candidates while the discriminative network evaluates them. This creates a contest based on data distributions, where the generator learns to map from a latent space to the true data distribution, aiming to produce candidates that the discriminator cannot distinguish from real data. The discriminator's goal is to correctly identify these candidates, but as the generator improves, its task becomes more challenging, increasing the discriminator's error rate.\nA known dataset serves as the initial training data for the discriminator. Training involves presenting it with samples from the training dataset until it achieves acceptable accuracy. The generator is trained based on whether it succeeds in fooling the discriminator. Typically, the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter, candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples, while the discriminator becomes more skilled at flagging synthetic samples. When used for image generation, the generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.\n\n\n=== Relation to other statistical machine learning methods ===\nGANs are implicit generative models, which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample, unlike alternatives such as flow-based generative model.\n\nCompared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general, GANs can generate one complete sample in one pass, rather than multiple passes through the network.\nCompared to Boltzmann machines and linear ICA, there is no restriction on the type of function used by the network.\nSince neural networks are universal approximators, GANs are asymptotically consistent. Variational autoencoders might be universal approximators, but it is not proven as of 2017.\n\n\n== Mathematical properties ==\n\n\n=== Measure-theoretic considerations ===\nThis section provides some of the mathematical theory behind these methods.\n\nIn modern probability theory based on measure theory, a probability space also needs to be equipped with a σ-algebra. As a result, a more rigorous definition of the GAN game would make the following changes:Each probability space \n  \n    \n      \n        (\n        Ω\n        ,\n        \n          \n            B\n          \n        \n        ,\n        \n          μ\n          \n            ref\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega ,{\\mathcal {B}},\\mu _{\\text{ref}})}\n  \n defines a GAN game.\nThe generator's strategy set is \n  \n    \n      \n        \n          \n            P\n          \n        \n        (\n        Ω\n        ,\n        \n          \n            B\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {P}}(\\Omega ,{\\mathcal {B}})}\n  \n, the set of all probability measures \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}}\n  \n on the measure-space \n  \n    \n      \n        (\n        Ω\n        ,\n        \n          \n            B\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega ,{\\mathcal {B}})}\n  \n.\n\nThe discriminator's strategy set is the set of Markov kernels \n  \n    \n      \n        \n          μ\n          \n            D\n          \n        \n        :\n        (\n        Ω\n        ,\n        \n          \n            B\n          \n        \n        )\n        →\n        \n          \n            P\n          \n        \n        (\n        [\n        0\n        ,\n        1\n        ]\n        ,\n        \n          \n            B\n          \n        \n        (\n        [\n        0\n        ,\n        1\n        ]\n        )\n        )\n      \n    \n    {\\displaystyle \\mu _{D}:(\\Omega ,{\\mathcal {B}})\\to {\\mathcal {P}}([0,1],{\\mathcal {B}}([0,1]))}\n  \n, where \n  \n    \n      \n        \n          \n            B\n          \n        \n        (\n        [\n        0\n        ,\n        1\n        ]\n        )\n      \n    \n    {\\displaystyle {\\mathcal {B}}([0,1])}\n  \n is the Borel σ-algebra on \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  \n.Since issues of measurability never arise in practice, these will not concern us further.\n\n\n=== Choice of the strategy set ===\nIn the most generic version of the GAN game described above, the strategy set for the discriminator contains all Markov kernels \n  \n    \n      \n        \n          μ\n          \n            D\n          \n        \n        :\n        Ω\n        →\n        \n          \n            P\n          \n        \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle \\mu _{D}:\\Omega \\to {\\mathcal {P}}[0,1]}\n  \n, and the strategy set for the generator contains arbitrary probability distributions \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}}\n  \n on \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n.\nHowever, as shown below, the optimal discriminator strategy against any \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}}\n  \n is deterministic, so there is no loss of generality in restricting the discriminator's strategies to deterministic functions \n  \n    \n      \n        D\n        :\n        Ω\n        →\n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle D:\\Omega \\to [0,1]}\n  \n. In most applications, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is a deep neural network function.\nAs for the generator, while \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}}\n  \n could theoretically be any computable probability distribution, in practice, it is usually implemented as a pushforward: \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n        =\n        \n          μ\n          \n            Z\n          \n        \n        ∘\n        \n          G\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}=\\mu _{Z}\\circ G^{-1}}\n  \n. That is, start with a random variable \n  \n    \n      \n        z\n        ∼\n        \n          μ\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle z\\sim \\mu _{Z}}\n  \n, where \n  \n    \n      \n        \n          μ\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle \\mu _{Z}}\n  \n is a probability distribution that is easy to compute (such as the uniform distribution, or the Gaussian distribution), then define a function \n  \n    \n      \n        G\n        :\n        \n          Ω\n          \n            Z\n          \n        \n        →\n        Ω\n      \n    \n    {\\displaystyle G:\\Omega _{Z}\\to \\Omega }\n  \n. Then the distribution \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}}\n  \n is the distribution of \n  \n    \n      \n        G\n        (\n        z\n        )\n      \n    \n    {\\displaystyle G(z)}\n  \n.\nConsequently, the generator's strategy is usually defined as just \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n, leaving \n  \n    \n      \n        z\n        ∼\n        \n          μ\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle z\\sim \\mu _{Z}}\n  \n implicit. In this formalism, the GAN game objective is\n  \n    \n      \n        L\n        (\n        G\n        ,\n        D\n        )\n        :=\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                ref\n              \n            \n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        D\n        (\n        x\n        )\n        ]\n        +\n        \n          E\n          \n            z\n            ∼\n            \n              μ\n              \n                Z\n              \n            \n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        (\n        1\n        −\n        D\n        (\n        G\n        (\n        z\n        )\n        )\n        )\n        ]\n        .\n      \n    \n    {\\displaystyle L(G,D):=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[\\ln D(x)]+\\operatorname {E} _{z\\sim \\mu _{Z}}[\\ln(1-D(G(z)))].}\n  \n\n\n=== Generative reparametrization ===\nThe GAN architecture has two main components. One is casting optimization into a game, of form \n  \n    \n      \n        \n          min\n          \n            G\n          \n        \n        \n          max\n          \n            D\n          \n        \n        L\n        (\n        G\n        ,\n        D\n        )\n      \n    \n    {\\displaystyle \\min _{G}\\max _{D}L(G,D)}\n  \n, which is different from the usual kind of optimization, of form \n  \n    \n      \n        \n          min\n          \n            θ\n          \n        \n        L\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle \\min _{\\theta }L(\\theta )}\n  \n. The other is the decomposition of \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}}\n  \n into \n  \n    \n      \n        \n          μ\n          \n            Z\n          \n        \n        ∘\n        \n          G\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\mu _{Z}\\circ G^{-1}}\n  \n, which can be understood as a reparametrization trick.\nTo see its significance, one must compare GAN with previous methods for learning generative models, which were plagued with \"intractable probabilistic computations that arise in maximum likelihood estimation and related strategies\".\nAt the same time, Kingma and Welling and Rezende et al. developed the same idea of reparametrization into a general stochastic backpropagation method. Among its first applications was the variational autoencoder.\n\n\n=== Move order and strategic equilibria ===\nIn the original paper, as well as most subsequent papers, it is usually assumed that the generator moves first, and the discriminator moves second, thus giving the following minimax game:\n  \n    \n      \n        \n          min\n          \n            \n              μ\n              \n                G\n              \n            \n          \n        \n        \n          max\n          \n            \n              μ\n              \n                D\n              \n            \n          \n        \n        L\n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        \n          μ\n          \n            D\n          \n        \n        )\n        :=\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                ref\n              \n            \n            ,\n            y\n            ∼\n            \n              μ\n              \n                D\n              \n            \n            (\n            x\n            )\n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        y\n        ]\n        +\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n            ,\n            y\n            ∼\n            \n              μ\n              \n                D\n              \n            \n            (\n            x\n            )\n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        (\n        1\n        −\n        y\n        )\n        ]\n        .\n      \n    \n    {\\displaystyle \\min _{\\mu _{G}}\\max _{\\mu _{D}}L(\\mu _{G},\\mu _{D}):=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},y\\sim \\mu _{D}(x)}[\\ln y]+\\operatorname {E} _{x\\sim \\mu _{G},y\\sim \\mu _{D}(x)}[\\ln(1-y)].}\n  \n\nIf both the generator's and the discriminator's strategy sets are spanned by a finite number of strategies, then by the minimax theorem,\n  \n    \n      \n        \n          min\n          \n            \n              μ\n              \n                G\n              \n            \n          \n        \n        \n          max\n          \n            \n              μ\n              \n                D\n              \n            \n          \n        \n        L\n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        \n          μ\n          \n            D\n          \n        \n        )\n        =\n        \n          max\n          \n            \n              μ\n              \n                D\n              \n            \n          \n        \n        \n          min\n          \n            \n              μ\n              \n                G\n              \n            \n          \n        \n        L\n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        \n          μ\n          \n            D\n          \n        \n        )\n      \n    \n    {\\displaystyle \\min _{\\mu _{G}}\\max _{\\mu _{D}}L(\\mu _{G},\\mu _{D})=\\max _{\\mu _{D}}\\min _{\\mu _{G}}L(\\mu _{G},\\mu _{D})}\n  \nthat is, the move order does not matter.\nHowever, since the strategy sets are both not finitely spanned, the minimax theorem does not apply, and the idea of an \"equilibrium\" becomes delicate. To wit, there are the following different concepts of equilibrium:\n\nEquilibrium when generator moves first, and discriminator moves second:\n  \n    \n      \n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            G\n          \n        \n        ∈\n        arg\n        ⁡\n        \n          min\n          \n            \n              μ\n              \n                G\n              \n            \n          \n        \n        \n          max\n          \n            \n              μ\n              \n                D\n              \n            \n          \n        \n        L\n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        \n          μ\n          \n            D\n          \n        \n        )\n        ,\n        \n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            D\n          \n        \n        ∈\n        arg\n        ⁡\n        \n          max\n          \n            \n              μ\n              \n                D\n              \n            \n          \n        \n        L\n        (\n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            G\n          \n        \n        ,\n        \n          μ\n          \n            D\n          \n        \n        )\n        ,\n        \n      \n    \n    {\\displaystyle {\\hat {\\mu }}_{G}\\in \\arg \\min _{\\mu _{G}}\\max _{\\mu _{D}}L(\\mu _{G},\\mu _{D}),\\quad {\\hat {\\mu }}_{D}\\in \\arg \\max _{\\mu _{D}}L({\\hat {\\mu }}_{G},\\mu _{D}),\\quad }\n  \n\nEquilibrium when discriminator moves first, and generator moves second:\n  \n    \n      \n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            D\n          \n        \n        ∈\n        arg\n        ⁡\n        \n          max\n          \n            \n              μ\n              \n                D\n              \n            \n          \n        \n        \n          min\n          \n            \n              μ\n              \n                G\n              \n            \n          \n        \n        L\n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        \n          μ\n          \n            D\n          \n        \n        )\n        ,\n        \n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            G\n          \n        \n        ∈\n        arg\n        ⁡\n        \n          min\n          \n            \n              μ\n              \n                G\n              \n            \n          \n        \n        L\n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            D\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle {\\hat {\\mu }}_{D}\\in \\arg \\max _{\\mu _{D}}\\min _{\\mu _{G}}L(\\mu _{G},\\mu _{D}),\\quad {\\hat {\\mu }}_{G}\\in \\arg \\min _{\\mu _{G}}L(\\mu _{G},{\\hat {\\mu }}_{D}),}\n  \n\nNash equilibrium \n  \n    \n      \n        (\n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            D\n          \n        \n        ,\n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            G\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\hat {\\mu }}_{D},{\\hat {\\mu }}_{G})}\n  \n, which is stable under simultaneous move order:\n  \n    \n      \n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            D\n          \n        \n        ∈\n        arg\n        ⁡\n        \n          max\n          \n            \n              μ\n              \n                D\n              \n            \n          \n        \n        L\n        (\n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            G\n          \n        \n        ,\n        \n          μ\n          \n            D\n          \n        \n        )\n        ,\n        \n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            G\n          \n        \n        ∈\n        arg\n        ⁡\n        \n          min\n          \n            \n              μ\n              \n                G\n              \n            \n          \n        \n        L\n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            D\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\hat {\\mu }}_{D}\\in \\arg \\max _{\\mu _{D}}L({\\hat {\\mu }}_{G},\\mu _{D}),\\quad {\\hat {\\mu }}_{G}\\in \\arg \\min _{\\mu _{G}}L(\\mu _{G},{\\hat {\\mu }}_{D})}\n  \n\nFor general games, these equilibria do not have to agree, or even to exist. For the original GAN game, these equilibria all exist, and are all equal. However, for more general GAN games, these do not necessarily exist, or agree.\n\n\n=== Main theorems for GAN game ===\nThe original GAN paper proved the following two theorems:\nInterpretation: For any fixed generator strategy \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}}\n  \n, the optimal discriminator keeps track of the likelihood ratio between the reference distribution and the generator distribution:\n  \n    \n      \n        \n          \n            \n              D\n              (\n              x\n              )\n            \n            \n              1\n              −\n              D\n              (\n              x\n              )\n            \n          \n        \n        =\n        \n          \n            \n              d\n              \n                μ\n                \n                  ref\n                \n              \n            \n            \n              d\n              \n                μ\n                \n                  G\n                \n              \n            \n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              \n                μ\n                \n                  ref\n                \n              \n              (\n              d\n              x\n              )\n            \n            \n              \n                μ\n                \n                  G\n                \n              \n              (\n              d\n              x\n              )\n            \n          \n        \n        ;\n        \n        D\n        (\n        x\n        )\n        =\n        σ\n        (\n        ln\n        ⁡\n        \n          μ\n          \n            ref\n          \n        \n        (\n        d\n        x\n        )\n        −\n        ln\n        ⁡\n        \n          μ\n          \n            G\n          \n        \n        (\n        d\n        x\n        )\n        )\n      \n    \n    {\\displaystyle {\\frac {D(x)}{1-D(x)}}={\\frac {d\\mu _{\\text{ref}}}{d\\mu _{G}}}(x)={\\frac {\\mu _{\\text{ref}}(dx)}{\\mu _{G}(dx)}};\\quad D(x)=\\sigma (\\ln \\mu _{\\text{ref}}(dx)-\\ln \\mu _{G}(dx))}\n  \nwhere \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n is the logistic function.\nIn particular, if the prior probability for an image \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n to come from the reference distribution is equal to \n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{2}}}\n  \n, then \n  \n    \n      \n        D\n        (\n        x\n        )\n      \n    \n    {\\displaystyle D(x)}\n  \n is just the posterior probability that \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n came from the reference distribution:\n  \n    \n      \n        D\n        (\n        x\n        )\n        =\n        Pr\n        (\n        x\n        \n           came from reference distribution\n        \n        ∣\n        x\n        )\n        .\n      \n    \n    {\\displaystyle D(x)=\\Pr(x{\\text{ came from reference distribution}}\\mid x).}\n  \n\n\n== Training and evaluating GAN ==\n\n\n=== Training ===\n\n\n==== Unstable convergence ====\nWhile the GAN game has a unique global equilibrium point when both the generator and discriminator have access to their entire strategy sets, the equilibrium is no longer guaranteed when they have a restricted strategy set.\nIn practice, the generator has access only to measures of form \n  \n    \n      \n        \n          μ\n          \n            Z\n          \n        \n        ∘\n        \n          G\n          \n            θ\n          \n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\mu _{Z}\\circ G_{\\theta }^{-1}}\n  \n, where \n  \n    \n      \n        \n          G\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle G_{\\theta }}\n  \n is a function computed by a neural network with parameters \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n, and \n  \n    \n      \n        \n          μ\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle \\mu _{Z}}\n  \n is an easily sampled distribution, such as the uniform or normal distribution. Similarly, the discriminator has access only to functions of form \n  \n    \n      \n        \n          D\n          \n            ζ\n          \n        \n      \n    \n    {\\displaystyle D_{\\zeta }}\n  \n, a function computed by a neural network with parameters \n  \n    \n      \n        ζ\n      \n    \n    {\\displaystyle \\zeta }\n  \n. These restricted strategy sets take up a vanishingly small proportion of their entire strategy sets.\nFurther, even if an equilibrium still exists, it can only be found by searching in the high-dimensional space of all possible neural network functions. The standard strategy of using gradient descent to find the equilibrium often does not work for GAN, and often the game \"collapses\" into one of several failure modes. To improve the convergence stability, some training strategies start with an easier task, such as generating low-resolution images or simple images (one object with uniform background), and gradually increase the difficulty of the task during training. This essentially translates to applying a curriculum learning scheme.\n\n\n==== Mode collapse ====\n\nGANs often suffer from mode collapse where they fail to generalize properly, missing entire modes from the input data. For example, a GAN trained on the MNIST dataset containing many samples of each digit might only generate pictures of digit 0. This was termed \"the Helvetica scenario\".\nOne way this can happen is if the generator learns too fast compared to the discriminator. If the discriminator \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is held constant, then the optimal generator would only output elements of \n  \n    \n      \n        arg\n        ⁡\n        \n          max\n          \n            x\n          \n        \n        D\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\arg \\max _{x}D(x)}\n  \n. So for example, if during GAN training for generating MNIST dataset, for a few epochs, the discriminator somehow prefers the digit 0 slightly more than other digits, the generator may seize the opportunity to generate only digit 0, then be unable to escape the local minimum after the discriminator improves.\nSome researchers perceive the root problem to be a weak discriminative network that fails to notice the pattern of omission, while others assign blame to a bad choice of objective function. Many solutions have been proposed, but it is still an open problem.\nEven the state-of-the-art architecture, BigGAN (2019), could not avoid mode collapse. The authors resorted to \"allowing collapse to occur at the later stages of training, by which time a model is sufficiently trained to achieve good results\".\n\n\n==== Two time-scale update rule ====\nThe two time-scale update rule (TTUR) is proposed to make GAN convergence more stable by making the learning rate of the generator lower than that of the discriminator. The authors argued that the generator should move slower than the discriminator, so that it does not \"drive the discriminator steadily into new regions without capturing its gathered information\".\nThey proved that a general class of games that included the GAN game, when trained under TTUR, \"converges under mild assumptions to a stationary local Nash equilibrium\".\nThey also proposed using the Adam stochastic optimization to avoid mode collapse, as well as the Fréchet inception distance for evaluating GAN performances.\n\n\n==== Vanishing gradient ====\nConversely, if the discriminator learns too fast compared to the generator, then the discriminator could almost perfectly distinguish \n  \n    \n      \n        \n          μ\n          \n            \n              G\n              \n                θ\n              \n            \n          \n        \n        ,\n        \n          μ\n          \n            ref\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G_{\\theta }},\\mu _{\\text{ref}}}\n  \n. In such case, the generator \n  \n    \n      \n        \n          G\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle G_{\\theta }}\n  \n could be stuck with a very high loss no matter which direction it changes its \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n, meaning that the gradient \n  \n    \n      \n        \n          ∇\n          \n            θ\n          \n        \n        L\n        (\n        \n          G\n          \n            θ\n          \n        \n        ,\n        \n          D\n          \n            ζ\n          \n        \n        )\n      \n    \n    {\\displaystyle \\nabla _{\\theta }L(G_{\\theta },D_{\\zeta })}\n  \n would be close to zero. In such case, the generator cannot learn, a case of the vanishing gradient problem.\nIntuitively speaking, the discriminator is too good, and since the generator cannot take any small step (only small steps are considered in gradient descent) to improve its payoff, it does not even try.\nOne important method for solving this problem is the Wasserstein GAN.\n\n\n=== Evaluation ===\nGANs are usually evaluated by Inception score (IS), which measures how varied the generator's outputs are (as classified by an image classifier, usually Inception-v3), or Fréchet inception distance (FID), which measures how similar the generator's outputs are to a reference set (as classified by a learned image featurizer, such as Inception-v3 without its final layer). Many papers that propose new GAN architectures for image generation report how their architectures break the state of the art on FID or IS.\nAnother evaluation method is the Learned Perceptual Image Patch Similarity (LPIPS), which starts with a learned image featurizer \n  \n    \n      \n        \n          f\n          \n            θ\n          \n        \n        :\n        \n          Image\n        \n        →\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle f_{\\theta }:{\\text{Image}}\\to \\mathbb {R} ^{n}}\n  \n, and finetunes it by supervised learning on a set of \n  \n    \n      \n        (\n        x\n        ,\n        \n          x\n          ′\n        \n        ,\n        \n          p\n          e\n          r\n          c\n          e\n          p\n          t\n          u\n          a\n          l\n           \n          d\n          i\n          f\n          f\n          e\n          r\n          e\n          n\n          c\n          e\n        \n        ⁡\n        (\n        x\n        ,\n        \n          x\n          ′\n        \n        )\n        )\n      \n    \n    {\\displaystyle (x,x',\\operatorname {perceptual~difference} (x,x'))}\n  \n, where \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is an image, \n  \n    \n      \n        \n          x\n          ′\n        \n      \n    \n    {\\displaystyle x'}\n  \n is a perturbed version of it, and \n  \n    \n      \n        \n          p\n          e\n          r\n          c\n          e\n          p\n          t\n          u\n          a\n          l\n           \n          d\n          i\n          f\n          f\n          e\n          r\n          e\n          n\n          c\n          e\n        \n        ⁡\n        (\n        x\n        ,\n        \n          x\n          ′\n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {perceptual~difference} (x,x')}\n  \n is how much they differ, as reported by human subjects. The model is finetuned so that it can approximate \n  \n    \n      \n        ‖\n        \n          f\n          \n            θ\n          \n        \n        (\n        x\n        )\n        −\n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          ′\n        \n        )\n        ‖\n        ≈\n        \n          p\n          e\n          r\n          c\n          e\n          p\n          t\n          u\n          a\n          l\n           \n          d\n          i\n          f\n          f\n          e\n          r\n          e\n          n\n          c\n          e\n        \n        ⁡\n        (\n        x\n        ,\n        \n          x\n          ′\n        \n        )\n      \n    \n    {\\displaystyle \\|f_{\\theta }(x)-f_{\\theta }(x')\\|\\approx \\operatorname {perceptual~difference} (x,x')}\n  \n. This finetuned model is then used to define \n  \n    \n      \n        LPIPS\n        ⁡\n        (\n        x\n        ,\n        \n          x\n          ′\n        \n        )\n        :=\n        ‖\n        \n          f\n          \n            θ\n          \n        \n        (\n        x\n        )\n        −\n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          ′\n        \n        )\n        ‖\n      \n    \n    {\\displaystyle \\operatorname {LPIPS} (x,x'):=\\|f_{\\theta }(x)-f_{\\theta }(x')\\|}\n  \n.\nOther evaluation methods are reviewed in.\n\n\n== Variants ==\nThere is a veritable zoo of GAN variants. Some of the most prominent are as follows:\n\n\n=== Conditional GAN ===\nConditional GANs are similar to standard GANs except they allow the model to conditionally generate samples based on additional information. For example, if we want to generate a cat face given a dog picture, we could use a conditional GAN.\nThe generator in a GAN game generates \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\mu _{G}}\n  \n, a probability distribution on the probability space \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n. This leads to the idea of a conditional GAN, where instead of generating one probability distribution on \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n, the generator generates a different probability distribution \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n        (\n        c\n        )\n      \n    \n    {\\displaystyle \\mu _{G}(c)}\n  \n on \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n, for each given class label \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n.\nFor example, for generating images that look like ImageNet, the generator should be able to generate a picture of cat when given the class label \"cat\".\nIn the original paper, the authors noted that GAN can be trivially extended to conditional GAN by providing the labels to both the generator and the discriminator.\nConcretely, the conditional GAN game is just the GAN game with class labels provided:\n  \n    \n      \n        L\n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        D\n        )\n        :=\n        \n          E\n          \n            c\n            ∼\n            \n              μ\n              \n                C\n              \n            \n            ,\n            x\n            ∼\n            \n              μ\n              \n                ref\n              \n            \n            (\n            c\n            )\n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        D\n        (\n        x\n        ,\n        c\n        )\n        ]\n        +\n        \n          E\n          \n            c\n            ∼\n            \n              μ\n              \n                C\n              \n            \n            ,\n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n            (\n            c\n            )\n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        (\n        1\n        −\n        D\n        (\n        x\n        ,\n        c\n        )\n        )\n        ]\n      \n    \n    {\\displaystyle L(\\mu _{G},D):=\\operatorname {E} _{c\\sim \\mu _{C},x\\sim \\mu _{\\text{ref}}(c)}[\\ln D(x,c)]+\\operatorname {E} _{c\\sim \\mu _{C},x\\sim \\mu _{G}(c)}[\\ln(1-D(x,c))]}\n  \nwhere \n  \n    \n      \n        \n          μ\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle \\mu _{C}}\n  \n is a probability distribution over classes, \n  \n    \n      \n        \n          μ\n          \n            ref\n          \n        \n        (\n        c\n        )\n      \n    \n    {\\displaystyle \\mu _{\\text{ref}}(c)}\n  \n is the probability distribution of real images of class \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n, and \n  \n    \n      \n        \n          μ\n          \n            G\n          \n        \n        (\n        c\n        )\n      \n    \n    {\\displaystyle \\mu _{G}(c)}\n  \n the probability distribution of images generated by the generator when given class label \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n.\nIn 2017, a conditional GAN learned to generate 1000 image classes of ImageNet.\n\n\n=== GANs with alternative architectures ===\nThe GAN game is a general framework and can be run with any reasonable parametrization of the generator \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n and discriminator \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n. In the original paper, the authors demonstrated it using multilayer perceptron networks and convolutional neural networks. Many alternative architectures have been tried.\nDeep convolutional GAN (DCGAN): For both generator and discriminator, uses only deep networks consisting entirely of convolution-deconvolution layers, that is, fully convolutional networks.\nSelf-attention GAN (SAGAN): Starts with the DCGAN, then adds residually-connected standard self-attention modules to the generator and discriminator.\nVariational autoencoder GAN (VAEGAN): Uses a variational autoencoder (VAE) for the generator.\nTransformer GAN (TransGAN): Uses the pure transformer architecture for both the generator and discriminator, entirely devoid of convolution-deconvolution layers.\nFlow-GAN: Uses flow-based generative model for the generator, allowing efficient computation of the likelihood function.\n\n\n=== GANs with alternative objectives ===\nMany GAN variants are merely obtained by changing the loss functions for the generator and discriminator.\nOriginal GAN:\nWe recast the original GAN objective into a form more convenient for comparison:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    min\n                    \n                      D\n                    \n                  \n                  \n                    L\n                    \n                      D\n                    \n                  \n                  (\n                  D\n                  ,\n                  \n                    μ\n                    \n                      G\n                    \n                  \n                  )\n                  =\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          G\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  D\n                  (\n                  x\n                  )\n                  ]\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          ref\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  (\n                  1\n                  −\n                  D\n                  (\n                  x\n                  )\n                  )\n                  ]\n                \n              \n              \n                \n                  \n                    min\n                    \n                      G\n                    \n                  \n                  \n                    L\n                    \n                      G\n                    \n                  \n                  (\n                  D\n                  ,\n                  \n                    μ\n                    \n                      G\n                    \n                  \n                  )\n                  =\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          G\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  (\n                  1\n                  −\n                  D\n                  (\n                  x\n                  )\n                  )\n                  ]\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}\\min _{D}L_{D}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln D(x)]-\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[\\ln(1-D(x))]\\\\\\min _{G}L_{G}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\end{cases}}}\n  \n\nOriginal GAN, non-saturating loss:\nThis objective for generator was recommended in the original paper for faster convergence.\n  \n    \n      \n        \n          L\n          \n            G\n          \n        \n        =\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        D\n        (\n        x\n        )\n        ]\n      \n    \n    {\\displaystyle L_{G}=\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln D(x)]}\n  \nThe effect of using this objective is analyzed in Section 2.2.2 of Arjovsky et al.\nOriginal GAN, maximum likelihood:\n\n  \n    \n      \n        \n          L\n          \n            G\n          \n        \n        =\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n          \n        \n        ⁡\n        [\n        (\n        \n          exp\n        \n        ∘\n        \n          σ\n          \n            −\n            1\n          \n        \n        ∘\n        D\n        )\n        (\n        x\n        )\n        ]\n      \n    \n    {\\displaystyle L_{G}=\\operatorname {E} _{x\\sim \\mu _{G}}[({\\exp }\\circ \\sigma ^{-1}\\circ D)(x)]}\n  \nwhere \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n is the logistic function. When the discriminator is optimal, the generator gradient is the same as in maximum likelihood estimation, even though GAN cannot perform maximum likelihood estimation itself.\nHinge loss GAN:\n  \n    \n      \n        \n          L\n          \n            D\n          \n        \n        =\n        −\n        \n          E\n          \n            x\n            ∼\n            \n              p\n              \n                ref\n              \n            \n          \n        \n        ⁡\n        \n          [\n          \n            min\n            \n              (\n              \n                0\n                ,\n                −\n                1\n                +\n                D\n                (\n                x\n                )\n              \n              )\n            \n          \n          ]\n        \n        −\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n          \n        \n        ⁡\n        \n          [\n          \n            min\n            \n              (\n              \n                0\n                ,\n                −\n                1\n                −\n                D\n                \n                  (\n                  x\n                  )\n                \n              \n              )\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle L_{D}=-\\operatorname {E} _{x\\sim p_{\\text{ref}}}\\left[\\min \\left(0,-1+D(x)\\right)\\right]-\\operatorname {E} _{x\\sim \\mu _{G}}\\left[\\min \\left(0,-1-D\\left(x\\right)\\right)\\right]}\n  \n\n  \n    \n      \n        \n          L\n          \n            G\n          \n        \n        =\n        −\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n          \n        \n        ⁡\n        [\n        D\n        (\n        x\n        )\n        ]\n      \n    \n    {\\displaystyle L_{G}=-\\operatorname {E} _{x\\sim \\mu _{G}}[D(x)]}\n  \nLeast squares GAN:\n  \n    \n      \n        \n          L\n          \n            D\n          \n        \n        =\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                ref\n              \n            \n          \n        \n        ⁡\n        [\n        (\n        D\n        (\n        x\n        )\n        −\n        b\n        \n          )\n          \n            2\n          \n        \n        ]\n        +\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n          \n        \n        ⁡\n        [\n        (\n        D\n        (\n        x\n        )\n        −\n        a\n        \n          )\n          \n            2\n          \n        \n        ]\n      \n    \n    {\\displaystyle L_{D}=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[(D(x)-b)^{2}]+\\operatorname {E} _{x\\sim \\mu _{G}}[(D(x)-a)^{2}]}\n  \n\n  \n    \n      \n        \n          L\n          \n            G\n          \n        \n        =\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n          \n        \n        ⁡\n        [\n        (\n        D\n        (\n        x\n        )\n        −\n        c\n        \n          )\n          \n            2\n          \n        \n        ]\n      \n    \n    {\\displaystyle L_{G}=\\operatorname {E} _{x\\sim \\mu _{G}}[(D(x)-c)^{2}]}\n  \nwhere \n  \n    \n      \n        a\n        ,\n        b\n        ,\n        c\n      \n    \n    {\\displaystyle a,b,c}\n  \n are parameters to be chosen. The authors recommended \n  \n    \n      \n        a\n        =\n        −\n        1\n        ,\n        b\n        =\n        1\n        ,\n        c\n        =\n        0\n      \n    \n    {\\displaystyle a=-1,b=1,c=0}\n  \n.\n\n\n=== Wasserstein GAN (WGAN) ===\n\nThe Wasserstein GAN modifies the GAN game at two points:\n\nThe discriminator's strategy set is the set of measurable functions of type \n  \n    \n      \n        D\n        :\n        Ω\n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle D:\\Omega \\to \\mathbb {R} }\n  \n with bounded Lipschitz norm: \n  \n    \n      \n        ‖\n        D\n        \n          ‖\n          \n            L\n          \n        \n        ≤\n        K\n      \n    \n    {\\displaystyle \\|D\\|_{L}\\leq K}\n  \n, where \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n is a fixed positive constant.\nThe objective is\n  \n    \n      \n        \n          L\n          \n            W\n            G\n            A\n            N\n          \n        \n        (\n        \n          μ\n          \n            G\n          \n        \n        ,\n        D\n        )\n        :=\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                G\n              \n            \n          \n        \n        ⁡\n        [\n        D\n        (\n        x\n        )\n        ]\n        −\n        \n          \n            E\n          \n          \n            x\n            ∼\n            \n              μ\n              \n                ref\n              \n            \n          \n        \n        [\n        D\n        (\n        x\n        )\n        ]\n      \n    \n    {\\displaystyle L_{WGAN}(\\mu _{G},D):=\\operatorname {E} _{x\\sim \\mu _{G}}[D(x)]-\\mathbb {E} _{x\\sim \\mu _{\\text{ref}}}[D(x)]}\n  \n\nOne of its purposes is to solve the problem of mode collapse (see above). The authors claim \"In no experiment did we see evidence of mode collapse for the WGAN algorithm\".\n\n\n=== GANs with more than two players ===\n\n\n==== Adversarial autoencoder ====\nAn adversarial autoencoder (AAE) is more autoencoder than GAN. The idea is to start with a plain autoencoder, but train a discriminator to discriminate the latent vectors from a reference distribution (often the normal distribution).\n\n\n==== InfoGAN ====\nIn conditional GAN, the generator receives both a noise vector \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n and a label \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n, and produces an image \n  \n    \n      \n        G\n        (\n        z\n        ,\n        c\n        )\n      \n    \n    {\\displaystyle G(z,c)}\n  \n. The discriminator receives image-label pairs \n  \n    \n      \n        (\n        x\n        ,\n        c\n        )\n      \n    \n    {\\displaystyle (x,c)}\n  \n, and computes \n  \n    \n      \n        D\n        (\n        x\n        ,\n        c\n        )\n      \n    \n    {\\displaystyle D(x,c)}\n  \n.\nWhen the training dataset is unlabeled, conditional GAN does not work directly.\nThe idea of InfoGAN is to decree that every latent vector in the latent space can be decomposed as \n  \n    \n      \n        (\n        z\n        ,\n        c\n        )\n      \n    \n    {\\displaystyle (z,c)}\n  \n: an incompressible noise part \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n, and an informative label part \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n, and encourage the generator to comply with the decree, by encouraging it to maximize \n  \n    \n      \n        I\n        (\n        c\n        ,\n        G\n        (\n        z\n        ,\n        c\n        )\n        )\n      \n    \n    {\\displaystyle I(c,G(z,c))}\n  \n, the mutual information between \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n and \n  \n    \n      \n        G\n        (\n        z\n        ,\n        c\n        )\n      \n    \n    {\\displaystyle G(z,c)}\n  \n, while making no demands on the mutual information \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n between \n  \n    \n      \n        G\n        (\n        z\n        ,\n        c\n        )\n      \n    \n    {\\displaystyle G(z,c)}\n  \n.\nUnfortunately, \n  \n    \n      \n        I\n        (\n        c\n        ,\n        G\n        (\n        z\n        ,\n        c\n        )\n        )\n      \n    \n    {\\displaystyle I(c,G(z,c))}\n  \n is intractable in general, The key idea of InfoGAN is Variational Mutual Information Maximization: indirectly maximize it by maximizing a lower bound\n  \n    \n      \n        \n          \n            \n              I\n              ^\n            \n          \n        \n        (\n        G\n        ,\n        Q\n        )\n        =\n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              μ\n              \n                Z\n              \n            \n            ,\n            c\n            ∼\n            \n              μ\n              \n                C\n              \n            \n          \n        \n        [\n        ln\n        ⁡\n        Q\n        (\n        c\n        ∣\n        G\n        (\n        z\n        ,\n        c\n        )\n        )\n        ]\n        ;\n        \n        I\n        (\n        c\n        ,\n        G\n        (\n        z\n        ,\n        c\n        )\n        )\n        ≥\n        \n          sup\n          \n            Q\n          \n        \n        \n          \n            \n              I\n              ^\n            \n          \n        \n        (\n        G\n        ,\n        Q\n        )\n      \n    \n    {\\displaystyle {\\hat {I}}(G,Q)=\\mathbb {E} _{z\\sim \\mu _{Z},c\\sim \\mu _{C}}[\\ln Q(c\\mid G(z,c))];\\quad I(c,G(z,c))\\geq \\sup _{Q}{\\hat {I}}(G,Q)}\n  \nwhere \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n ranges over all Markov kernels of type \n  \n    \n      \n        Q\n        :\n        \n          Ω\n          \n            Y\n          \n        \n        →\n        \n          \n            P\n          \n        \n        (\n        \n          Ω\n          \n            C\n          \n        \n        )\n      \n    \n    {\\displaystyle Q:\\Omega _{Y}\\to {\\mathcal {P}}(\\Omega _{C})}\n  \n.\n\nThe InfoGAN game is defined as follows:Three probability spaces define an InfoGAN game:\n\n  \n    \n      \n        (\n        \n          Ω\n          \n            X\n          \n        \n        ,\n        \n          μ\n          \n            ref\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega _{X},\\mu _{\\text{ref}})}\n  \n, the space of reference images.\n\n  \n    \n      \n        (\n        \n          Ω\n          \n            Z\n          \n        \n        ,\n        \n          μ\n          \n            Z\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega _{Z},\\mu _{Z})}\n  \n, the fixed random noise generator.\n\n  \n    \n      \n        (\n        \n          Ω\n          \n            C\n          \n        \n        ,\n        \n          μ\n          \n            C\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega _{C},\\mu _{C})}\n  \n, the fixed random information generator.\nThere are 3 players in 2 teams: generator, Q, and discriminator. The generator and Q are on one team, and the discriminator on the other team.\nThe objective function is\n  \n    \n      \n        L\n        (\n        G\n        ,\n        Q\n        ,\n        D\n        )\n        =\n        \n          L\n          \n            G\n            A\n            N\n          \n        \n        (\n        G\n        ,\n        D\n        )\n        −\n        λ\n        \n          \n            \n              I\n              ^\n            \n          \n        \n        (\n        G\n        ,\n        Q\n        )\n      \n    \n    {\\displaystyle L(G,Q,D)=L_{GAN}(G,D)-\\lambda {\\hat {I}}(G,Q)}\n  \nwhere \n  \n    \n      \n        \n          L\n          \n            G\n            A\n            N\n          \n        \n        (\n        G\n        ,\n        D\n        )\n        =\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                ref\n              \n            \n            ,\n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        D\n        (\n        x\n        )\n        ]\n        +\n        \n          E\n          \n            z\n            ∼\n            \n              μ\n              \n                Z\n              \n            \n          \n        \n        ⁡\n        [\n        ln\n        ⁡\n        (\n        1\n        −\n        D\n        (\n        G\n        (\n        z\n        ,\n        c\n        )\n        )\n        )\n        ]\n      \n    \n    {\\displaystyle L_{GAN}(G,D)=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},}[\\ln D(x)]+\\operatorname {E} _{z\\sim \\mu _{Z}}[\\ln(1-D(G(z,c)))]}\n  \n is the original GAN game objective, and \n  \n    \n      \n        \n          \n            \n              I\n              ^\n            \n          \n        \n        (\n        G\n        ,\n        Q\n        )\n        =\n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              μ\n              \n                Z\n              \n            \n            ,\n            c\n            ∼\n            \n              μ\n              \n                C\n              \n            \n          \n        \n        [\n        ln\n        ⁡\n        Q\n        (\n        c\n        ∣\n        G\n        (\n        z\n        ,\n        c\n        )\n        )\n        ]\n      \n    \n    {\\displaystyle {\\hat {I}}(G,Q)=\\mathbb {E} _{z\\sim \\mu _{Z},c\\sim \\mu _{C}}[\\ln Q(c\\mid G(z,c))]}\n  \n\nGenerator-Q team aims to minimize the objective, and discriminator aims to maximize it:\n  \n    \n      \n        \n          min\n          \n            G\n            ,\n            Q\n          \n        \n        \n          max\n          \n            D\n          \n        \n        L\n        (\n        G\n        ,\n        Q\n        ,\n        D\n        )\n      \n    \n    {\\displaystyle \\min _{G,Q}\\max _{D}L(G,Q,D)}\n  \n\n\n==== Bidirectional GAN (BiGAN) ====\nThe standard GAN generator is a function of type \n  \n    \n      \n        G\n        :\n        \n          Ω\n          \n            Z\n          \n        \n        →\n        \n          Ω\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle G:\\Omega _{Z}\\to \\Omega _{X}}\n  \n, that is, it is a mapping from a latent space \n  \n    \n      \n        \n          Ω\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle \\Omega _{Z}}\n  \n to the image space \n  \n    \n      \n        \n          Ω\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\Omega _{X}}\n  \n. This can be understood as a \"decoding\" process, whereby every latent vector \n  \n    \n      \n        z\n        ∈\n        \n          Ω\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle z\\in \\Omega _{Z}}\n  \n is a code for an image \n  \n    \n      \n        x\n        ∈\n        \n          Ω\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle x\\in \\Omega _{X}}\n  \n, and the generator performs the decoding. This naturally leads to the idea of training another network that performs \"encoding\", creating an autoencoder out of the encoder-generator pair.\nAlready in the original paper, the authors noted that \"Learned approximate inference can be performed by training an auxiliary network to predict \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n given \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n\". The bidirectional GAN architecture performs exactly this.\n\nThe BiGAN is defined as follows: Two probability spaces define a BiGAN game:\n\n  \n    \n      \n        (\n        \n          Ω\n          \n            X\n          \n        \n        ,\n        \n          μ\n          \n            X\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega _{X},\\mu _{X})}\n  \n, the space of reference images.\n\n  \n    \n      \n        (\n        \n          Ω\n          \n            Z\n          \n        \n        ,\n        \n          μ\n          \n            Z\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega _{Z},\\mu _{Z})}\n  \n, the latent space.\nThere are 3 players in 2 teams: generator, encoder, and discriminator. The generator and encoder are on one team, and the discriminator on the other team.\nThe generator's strategies are functions \n  \n    \n      \n        G\n        :\n        \n          Ω\n          \n            Z\n          \n        \n        →\n        \n          Ω\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle G:\\Omega _{Z}\\to \\Omega _{X}}\n  \n, and the encoder's strategies are functions \n  \n    \n      \n        E\n        :\n        \n          Ω\n          \n            X\n          \n        \n        →\n        \n          Ω\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle E:\\Omega _{X}\\to \\Omega _{Z}}\n  \n. The discriminator's strategies are functions \n  \n    \n      \n        D\n        :\n        \n          Ω\n          \n            X\n          \n        \n        →\n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle D:\\Omega _{X}\\to [0,1]}\n  \n.\nThe objective function is\n  \n    \n      \n        L\n        (\n        G\n        ,\n        E\n        ,\n        D\n        )\n        =\n        \n          \n            E\n          \n          \n            x\n            ∼\n            \n              μ\n              \n                X\n              \n            \n          \n        \n        [\n        ln\n        ⁡\n        D\n        (\n        x\n        ,\n        E\n        (\n        x\n        )\n        )\n        ]\n        +\n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              μ\n              \n                Z\n              \n            \n          \n        \n        [\n        ln\n        ⁡\n        (\n        1\n        −\n        D\n        (\n        G\n        (\n        z\n        )\n        ,\n        z\n        )\n        )\n        ]\n      \n    \n    {\\displaystyle L(G,E,D)=\\mathbb {E} _{x\\sim \\mu _{X}}[\\ln D(x,E(x))]+\\mathbb {E} _{z\\sim \\mu _{Z}}[\\ln(1-D(G(z),z))]}\n  \n\nGenerator-encoder team aims to minimize the objective, and discriminator aims to maximize it:\n  \n    \n      \n        \n          min\n          \n            G\n            ,\n            E\n          \n        \n        \n          max\n          \n            D\n          \n        \n        L\n        (\n        G\n        ,\n        E\n        ,\n        D\n        )\n      \n    \n    {\\displaystyle \\min _{G,E}\\max _{D}L(G,E,D)}\n  \n In the paper, they gave a more abstract definition of the objective as:\n  \n    \n      \n        L\n        (\n        G\n        ,\n        E\n        ,\n        D\n        )\n        =\n        \n          \n            E\n          \n          \n            (\n            x\n            ,\n            z\n            )\n            ∼\n            \n              μ\n              \n                E\n                ,\n                X\n              \n            \n          \n        \n        [\n        ln\n        ⁡\n        D\n        (\n        x\n        ,\n        z\n        )\n        ]\n        +\n        \n          \n            E\n          \n          \n            (\n            x\n            ,\n            z\n            )\n            ∼\n            \n              μ\n              \n                G\n                ,\n                Z\n              \n            \n          \n        \n        [\n        ln\n        ⁡\n        (\n        1\n        −\n        D\n        (\n        x\n        ,\n        z\n        )\n        )\n        ]\n      \n    \n    {\\displaystyle L(G,E,D)=\\mathbb {E} _{(x,z)\\sim \\mu _{E,X}}[\\ln D(x,z)]+\\mathbb {E} _{(x,z)\\sim \\mu _{G,Z}}[\\ln(1-D(x,z))]}\n  \nwhere \n  \n    \n      \n        \n          μ\n          \n            E\n            ,\n            X\n          \n        \n        (\n        d\n        x\n        ,\n        d\n        z\n        )\n        =\n        \n          μ\n          \n            X\n          \n        \n        (\n        d\n        x\n        )\n        ⋅\n        \n          δ\n          \n            E\n            (\n            x\n            )\n          \n        \n        (\n        d\n        z\n        )\n      \n    \n    {\\displaystyle \\mu _{E,X}(dx,dz)=\\mu _{X}(dx)\\cdot \\delta _{E(x)}(dz)}\n  \n is the probability distribution on \n  \n    \n      \n        \n          Ω\n          \n            X\n          \n        \n        ×\n        \n          Ω\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle \\Omega _{X}\\times \\Omega _{Z}}\n  \n obtained by pushing \n  \n    \n      \n        \n          μ\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\mu _{X}}\n  \n forward via \n  \n    \n      \n        x\n        ↦\n        (\n        x\n        ,\n        E\n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle x\\mapsto (x,E(x))}\n  \n, and \n  \n    \n      \n        \n          μ\n          \n            G\n            ,\n            Z\n          \n        \n        (\n        d\n        x\n        ,\n        d\n        z\n        )\n        =\n        \n          δ\n          \n            G\n            (\n            z\n            )\n          \n        \n        (\n        d\n        x\n        )\n        ⋅\n        \n          μ\n          \n            Z\n          \n        \n        (\n        d\n        z\n        )\n      \n    \n    {\\displaystyle \\mu _{G,Z}(dx,dz)=\\delta _{G(z)}(dx)\\cdot \\mu _{Z}(dz)}\n  \n is the probability distribution on \n  \n    \n      \n        \n          Ω\n          \n            X\n          \n        \n        ×\n        \n          Ω\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle \\Omega _{X}\\times \\Omega _{Z}}\n  \n obtained by pushing \n  \n    \n      \n        \n          μ\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle \\mu _{Z}}\n  \n forward via \n  \n    \n      \n        z\n        ↦\n        (\n        G\n        (\n        x\n        )\n        ,\n        z\n        )\n      \n    \n    {\\displaystyle z\\mapsto (G(x),z)}\n  \n.\nApplications of bidirectional models include semi-supervised learning, interpretable machine learning, and neural machine translation.\n\n\n==== CycleGAN ====\nCycleGAN is an architecture for performing translations between two domains, such as between photos of horses and photos of zebras, or photos of night cities and photos of day cities.\n\nThe CycleGAN game is defined as follows:There are two probability spaces \n  \n    \n      \n        (\n        \n          Ω\n          \n            X\n          \n        \n        ,\n        \n          μ\n          \n            X\n          \n        \n        )\n        ,\n        (\n        \n          Ω\n          \n            Y\n          \n        \n        ,\n        \n          μ\n          \n            Y\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega _{X},\\mu _{X}),(\\Omega _{Y},\\mu _{Y})}\n  \n, corresponding to the two domains needed for translations fore-and-back.\nThere are 4 players in 2 teams: generators \n  \n    \n      \n        \n          G\n          \n            X\n          \n        \n        :\n        \n          Ω\n          \n            X\n          \n        \n        →\n        \n          Ω\n          \n            Y\n          \n        \n        ,\n        \n          G\n          \n            Y\n          \n        \n        :\n        \n          Ω\n          \n            Y\n          \n        \n        →\n        \n          Ω\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle G_{X}:\\Omega _{X}\\to \\Omega _{Y},G_{Y}:\\Omega _{Y}\\to \\Omega _{X}}\n  \n, and discriminators \n  \n    \n      \n        \n          D\n          \n            X\n          \n        \n        :\n        \n          Ω\n          \n            X\n          \n        \n        →\n        [\n        0\n        ,\n        1\n        ]\n        ,\n        \n          D\n          \n            Y\n          \n        \n        :\n        \n          Ω\n          \n            Y\n          \n        \n        →\n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle D_{X}:\\Omega _{X}\\to [0,1],D_{Y}:\\Omega _{Y}\\to [0,1]}\n  \n.\nThe objective function is\n  \n    \n      \n        L\n        (\n        \n          G\n          \n            X\n          \n        \n        ,\n        \n          G\n          \n            Y\n          \n        \n        ,\n        \n          D\n          \n            X\n          \n        \n        ,\n        \n          D\n          \n            Y\n          \n        \n        )\n        =\n        \n          L\n          \n            G\n            A\n            N\n          \n        \n        (\n        \n          G\n          \n            X\n          \n        \n        ,\n        \n          D\n          \n            X\n          \n        \n        )\n        +\n        \n          L\n          \n            G\n            A\n            N\n          \n        \n        (\n        \n          G\n          \n            Y\n          \n        \n        ,\n        \n          D\n          \n            Y\n          \n        \n        )\n        +\n        λ\n        \n          L\n          \n            c\n            y\n            c\n            l\n            e\n          \n        \n        (\n        \n          G\n          \n            X\n          \n        \n        ,\n        \n          G\n          \n            Y\n          \n        \n        )\n      \n    \n    {\\displaystyle L(G_{X},G_{Y},D_{X},D_{Y})=L_{GAN}(G_{X},D_{X})+L_{GAN}(G_{Y},D_{Y})+\\lambda L_{cycle}(G_{X},G_{Y})}\n  \n\nwhere \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n is a positive adjustable parameter, \n  \n    \n      \n        \n          L\n          \n            G\n            A\n            N\n          \n        \n      \n    \n    {\\displaystyle L_{GAN}}\n  \n is the GAN game objective, and \n  \n    \n      \n        \n          L\n          \n            c\n            y\n            c\n            l\n            e\n          \n        \n      \n    \n    {\\displaystyle L_{cycle}}\n  \n is the cycle consistency loss:\n  \n    \n      \n        \n          L\n          \n            c\n            y\n            c\n            l\n            e\n          \n        \n        (\n        \n          G\n          \n            X\n          \n        \n        ,\n        \n          G\n          \n            Y\n          \n        \n        )\n        =\n        \n          E\n          \n            x\n            ∼\n            \n              μ\n              \n                X\n              \n            \n          \n        \n        ‖\n        \n          G\n          \n            X\n          \n        \n        (\n        \n          G\n          \n            Y\n          \n        \n        (\n        x\n        )\n        )\n        −\n        x\n        ‖\n        +\n        \n          E\n          \n            y\n            ∼\n            \n              μ\n              \n                Y\n              \n            \n          \n        \n        ‖\n        \n          G\n          \n            Y\n          \n        \n        (\n        \n          G\n          \n            X\n          \n        \n        (\n        y\n        )\n        )\n        −\n        y\n        ‖\n      \n    \n    {\\displaystyle L_{cycle}(G_{X},G_{Y})=E_{x\\sim \\mu _{X}}\\|G_{X}(G_{Y}(x))-x\\|+E_{y\\sim \\mu _{Y}}\\|G_{Y}(G_{X}(y))-y\\|}\n  \nThe generators aim to minimize the objective, and the discriminators aim to maximize it:\n  \n    \n      \n        \n          min\n          \n            \n              G\n              \n                X\n              \n            \n            ,\n            \n              G\n              \n                Y\n              \n            \n          \n        \n        \n          max\n          \n            \n              D\n              \n                X\n              \n            \n            ,\n            \n              D\n              \n                Y\n              \n            \n          \n        \n        L\n        (\n        \n          G\n          \n            X\n          \n        \n        ,\n        \n          G\n          \n            Y\n          \n        \n        ,\n        \n          D\n          \n            X\n          \n        \n        ,\n        \n          D\n          \n            Y\n          \n        \n        )\n      \n    \n    {\\displaystyle \\min _{G_{X},G_{Y}}\\max _{D_{X},D_{Y}}L(G_{X},G_{Y},D_{X},D_{Y})}\n  \n  Unlike previous work like pix2pix, which requires paired training data, cycleGAN requires no paired data. For example, to train a pix2pix model to turn a summer scenery photo to winter scenery photo and back, the dataset must contain pairs of the same place in summer and winter, shot at the same angle; cycleGAN would only need a set of summer scenery photos, and an unrelated set of winter scenery photos.\n\n\n=== GANs with particularly large or small scales ===\n\n\n==== BigGAN ====\nThe BigGAN is essentially a self-attention GAN trained on a large scale (up to 80 million parameters) to generate large images of ImageNet (up to 512 x 512 resolution), with numerous engineering tricks to make it converge.\n\n\n==== Invertible data augmentation ====\nWhen there is insufficient training data, the reference distribution \n  \n    \n      \n        \n          μ\n          \n            ref\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{ref}}}\n  \n cannot be well-approximated by the empirical distribution given by the training dataset. In such cases, data augmentation can be applied, to allow training GAN on smaller datasets. Naïve data augmentation, however, brings its problems.\nConsider the original GAN game, slightly reformulated as follows:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    min\n                    \n                      D\n                    \n                  \n                  \n                    L\n                    \n                      D\n                    \n                  \n                  (\n                  D\n                  ,\n                  \n                    μ\n                    \n                      G\n                    \n                  \n                  )\n                  =\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          ref\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  D\n                  (\n                  x\n                  )\n                  ]\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          G\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  (\n                  1\n                  −\n                  D\n                  (\n                  x\n                  )\n                  )\n                  ]\n                \n              \n              \n                \n                  \n                    min\n                    \n                      G\n                    \n                  \n                  \n                    L\n                    \n                      G\n                    \n                  \n                  (\n                  D\n                  ,\n                  \n                    μ\n                    \n                      G\n                    \n                  \n                  )\n                  =\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          G\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  (\n                  1\n                  −\n                  D\n                  (\n                  x\n                  )\n                  )\n                  ]\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}\\min _{D}L_{D}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[\\ln D(x)]-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\\\\\min _{G}L_{G}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\end{cases}}}\n  \nNow we use data augmentation by randomly sampling semantic-preserving transforms \n  \n    \n      \n        T\n        :\n        Ω\n        →\n        Ω\n      \n    \n    {\\displaystyle T:\\Omega \\to \\Omega }\n  \n and applying them to the dataset, to obtain the reformulated GAN game:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    min\n                    \n                      D\n                    \n                  \n                  \n                    L\n                    \n                      D\n                    \n                  \n                  (\n                  D\n                  ,\n                  \n                    μ\n                    \n                      G\n                    \n                  \n                  )\n                  =\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          ref\n                        \n                      \n                      ,\n                      T\n                      ∼\n                      \n                        μ\n                        \n                          trans\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  D\n                  (\n                  T\n                  (\n                  x\n                  )\n                  )\n                  ]\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          G\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  (\n                  1\n                  −\n                  D\n                  (\n                  x\n                  )\n                  )\n                  ]\n                \n              \n              \n                \n                  \n                    min\n                    \n                      G\n                    \n                  \n                  \n                    L\n                    \n                      G\n                    \n                  \n                  (\n                  D\n                  ,\n                  \n                    μ\n                    \n                      G\n                    \n                  \n                  )\n                  =\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          G\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  (\n                  1\n                  −\n                  D\n                  (\n                  x\n                  )\n                  )\n                  ]\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}\\min _{D}L_{D}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},T\\sim \\mu _{\\text{trans}}}[\\ln D(T(x))]-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\\\\\min _{G}L_{G}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\end{cases}}}\n  \nThis is equivalent to a GAN game with a different distribution \n  \n    \n      \n        \n          μ\n          \n            ref\n          \n          ′\n        \n      \n    \n    {\\displaystyle \\mu _{\\text{ref}}'}\n  \n, sampled by \n  \n    \n      \n        T\n        (\n        x\n        )\n      \n    \n    {\\displaystyle T(x)}\n  \n, with \n  \n    \n      \n        x\n        ∼\n        \n          μ\n          \n            ref\n          \n        \n        ,\n        T\n        ∼\n        \n          μ\n          \n            trans\n          \n        \n      \n    \n    {\\displaystyle x\\sim \\mu _{\\text{ref}},T\\sim \\mu _{\\text{trans}}}\n  \n. For example, if \n  \n    \n      \n        \n          μ\n          \n            ref\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{ref}}}\n  \n is the distribution of images in ImageNet, and \n  \n    \n      \n        \n          μ\n          \n            trans\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{trans}}}\n  \n samples identity-transform with probability 0.5, and horizontal-reflection with probability 0.5, then \n  \n    \n      \n        \n          μ\n          \n            ref\n          \n          ′\n        \n      \n    \n    {\\displaystyle \\mu _{\\text{ref}}'}\n  \n is the distribution of images in ImageNet and horizontally-reflected ImageNet, combined.\nThe result of such training would be a generator that mimics \n  \n    \n      \n        \n          μ\n          \n            ref\n          \n          ′\n        \n      \n    \n    {\\displaystyle \\mu _{\\text{ref}}'}\n  \n. For example, it would generate images that look like they are randomly cropped, if the data augmentation uses random cropping.\nThe solution is to apply data augmentation to both generated and real images:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    min\n                    \n                      D\n                    \n                  \n                  \n                    L\n                    \n                      D\n                    \n                  \n                  (\n                  D\n                  ,\n                  \n                    μ\n                    \n                      G\n                    \n                  \n                  )\n                  =\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          ref\n                        \n                      \n                      ,\n                      T\n                      ∼\n                      \n                        μ\n                        \n                          trans\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  D\n                  (\n                  T\n                  (\n                  x\n                  )\n                  )\n                  ]\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          G\n                        \n                      \n                      ,\n                      T\n                      ∼\n                      \n                        μ\n                        \n                          trans\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  (\n                  1\n                  −\n                  D\n                  (\n                  T\n                  (\n                  x\n                  )\n                  )\n                  )\n                  ]\n                \n              \n              \n                \n                  \n                    min\n                    \n                      G\n                    \n                  \n                  \n                    L\n                    \n                      G\n                    \n                  \n                  (\n                  D\n                  ,\n                  \n                    μ\n                    \n                      G\n                    \n                  \n                  )\n                  =\n                  −\n                  \n                    E\n                    \n                      x\n                      ∼\n                      \n                        μ\n                        \n                          G\n                        \n                      \n                      ,\n                      T\n                      ∼\n                      \n                        μ\n                        \n                          trans\n                        \n                      \n                    \n                  \n                  ⁡\n                  [\n                  ln\n                  ⁡\n                  (\n                  1\n                  −\n                  D\n                  (\n                  T\n                  (\n                  x\n                  )\n                  )\n                  )\n                  ]\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}\\min _{D}L_{D}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},T\\sim \\mu _{\\text{trans}}}[\\ln D(T(x))]-\\operatorname {E} _{x\\sim \\mu _{G},T\\sim \\mu _{\\text{trans}}}[\\ln(1-D(T(x)))]\\\\\\min _{G}L_{G}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G},T\\sim \\mu _{\\text{trans}}}[\\ln(1-D(T(x)))]\\end{cases}}}\n  \nThe authors demonstrated high-quality generation using just 100-picture-large datasets.\nThe StyleGAN-2-ADA paper points out a further point on data augmentation: it must be invertible. Continue with the example of generating ImageNet pictures. If the data augmentation is \"randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability\", then there is no way for the generator to know which is the true orientation: Consider two generators \n  \n    \n      \n        G\n        ,\n        \n          G\n          ′\n        \n      \n    \n    {\\displaystyle G,G'}\n  \n, such that for any latent \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n, the generated image \n  \n    \n      \n        G\n        (\n        z\n        )\n      \n    \n    {\\displaystyle G(z)}\n  \n is a 90-degree rotation of \n  \n    \n      \n        \n          G\n          ′\n        \n        (\n        z\n        )\n      \n    \n    {\\displaystyle G'(z)}\n  \n. They would have exactly the same expected loss, and so neither is preferred over the other.\nThe solution is to only use invertible data augmentation: instead of \"randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability\", use \"randomly rotate the picture by 90, 180, 270 degrees with 0.1 probability, and keep the picture as it is with 0.7 probability\". This way, the generator is still rewarded  to keep images oriented the same way as un-augmented ImageNet pictures.\nAbstractly, the effect of randomly sampling transformations \n  \n    \n      \n        T\n        :\n        Ω\n        →\n        Ω\n      \n    \n    {\\displaystyle T:\\Omega \\to \\Omega }\n  \n from the distribution \n  \n    \n      \n        \n          μ\n          \n            trans\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{trans}}}\n  \n is to define a Markov kernel \n  \n    \n      \n        \n          K\n          \n            trans\n          \n        \n        :\n        Ω\n        →\n        \n          \n            P\n          \n        \n        (\n        Ω\n        )\n      \n    \n    {\\displaystyle K_{\\text{trans}}:\\Omega \\to {\\mathcal {P}}(\\Omega )}\n  \n. Then, the data-augmented GAN game pushes the generator to find some \n  \n    \n      \n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            G\n          \n        \n        ∈\n        \n          \n            P\n          \n        \n        (\n        Ω\n        )\n      \n    \n    {\\displaystyle {\\hat {\\mu }}_{G}\\in {\\mathcal {P}}(\\Omega )}\n  \n, such that \n  \n    \n      \n        \n          K\n          \n            trans\n          \n        \n        ∗\n        \n          μ\n          \n            ref\n          \n        \n        =\n        \n          K\n          \n            trans\n          \n        \n        ∗\n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            G\n          \n        \n      \n    \n    {\\displaystyle K_{\\text{trans}}*\\mu _{\\text{ref}}=K_{\\text{trans}}*{\\hat {\\mu }}_{G}}\n  \nwhere \n  \n    \n      \n        ∗\n      \n    \n    {\\displaystyle *}\n  \n is the Markov kernel convolution.\nA data-augmentation method is defined to be invertible if its Markov kernel \n  \n    \n      \n        \n          K\n          \n            trans\n          \n        \n      \n    \n    {\\displaystyle K_{\\text{trans}}}\n  \n satisfies\n  \n    \n      \n        \n          K\n          \n            trans\n          \n        \n        ∗\n        μ\n        =\n        \n          K\n          \n            trans\n          \n        \n        ∗\n        \n          μ\n          ′\n        \n        \n        ⟹\n        \n        μ\n        =\n        \n          μ\n          ′\n        \n        \n        ∀\n        μ\n        ,\n        \n          μ\n          ′\n        \n        ∈\n        \n          \n            P\n          \n        \n        (\n        Ω\n        )\n      \n    \n    {\\displaystyle K_{\\text{trans}}*\\mu =K_{\\text{trans}}*\\mu '\\implies \\mu =\\mu '\\quad \\forall \\mu ,\\mu '\\in {\\mathcal {P}}(\\Omega )}\n  \nImmediately by definition, we see that composing multiple invertible data-augmentation methods results in yet another invertible method. Also by definition, if the data-augmentation method is invertible, then using it in a GAN game does not change the optimal strategy \n  \n    \n      \n        \n          \n            \n              \n                μ\n                ^\n              \n            \n          \n          \n            G\n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mu }}_{G}}\n  \n for the generator, which is still \n  \n    \n      \n        \n          μ\n          \n            ref\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{ref}}}\n  \n.\nThere are two prototypical examples of invertible Markov kernels:\nDiscrete case: Invertible stochastic matrices, when \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n is finite.\nFor example, if \n  \n    \n      \n        Ω\n        =\n        {\n        ↑\n        ,\n        ↓\n        ,\n        ←\n        ,\n        →\n        }\n      \n    \n    {\\displaystyle \\Omega =\\{\\uparrow ,\\downarrow ,\\leftarrow ,\\rightarrow \\}}\n  \n is the set of four images of an arrow, pointing in 4 directions, and the data augmentation is \"randomly rotate the picture by 90, 180, 270 degrees with probability \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, and keep the picture as it is with probability \n  \n    \n      \n        (\n        1\n        −\n        3\n        p\n        )\n      \n    \n    {\\displaystyle (1-3p)}\n  \n\", then the Markov kernel \n  \n    \n      \n        \n          K\n          \n            trans\n          \n        \n      \n    \n    {\\displaystyle K_{\\text{trans}}}\n  \n can be represented as a stochastic matrix:\n  \n    \n      \n        [\n        \n          K\n          \n            trans\n          \n        \n        ]\n        =\n        \n          \n            [\n            \n              \n                \n                  (\n                  1\n                  −\n                  3\n                  p\n                  )\n                \n                \n                  p\n                \n                \n                  p\n                \n                \n                  p\n                \n              \n              \n                \n                  p\n                \n                \n                  (\n                  1\n                  −\n                  3\n                  p\n                  )\n                \n                \n                  p\n                \n                \n                  p\n                \n              \n              \n                \n                  p\n                \n                \n                  p\n                \n                \n                  (\n                  1\n                  −\n                  3\n                  p\n                  )\n                \n                \n                  p\n                \n              \n              \n                \n                  p\n                \n                \n                  p\n                \n                \n                  p\n                \n                \n                  (\n                  1\n                  −\n                  3\n                  p\n                  )\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle [K_{\\text{trans}}]={\\begin{bmatrix}(1-3p)&p&p&p\\\\p&(1-3p)&p&p\\\\p&p&(1-3p)&p\\\\p&p&p&(1-3p)\\end{bmatrix}}}\n  \n and \n  \n    \n      \n        \n          K\n          \n            trans\n          \n        \n      \n    \n    {\\displaystyle K_{\\text{trans}}}\n  \n is an invertible kernel iff \n  \n    \n      \n        [\n        \n          K\n          \n            trans\n          \n        \n        ]\n      \n    \n    {\\displaystyle [K_{\\text{trans}}]}\n  \n is an invertible matrix, that is, \n  \n    \n      \n        p\n        ≠\n        1\n        \n          /\n        \n        4\n      \n    \n    {\\displaystyle p\\neq 1/4}\n  \n.\nContinuous case: The gaussian kernel, when \n  \n    \n      \n        Ω\n        =\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\Omega =\\mathbb {R} ^{n}}\n  \n for some \n  \n    \n      \n        n\n        ≥\n        1\n      \n    \n    {\\displaystyle n\\geq 1}\n  \n.\nFor example, if \n  \n    \n      \n        Ω\n        =\n        \n          \n            R\n          \n          \n            \n              256\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\Omega =\\mathbb {R} ^{256^{2}}}\n  \n is the space of 256x256 images, and the data-augmentation method is \"generate a gaussian noise \n  \n    \n      \n        z\n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        \n          I\n          \n            \n              256\n              \n                2\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle z\\sim {\\mathcal {N}}(0,I_{256^{2}})}\n  \n, then add \n  \n    \n      \n        ϵ\n        z\n      \n    \n    {\\displaystyle \\epsilon z}\n  \n to the image\", then \n  \n    \n      \n        \n          K\n          \n            trans\n          \n        \n      \n    \n    {\\displaystyle K_{\\text{trans}}}\n  \n is just convolution by the density function of \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        \n          ϵ\n          \n            2\n          \n        \n        \n          I\n          \n            \n              256\n              \n                2\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(0,\\epsilon ^{2}I_{256^{2}})}\n  \n. This is invertible, because convolution by a gaussian is just convolution by the heat kernel, so given any \n  \n    \n      \n        μ\n        ∈\n        \n          \n            P\n          \n        \n        (\n        \n          \n            R\n          \n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mu \\in {\\mathcal {P}}(\\mathbb {R} ^{n})}\n  \n, the convolved distribution \n  \n    \n      \n        \n          K\n          \n            trans\n          \n        \n        ∗\n        μ\n      \n    \n    {\\displaystyle K_{\\text{trans}}*\\mu }\n  \n can be obtained by heating up \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n}}\n  \n precisely according to \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n, then wait for time \n  \n    \n      \n        \n          ϵ\n          \n            2\n          \n        \n        \n          /\n        \n        4\n      \n    \n    {\\displaystyle \\epsilon ^{2}/4}\n  \n. With that, we can recover \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n by running the heat equation backwards in time for \n  \n    \n      \n        \n          ϵ\n          \n            2\n          \n        \n        \n          /\n        \n        4\n      \n    \n    {\\displaystyle \\epsilon ^{2}/4}\n  \n.\nMore examples of invertible data augmentations are found in the paper.\n\n\n==== SinGAN ====\nSinGAN pushes data augmentation to the limit, by using only a single image as training data and performing data augmentation on it. The GAN architecture is adapted to this training method by using a multi-scale pipeline.\nThe generator \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n is decomposed into a pyramid of generators \n  \n    \n      \n        G\n        =\n        \n          G\n          \n            1\n          \n        \n        ∘\n        \n          G\n          \n            2\n          \n        \n        ∘\n        ⋯\n        ∘\n        \n          G\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle G=G_{1}\\circ G_{2}\\circ \\cdots \\circ G_{N}}\n  \n, with the lowest one generating the image \n  \n    \n      \n        \n          G\n          \n            N\n          \n        \n        (\n        \n          z\n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle G_{N}(z_{N})}\n  \n at the lowest resolution, then the generated image is scaled up to \n  \n    \n      \n        r\n        (\n        \n          G\n          \n            N\n          \n        \n        (\n        \n          z\n          \n            N\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle r(G_{N}(z_{N}))}\n  \n, and fed to the next level to generate an image \n  \n    \n      \n        \n          G\n          \n            N\n            −\n            1\n          \n        \n        (\n        \n          z\n          \n            N\n            −\n            1\n          \n        \n        +\n        r\n        (\n        \n          G\n          \n            N\n          \n        \n        (\n        \n          z\n          \n            N\n          \n        \n        )\n        )\n        )\n      \n    \n    {\\displaystyle G_{N-1}(z_{N-1}+r(G_{N}(z_{N})))}\n  \n at a higher resolution, and so on. The discriminator is decomposed into a pyramid as well.\n\n\n=== StyleGAN series ===\n\nThe StyleGAN family is a series of architectures published by Nvidia's research division.\n\n\n==== Progressive GAN ====\nProgressive GAN is a method for training GAN for large-scale image generation stably, by growing a GAN generator from small to large scale in a pyramidal fashion. Like SinGAN, it decomposes the generator as\n  \n    \n      \n        G\n        =\n        \n          G\n          \n            1\n          \n        \n        ∘\n        \n          G\n          \n            2\n          \n        \n        ∘\n        ⋯\n        ∘\n        \n          G\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle G=G_{1}\\circ G_{2}\\circ \\cdots \\circ G_{N}}\n  \n, and the discriminator as \n  \n    \n      \n        D\n        =\n        \n          D\n          \n            1\n          \n        \n        ∘\n        \n          D\n          \n            2\n          \n        \n        ∘\n        ⋯\n        ∘\n        \n          D\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle D=D_{1}\\circ D_{2}\\circ \\cdots \\circ D_{N}}\n  \n.\nDuring training, at first only \n  \n    \n      \n        \n          G\n          \n            N\n          \n        \n        ,\n        \n          D\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle G_{N},D_{N}}\n  \n are used in a GAN game to generate 4x4 images. Then \n  \n    \n      \n        \n          G\n          \n            N\n            −\n            1\n          \n        \n        ,\n        \n          D\n          \n            N\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle G_{N-1},D_{N-1}}\n  \n are added to reach the second stage of GAN game, to generate 8x8 images, and so on, until we reach a GAN game to generate 1024x1024 images.\nTo avoid shock between stages of the GAN game, each new layer is \"blended in\" (Figure 2 of the paper). For example, this is how the second stage GAN game starts:\n\nJust before, the GAN game consists of the pair \n  \n    \n      \n        \n          G\n          \n            N\n          \n        \n        ,\n        \n          D\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle G_{N},D_{N}}\n  \n generating and discriminating 4x4 images.\nJust after, the GAN game consists of the pair \n  \n    \n      \n        (\n        (\n        1\n        −\n        α\n        )\n        +\n        α\n        ⋅\n        \n          G\n          \n            N\n            −\n            1\n          \n        \n        )\n        ∘\n        u\n        ∘\n        \n          G\n          \n            N\n          \n        \n        ,\n        \n          D\n          \n            N\n          \n        \n        ∘\n        d\n        ∘\n        (\n        (\n        1\n        −\n        α\n        )\n        +\n        α\n        ⋅\n        \n          D\n          \n            N\n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle ((1-\\alpha )+\\alpha \\cdot G_{N-1})\\circ u\\circ G_{N},D_{N}\\circ d\\circ ((1-\\alpha )+\\alpha \\cdot D_{N-1})}\n  \n generating and discriminating 8x8 images. Here, the functions \n  \n    \n      \n        u\n        ,\n        d\n      \n    \n    {\\displaystyle u,d}\n  \n are image up- and down-sampling functions, and \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n is a blend-in factor (much like an alpha in image composing) that smoothly glides from 0 to 1.\n\n\n==== StyleGAN-1 ====\n\nStyleGAN-1 is designed as a combination of Progressive GAN with neural style transfer.\nThe key architectural choice of StyleGAN-1 is a progressive growth mechanism, similar to Progressive GAN. Each generated image starts as a constant \n  \n    \n      \n        4\n        ×\n        4\n        ×\n        512\n      \n    \n    {\\displaystyle 4\\times 4\\times 512}\n  \n array, and repeatedly passed through style blocks. Each style block applies a \"style latent vector\" via affine transform (\"adaptive instance normalization\"), similar to how neural style transfer uses Gramian matrix. It then adds noise, and normalize (subtract the mean, then divide by the variance).\nAt training time, usually only one style latent vector is used per image generated, but sometimes two (\"mixing regularization\") in order to encourage each style block to independently perform its stylization without expecting help from other style blocks (since they might receive an entirely different style latent vector).\nAfter training, multiple style latent vectors can be fed into each style block. Those fed to the lower layers control the large-scale styles, and those fed to the higher layers control the fine-detail styles.\nStyle-mixing between two images \n  \n    \n      \n        x\n        ,\n        \n          x\n          ′\n        \n      \n    \n    {\\displaystyle x,x'}\n  \n can be performed as well. First, run a gradient descent to find \n  \n    \n      \n        z\n        ,\n        \n          z\n          ′\n        \n      \n    \n    {\\displaystyle z,z'}\n  \n such that \n  \n    \n      \n        G\n        (\n        z\n        )\n        ≈\n        x\n        ,\n        G\n        (\n        \n          z\n          ′\n        \n        )\n        ≈\n        \n          x\n          ′\n        \n      \n    \n    {\\displaystyle G(z)\\approx x,G(z')\\approx x'}\n  \n. This is called \"projecting an image back to style latent space\". Then, \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n can be fed to the lower style blocks, and \n  \n    \n      \n        \n          z\n          ′\n        \n      \n    \n    {\\displaystyle z'}\n  \n to the higher style blocks, to generate a composite image that has the large-scale style of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, and the fine-detail style of \n  \n    \n      \n        \n          x\n          ′\n        \n      \n    \n    {\\displaystyle x'}\n  \n. Multiple images can also be composed this way.\n\n\n==== StyleGAN-2 ====\nStyleGAN-2 improves upon StyleGAN-1, by using the style latent vector to transform the convolution layer's weights instead, thus solving the \"blob\" problem.\nThis was updated by the StyleGAN-2-ADA (\"ADA\" stands for \"adaptive\"), which uses invertible data augmentation as described above. It also tunes the amount of data augmentation applied by starting at zero, and gradually increasing it until an \"overfitting heuristic\" reaches a target level, thus the name \"adaptive\".\n\n\n==== StyleGAN-3 ====\nStyleGAN-3 improves upon StyleGAN-2 by solving the \"texture sticking\" problem, which can be seen in the official videos. They analyzed the problem by the Nyquist–Shannon sampling theorem, and argued that the layers in the generator learned to exploit the high-frequency signal in the pixels they operate upon.\nTo solve this, they proposed imposing strict lowpass filters between each generator's layers, so that the generator is forced to operate on the pixels in a way faithful to the continuous signals they represent, rather than operate on them as merely discrete signals. They further imposed rotational and translational invariance by using more signal filters. The resulting StyleGAN-3 is able to solve the texture sticking problem, as well as generating images that rotate and translate smoothly.\n\n\n== Other uses ==\nOther than for generative and discriminative modelling of data, GANs have been used for other things.\nGANs have been used for transfer learning to enforce the alignment of the latent feature space, such as in deep reinforcement learning. This works by feeding the embeddings of the source and target task to the discriminator which tries to guess the context. The resulting loss is then (inversely) backpropagated through the encoder.\n\n\n== Applications ==\n\n\n=== Science ===\nIteratively reconstruct astronomical images\nSimulate gravitational lensing for dark matter research.\nModel the distribution of dark matter in a particular direction in space and to predict the gravitational lensing that will occur.\nModel high energy jet formation and showers through calorimeters of high-energy physics experiments.\nApproximate bottlenecks in computationally expensive simulations of particle physics experiments. Applications in the context of present and proposed CERN experiments have demonstrated the potential of these methods for accelerating simulation and/or improving simulation fidelity.\nReconstruct velocity and scalar fields in turbulent flows.\nGAN-generated molecules were validated experimentally in mice.\n\n\n=== Medical ===\nOne of the major concerns in medical imaging is preserving patient privacy. Due to these reasons, researchers often face difficulties in obtaining medical images for their research purposes. GAN has been used for generating synthetic medical images, such as MRI and PET images to address this challenge.\nGAN can be used to detect glaucomatous images helping the early diagnosis which is essential to avoid partial or total loss of vision.\nGANs have been used to create forensic facial reconstructions of deceased historical figures.\n\n\n=== Malicious ===\n\nConcerns have been raised about the potential use of GAN-based human image synthesis for sinister purposes, e.g., to produce fake, possibly incriminating, photographs and videos.\nGANs can be used to generate unique, realistic profile photos of people who do not exist, in order to automate creation of fake social media profiles.\nIn 2019 the state of California considered and passed on October 3, 2019, the bill AB-602, which bans the use of human image synthesis technologies to make fake pornography without the consent of the people depicted, and bill AB-730, which prohibits distribution of manipulated videos of a political candidate within 60 days of an election. Both bills were authored by Assembly member Marc Berman and signed by Governor Gavin Newsom. The laws went into effect in 2020.\nDARPA's Media Forensics program studies ways to counteract fake media, including fake media produced using GANs.\n\n\n=== Fashion, art and advertising ===\nGANs can be used to generate art; The Verge wrote in March 2019 that \"The images created by GANs have become the defining look of contemporary AI art.\" GANs can also be used to\n\ninpaint photographs\ngenerate fashion models, shadows, photorealistic renders of interior design, industrial design, shoes, etc. Such networks were reported to be used by Facebook.\nSome have worked with using GAN for artistic creativity, as \"creative adversarial network\". A GAN, trained on a set of 15,000 portraits from WikiArt from the 14th to the 19th century, created the 2018 painting Edmond de Belamy, which sold for US$432,500.\nGANs were used by the video game modding community to up-scale low-resolution 2D textures in old video games by recreating them in 4k or higher resolutions via image training, and then down-sampling them to fit the game's native resolution (resembling supersampling anti-aliasing).\nIn 2020, Artbreeder was used to create the main antagonist in the sequel to the psychological web horror series Ben Drowned. The author would later go on to praise GAN applications for their ability to help generate assets for independent artists who are short on budget and manpower.\nIn May 2020, Nvidia researchers taught an AI system (termed \"GameGAN\") to recreate the game of Pac-Man simply by watching it being played.\nIn August 2019, a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment was created for neural melody generation from lyrics using conditional GAN-LSTM (refer to sources at GitHub AI Melody Generation from Lyrics).\n\n\n=== Miscellaneous ===\nGANs have been used to \n\nshow how an individual's appearance might change with age.\nreconstruct 3D models of objects from images,\ngenerate novel objects as 3D point clouds,\nmodel patterns of motion in video.\ninpaint missing features in maps, transfer map styles in cartography or augment street view imagery.\nuse feedback to generate images and replace image search systems.\nvisualize the effect that climate change will have on specific houses.\nreconstruct an image of a person's face after listening to their voice.\nproduces videos of a person speaking, given only a single photo of that person.\nrecurrent sequence generation.\n\n\n== History ==\nIn 1991, Juergen Schmidhuber published \"artificial curiosity\", neural networks in a zero-sum game. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. GANs can be regarded as a case where the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set.\nOther people had similar ideas but did not develop them similarly. An idea involving adversarial networks was published in a 2010 blog post by Olli Niemitalo. This idea was never implemented and did not involve stochasticity in the generator and thus was not a generative model. It is now known as a conditional GAN or cGAN. An idea similar to GANs was used to model animal behavior by Wei Li, Melvin Gauci and Roderich Gross in 2013.\nAnother inspiration for GANs was noise-contrastive estimation, which uses the same loss function as GANs and which Goodfellow studied during his PhD in 2010–2014.\nAdversarial machine learning has other uses besides generative modeling and can be applied to models other than neural networks. In control theory, adversarial learning based on neural networks was used in 2006 to train robust controllers in a game theoretic sense, by alternating the iterations between a minimizer policy, the controller, and a maximizer policy, the disturbance.\nIn 2017, a GAN was used for image enhancement focusing on realistic textures rather than pixel-accuracy, producing a higher image quality at high magnification. In 2017, the first faces were generated. These were exhibited in February 2018 at the Grand Palais. Faces generated by StyleGAN in 2019 drew comparisons with Deepfakes.\n\n\n== See also ==\nArtificial intelligence art – Visual media created with AIPages displaying short descriptions of redirect targets\nDeepfake – Realistic artificially generated media\nDeep learning – Branch of machine learning\nDiffusion model – Technique for the generative modeling of a continuous probability distribution\nGenerative artificial intelligence – Subset of AI using generative models\nSynthetic media – Artificial production of media by automated means\n\n\n== References ==\n\n\n== External links ==\n\nKnight, Will. \"5 Big Predictions for Artificial Intelligence in 2017\". MIT Technology Review. Retrieved January 5, 2017.\nKarras, Tero; Laine, Samuli; Aila, Timo (2018). \"A Style-Based Generator Architecture for Generative Adversarial Networks\". arXiv:1812.04948 [cs.NE].\nThis Person Does Not Exist –  photorealistic images of people who do not exist, generated by StyleGAN\nThis Cat Does Not Exist Archived March 5, 2019, at the Wayback Machine –  photorealistic images of cats who do not exist, generated by StyleGAN\nWang, Zhengwei; She, Qi; Ward, Tomas E. (2019). \"Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy\". arXiv:1906.01529 [cs.LG].",
    "categories": [
      "2014 in artificial intelligence",
      "All articles with unsourced statements",
      "Articles with short description",
      "Articles with unsourced statements from August 2025",
      "CS1 maint: multiple names: authors list",
      "CS1 maint: numeric names: authors list",
      "Cognitive science",
      "Generative artificial intelligence",
      "Neural network architectures",
      "Pages displaying short descriptions of redirect targets via Module:Annotated link",
      "Short description is different from Wikidata",
      "Unsupervised learning",
      "Use mdy dates from April 2021",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 2017
  },
  {
    "title": "Variational autoencoder",
    "url": "https://en.wikipedia.org/wiki/Variational_autoencoder",
    "content": "In machine learning, a variational autoencoder (VAE) is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling in 2013. It is part of the families of probabilistic graphical models and variational Bayesian methods.\nIn addition to being seen as an autoencoder neural network architecture, variational autoencoders can also be studied within the mathematical formulation of variational Bayesian methods, connecting a neural encoder network to its decoder through a probabilistic latent space (for example, as a multivariate Gaussian distribution) that corresponds to the parameters of a variational distribution. \nThus, the encoder maps each point (such as an image) from a large complex dataset into a distribution within the latent space, rather than to a single point in that space. The decoder has the opposite function, which is to map from the latent space to the input space, again according to a distribution (although in practice, noise is rarely added during the decoding stage). By mapping a point to a distribution instead of a single point, the network can avoid overfitting the training data. Both networks are typically trained together with the usage of the reparameterization trick, although the variance of the noise model can be learned separately.\nAlthough this type of model was initially designed for unsupervised learning, its effectiveness has been proven for semi-supervised learning and supervised learning.\n\n\n== Overview of architecture and operation ==\nA variational autoencoder is a generative model with a prior and noise distribution respectively. Usually such models are trained using the expectation-maximization meta-algorithm (e.g. probabilistic PCA, (spike & slab) sparse coding). Such a scheme optimizes a lower bound of the data likelihood, which is usually computationally intractable, and in doing so requires the discovery of q-distributions, or variational posteriors. These q-distributions are normally parameterized for each individual data point in a separate optimization process. However, variational autoencoders use a neural network as an amortized approach to jointly optimize across data points. In that way, the same parameters are reused for multiple data points, which can result in massive memory savings. The first neural network takes as input the data points themselves, and outputs parameters for the variational distribution. As it maps from a known input space to the low-dimensional latent space, it is called the encoder.\nThe decoder is the second neural network of this model. It is a function that maps from the latent space to the input space, e.g. as the means of the noise distribution. It is possible to use another neural network that maps to the variance, however this can be omitted for simplicity. In such a case, the variance can be optimized with gradient descent.\nTo optimize this model, one needs to know two terms: the \"reconstruction error\", and the Kullback–Leibler divergence (KL-D). Both terms are derived from the free energy expression of the probabilistic model, and therefore differ depending on the noise distribution and the assumed prior of the data, here referred to as p-distribution. For example, a standard VAE task such as IMAGENET is typically assumed to have a gaussianly distributed noise; however, tasks such as binarized MNIST require a Bernoulli noise. The KL-D from the free energy expression maximizes the probability mass of the q-distribution that overlaps with the p-distribution, which unfortunately can result in mode-seeking behaviour. The \"reconstruction\" term is the remainder of the free energy expression, and requires a sampling approximation to compute its expectation value. \nMore recent approaches replace Kullback–Leibler divergence (KL-D) with various statistical distances, see \"Statistical distance VAE variants\" below.\n\n\n== Formulation ==\nFrom the point of view of probabilistic modeling, one wants to maximize the likelihood of the data \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n by their chosen parameterized probability distribution \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        )\n        =\n        p\n        (\n        x\n        \n          |\n        \n        θ\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x)=p(x|\\theta )}\n  \n. This distribution is usually chosen to be a Gaussian \n  \n    \n      \n        N\n        (\n        x\n        \n          |\n        \n        μ\n        ,\n        σ\n        )\n      \n    \n    {\\displaystyle N(x|\\mu ,\\sigma )}\n  \n which is parameterized by \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n and \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n respectively, and as a member of the exponential family it is easy to work with as a noise distribution. Simple distributions are easy enough to maximize, however distributions where a prior is assumed over the latents \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n results in intractable integrals. Let us find \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x)}\n  \n via marginalizing over \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n.\n\n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        )\n        =\n        \n          ∫\n          \n            z\n          \n        \n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          ,\n          z\n        \n        )\n        \n        d\n        z\n        ,\n      \n    \n    {\\displaystyle p_{\\theta }(x)=\\int _{z}p_{\\theta }({x,z})\\,dz,}\n  \n\nwhere \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          ,\n          z\n        \n        )\n      \n    \n    {\\displaystyle p_{\\theta }({x,z})}\n  \n represents the joint distribution under \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle p_{\\theta }}\n  \n of the observable data \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n and its latent representation or encoding \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n. According to the chain rule, the equation can be rewritten as\n\n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        )\n        =\n        \n          ∫\n          \n            z\n          \n        \n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            |\n          \n          z\n        \n        )\n        \n          p\n          \n            θ\n          \n        \n        (\n        z\n        )\n        \n        d\n        z\n      \n    \n    {\\displaystyle p_{\\theta }(x)=\\int _{z}p_{\\theta }({x|z})p_{\\theta }(z)\\,dz}\n  \n\nIn the vanilla variational autoencoder, \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n is usually taken to be a finite-dimensional vector of real numbers, and \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            |\n          \n          z\n        \n        )\n      \n    \n    {\\displaystyle p_{\\theta }({x|z})}\n  \n to be a Gaussian distribution. Then \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x)}\n  \n is a mixture of Gaussian distributions.\nIt is now possible to define the set of the relationships between the input data and its latent representation as\n\nPrior \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        z\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(z)}\n  \n\nLikelihood \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        \n          |\n        \n        z\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x|z)}\n  \n\nPosterior \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        z\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(z|x)}\n  \n\nUnfortunately, the computation of \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        z\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(z|x)}\n  \n is expensive and in most cases intractable. To speed up the calculus to make it feasible, it is necessary to introduce a further function to approximate the posterior distribution as\n\n  \n    \n      \n        \n          q\n          \n            ϕ\n          \n        \n        (\n        \n          z\n          \n            |\n          \n          x\n        \n        )\n        ≈\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          z\n          \n            |\n          \n          x\n        \n        )\n      \n    \n    {\\displaystyle q_{\\phi }({z|x})\\approx p_{\\theta }({z|x})}\n  \n\nwith \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n defined as the set of real values that parametrize \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n. This is sometimes called amortized inference, since by \"investing\" in finding a good \n  \n    \n      \n        \n          q\n          \n            ϕ\n          \n        \n      \n    \n    {\\displaystyle q_{\\phi }}\n  \n, one can later infer \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n from \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n quickly without doing any integrals.\nIn this way, the problem is to find a good probabilistic autoencoder, in which the conditional likelihood distribution \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        \n          |\n        \n        z\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(x|z)}\n  \n is computed by the probabilistic decoder, and the approximated posterior distribution \n  \n    \n      \n        \n          q\n          \n            ϕ\n          \n        \n        (\n        z\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle q_{\\phi }(z|x)}\n  \n is computed by the probabilistic encoder.\nParametrize the encoder as \n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n      \n    \n    {\\displaystyle E_{\\phi }}\n  \n, and the decoder as \n  \n    \n      \n        \n          D\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle D_{\\theta }}\n  \n.\n\n\n== Evidence lower bound (ELBO) ==\n\nLike many deep learning approaches that use gradient-based optimization, VAEs require a differentiable loss function to update the network weights through backpropagation.\nFor variational autoencoders, the idea is to jointly optimize the generative model parameters \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n to reduce the reconstruction error between the input and the output, and \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n to make \n  \n    \n      \n        \n          q\n          \n            ϕ\n          \n        \n        (\n        \n          z\n          \n            |\n          \n          x\n        \n        )\n      \n    \n    {\\displaystyle q_{\\phi }({z|x})}\n  \n as close as possible to \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        z\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(z|x)}\n  \n. As reconstruction loss, mean squared error and cross entropy are often used.\nThe Kullback–Leibler divergence \n  \n    \n      \n        \n          D\n          \n            K\n            L\n          \n        \n        (\n        \n          q\n          \n            ϕ\n          \n        \n        (\n        \n          z\n          \n            |\n          \n          x\n        \n        )\n        ∥\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          z\n          \n            |\n          \n          x\n        \n        )\n        )\n      \n    \n    {\\displaystyle D_{KL}(q_{\\phi }({z|x})\\parallel p_{\\theta }({z|x}))}\n  \n can be used as a loss function to squeeze \n  \n    \n      \n        \n          q\n          \n            ϕ\n          \n        \n        (\n        \n          z\n          \n            |\n          \n          x\n        \n        )\n      \n    \n    {\\displaystyle q_{\\phi }({z|x})}\n  \n under \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        z\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(z|x)}\n  \n. This divergence loss expands to\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  D\n                  \n                    K\n                    L\n                  \n                \n                (\n                \n                  q\n                  \n                    ϕ\n                  \n                \n                (\n                \n                  z\n                  \n                    |\n                  \n                  x\n                \n                )\n                ∥\n                \n                  p\n                  \n                    θ\n                  \n                \n                (\n                \n                  z\n                  \n                    |\n                  \n                  x\n                \n                )\n                )\n              \n              \n                \n                =\n                \n                  \n                    E\n                  \n                  \n                    z\n                    ∼\n                    \n                      q\n                      \n                        ϕ\n                      \n                    \n                    (\n                    ⋅\n                    \n                      |\n                    \n                    x\n                    )\n                  \n                \n                \n                  [\n                  \n                    ln\n                    ⁡\n                    \n                      \n                        \n                          \n                            q\n                            \n                              ϕ\n                            \n                          \n                          (\n                          z\n                          \n                            |\n                          \n                          x\n                          )\n                        \n                        \n                          \n                            p\n                            \n                              θ\n                            \n                          \n                          (\n                          z\n                          \n                            |\n                          \n                          x\n                          )\n                        \n                      \n                    \n                  \n                  ]\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    E\n                  \n                  \n                    z\n                    ∼\n                    \n                      q\n                      \n                        ϕ\n                      \n                    \n                    (\n                    ⋅\n                    \n                      |\n                    \n                    x\n                    )\n                  \n                \n                \n                  [\n                  \n                    ln\n                    ⁡\n                    \n                      \n                        \n                          \n                            q\n                            \n                              ϕ\n                            \n                          \n                          (\n                          \n                            z\n                            \n                              |\n                            \n                            x\n                          \n                          )\n                          \n                            p\n                            \n                              θ\n                            \n                          \n                          (\n                          x\n                          )\n                        \n                        \n                          \n                            p\n                            \n                              θ\n                            \n                          \n                          (\n                          x\n                          ,\n                          z\n                          )\n                        \n                      \n                    \n                  \n                  ]\n                \n              \n            \n            \n              \n              \n                \n                =\n                ln\n                ⁡\n                \n                  p\n                  \n                    θ\n                  \n                \n                (\n                x\n                )\n                +\n                \n                  \n                    E\n                  \n                  \n                    z\n                    ∼\n                    \n                      q\n                      \n                        ϕ\n                      \n                    \n                    (\n                    ⋅\n                    \n                      |\n                    \n                    x\n                    )\n                  \n                \n                \n                  [\n                  \n                    ln\n                    ⁡\n                    \n                      \n                        \n                          \n                            q\n                            \n                              ϕ\n                            \n                          \n                          (\n                          \n                            z\n                            \n                              |\n                            \n                            x\n                          \n                          )\n                        \n                        \n                          \n                            p\n                            \n                              θ\n                            \n                          \n                          (\n                          x\n                          ,\n                          z\n                          )\n                        \n                      \n                    \n                  \n                  ]\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}D_{KL}(q_{\\phi }({z|x})\\parallel p_{\\theta }({z|x}))&=\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\ln {\\frac {q_{\\phi }(z|x)}{p_{\\theta }(z|x)}}\\right]\\\\&=\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\ln {\\frac {q_{\\phi }({z|x})p_{\\theta }(x)}{p_{\\theta }(x,z)}}\\right]\\\\&=\\ln p_{\\theta }(x)+\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\ln {\\frac {q_{\\phi }({z|x})}{p_{\\theta }(x,z)}}\\right].\\end{aligned}}}\n  \n\nNow, define the evidence lower bound (ELBO):\n  \n    \n      \n        \n          L\n          \n            θ\n            ,\n            ϕ\n          \n        \n        (\n        x\n        )\n        :=\n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              q\n              \n                ϕ\n              \n            \n            (\n            ⋅\n            \n              |\n            \n            x\n            )\n          \n        \n        \n          [\n          \n            ln\n            ⁡\n            \n              \n                \n                  \n                    p\n                    \n                      θ\n                    \n                  \n                  (\n                  x\n                  ,\n                  z\n                  )\n                \n                \n                  \n                    q\n                    \n                      ϕ\n                    \n                  \n                  (\n                  \n                    z\n                    \n                      |\n                    \n                    x\n                  \n                  )\n                \n              \n            \n          \n          ]\n        \n        =\n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        )\n        −\n        \n          D\n          \n            K\n            L\n          \n        \n        (\n        \n          q\n          \n            ϕ\n          \n        \n        (\n        \n          ⋅\n          \n            |\n          \n          x\n        \n        )\n        ∥\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          ⋅\n          \n            |\n          \n          x\n        \n        )\n        )\n      \n    \n    {\\displaystyle L_{\\theta ,\\phi }(x):=\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\ln {\\frac {p_{\\theta }(x,z)}{q_{\\phi }({z|x})}}\\right]=\\ln p_{\\theta }(x)-D_{KL}(q_{\\phi }({\\cdot |x})\\parallel p_{\\theta }({\\cdot |x}))}\n  \nMaximizing the ELBO\n  \n    \n      \n        \n          θ\n          \n            ∗\n          \n        \n        ,\n        \n          ϕ\n          \n            ∗\n          \n        \n        =\n        \n          \n            argmax\n            \n              θ\n              ,\n              ϕ\n            \n          \n        \n        \n        \n          L\n          \n            θ\n            ,\n            ϕ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\theta ^{*},\\phi ^{*}={\\underset {\\theta ,\\phi }{\\operatorname {argmax} }}\\,L_{\\theta ,\\phi }(x)}\n  \nis equivalent to simultaneously maximizing \n  \n    \n      \n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\ln p_{\\theta }(x)}\n  \n and minimizing \n  \n    \n      \n        \n          D\n          \n            K\n            L\n          \n        \n        (\n        \n          q\n          \n            ϕ\n          \n        \n        (\n        \n          z\n          \n            |\n          \n          x\n        \n        )\n        ∥\n        \n          p\n          \n            θ\n          \n        \n        (\n        \n          z\n          \n            |\n          \n          x\n        \n        )\n        )\n      \n    \n    {\\displaystyle D_{KL}(q_{\\phi }({z|x})\\parallel p_{\\theta }({z|x}))}\n  \n. That is, maximizing the log-likelihood of the observed data, and minimizing the divergence from the approximate posterior \n  \n    \n      \n        \n          q\n          \n            ϕ\n          \n        \n        (\n        ⋅\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle q_{\\phi }(\\cdot |x)}\n  \n to the exact posterior \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        ⋅\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(\\cdot |x)}\n  \n.\nThe form given is not very convenient for maximization, but the following, equivalent form, is:\n  \n    \n      \n        \n          L\n          \n            θ\n            ,\n            ϕ\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              q\n              \n                ϕ\n              \n            \n            (\n            ⋅\n            \n              |\n            \n            x\n            )\n          \n        \n        \n          [\n          \n            ln\n            ⁡\n            \n              p\n              \n                θ\n              \n            \n            (\n            x\n            \n              |\n            \n            z\n            )\n          \n          ]\n        \n        −\n        \n          D\n          \n            K\n            L\n          \n        \n        (\n        \n          q\n          \n            ϕ\n          \n        \n        (\n        \n          ⋅\n          \n            |\n          \n          x\n        \n        )\n        ∥\n        \n          p\n          \n            θ\n          \n        \n        (\n        ⋅\n        )\n        )\n      \n    \n    {\\displaystyle L_{\\theta ,\\phi }(x)=\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\ln p_{\\theta }(x|z)\\right]-D_{KL}(q_{\\phi }({\\cdot |x})\\parallel p_{\\theta }(\\cdot ))}\n  \nwhere \n  \n    \n      \n        ln\n        ⁡\n        \n          p\n          \n            θ\n          \n        \n        (\n        x\n        \n          |\n        \n        z\n        )\n      \n    \n    {\\displaystyle \\ln p_{\\theta }(x|z)}\n  \n is implemented as \n  \n    \n      \n        −\n        \n          \n            1\n            2\n          \n        \n        ‖\n        x\n        −\n        \n          D\n          \n            θ\n          \n        \n        (\n        z\n        )\n        \n          ‖\n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle -{\\frac {1}{2}}\\|x-D_{\\theta }(z)\\|_{2}^{2}}\n  \n, since that is, up to an additive constant, what \n  \n    \n      \n        x\n        \n          |\n        \n        z\n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          D\n          \n            θ\n          \n        \n        (\n        z\n        )\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle x|z\\sim {\\mathcal {N}}(D_{\\theta }(z),I)}\n  \n yields. That is, we model the distribution of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n conditional on \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n to be a Gaussian distribution centered on \n  \n    \n      \n        \n          D\n          \n            θ\n          \n        \n        (\n        z\n        )\n      \n    \n    {\\displaystyle D_{\\theta }(z)}\n  \n. The distribution of \n  \n    \n      \n        \n          q\n          \n            ϕ\n          \n        \n        (\n        z\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle q_{\\phi }(z|x)}\n  \n and \n  \n    \n      \n        \n          p\n          \n            θ\n          \n        \n        (\n        z\n        )\n      \n    \n    {\\displaystyle p_{\\theta }(z)}\n  \n are often also chosen to be Gaussians as \n  \n    \n      \n        z\n        \n          |\n        \n        x\n        ∼\n        \n          \n            N\n          \n        \n        (\n        \n          E\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        ,\n        \n          σ\n          \n            ϕ\n          \n        \n        (\n        x\n        \n          )\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle z|x\\sim {\\mathcal {N}}(E_{\\phi }(x),\\sigma _{\\phi }(x)^{2}I)}\n  \n and \n  \n    \n      \n        z\n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        I\n        )\n      \n    \n    {\\displaystyle z\\sim {\\mathcal {N}}(0,I)}\n  \n, with which we obtain by the formula for KL divergence of Gaussians:\n  \n    \n      \n        \n          L\n          \n            θ\n            ,\n            ϕ\n          \n        \n        (\n        x\n        )\n        =\n        −\n        \n          \n            1\n            2\n          \n        \n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              q\n              \n                ϕ\n              \n            \n            (\n            ⋅\n            \n              |\n            \n            x\n            )\n          \n        \n        \n          [\n          \n            ‖\n            x\n            −\n            \n              D\n              \n                θ\n              \n            \n            (\n            z\n            )\n            \n              ‖\n              \n                2\n              \n              \n                2\n              \n            \n          \n          ]\n        \n        −\n        \n          \n            1\n            2\n          \n        \n        \n          (\n          \n            N\n            \n              σ\n              \n                ϕ\n              \n            \n            (\n            x\n            \n              )\n              \n                2\n              \n            \n            +\n            ‖\n            \n              E\n              \n                ϕ\n              \n            \n            (\n            x\n            )\n            \n              ‖\n              \n                2\n              \n              \n                2\n              \n            \n            −\n            2\n            N\n            ln\n            ⁡\n            \n              σ\n              \n                ϕ\n              \n            \n            (\n            x\n            )\n          \n          )\n        \n        +\n        C\n        o\n        n\n        s\n        t\n      \n    \n    {\\displaystyle L_{\\theta ,\\phi }(x)=-{\\frac {1}{2}}\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\|x-D_{\\theta }(z)\\|_{2}^{2}\\right]-{\\frac {1}{2}}\\left(N\\sigma _{\\phi }(x)^{2}+\\|E_{\\phi }(x)\\|_{2}^{2}-2N\\ln \\sigma _{\\phi }(x)\\right)+Const}\n  \nHere \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the dimension of \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n. For a more detailed derivation and more interpretations of ELBO and its maximization, see its main page.\n\n\n== Reparameterization ==\n\nTo efficiently search for \n  \n    \n      \n        \n          θ\n          \n            ∗\n          \n        \n        ,\n        \n          ϕ\n          \n            ∗\n          \n        \n        =\n        \n          \n            argmax\n            \n              θ\n              ,\n              ϕ\n            \n          \n        \n        \n        \n          L\n          \n            θ\n            ,\n            ϕ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\theta ^{*},\\phi ^{*}={\\underset {\\theta ,\\phi }{\\operatorname {argmax} }}\\,L_{\\theta ,\\phi }(x)}\n  \nthe typical method is gradient ascent.\nIt is straightforward to find\n  \n    \n      \n        \n          ∇\n          \n            θ\n          \n        \n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              q\n              \n                ϕ\n              \n            \n            (\n            ⋅\n            \n              |\n            \n            x\n            )\n          \n        \n        \n          [\n          \n            ln\n            ⁡\n            \n              \n                \n                  \n                    p\n                    \n                      θ\n                    \n                  \n                  (\n                  x\n                  ,\n                  z\n                  )\n                \n                \n                  \n                    q\n                    \n                      ϕ\n                    \n                  \n                  (\n                  \n                    z\n                    \n                      |\n                    \n                    x\n                  \n                  )\n                \n              \n            \n          \n          ]\n        \n        =\n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              q\n              \n                ϕ\n              \n            \n            (\n            ⋅\n            \n              |\n            \n            x\n            )\n          \n        \n        \n          [\n          \n            \n              ∇\n              \n                θ\n              \n            \n            ln\n            ⁡\n            \n              \n                \n                  \n                    p\n                    \n                      θ\n                    \n                  \n                  (\n                  x\n                  ,\n                  z\n                  )\n                \n                \n                  \n                    q\n                    \n                      ϕ\n                    \n                  \n                  (\n                  \n                    z\n                    \n                      |\n                    \n                    x\n                  \n                  )\n                \n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\nabla _{\\theta }\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\ln {\\frac {p_{\\theta }(x,z)}{q_{\\phi }({z|x})}}\\right]=\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\nabla _{\\theta }\\ln {\\frac {p_{\\theta }(x,z)}{q_{\\phi }({z|x})}}\\right]}\n  \nHowever, \n  \n    \n      \n        \n          ∇\n          \n            ϕ\n          \n        \n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              q\n              \n                ϕ\n              \n            \n            (\n            ⋅\n            \n              |\n            \n            x\n            )\n          \n        \n        \n          [\n          \n            ln\n            ⁡\n            \n              \n                \n                  \n                    p\n                    \n                      θ\n                    \n                  \n                  (\n                  x\n                  ,\n                  z\n                  )\n                \n                \n                  \n                    q\n                    \n                      ϕ\n                    \n                  \n                  (\n                  \n                    z\n                    \n                      |\n                    \n                    x\n                  \n                  )\n                \n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\nabla _{\\phi }\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\ln {\\frac {p_{\\theta }(x,z)}{q_{\\phi }({z|x})}}\\right]}\n  \ndoes not allow one to put the \n  \n    \n      \n        \n          ∇\n          \n            ϕ\n          \n        \n      \n    \n    {\\displaystyle \\nabla _{\\phi }}\n  \n inside the expectation, since \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n appears in the probability distribution itself. The reparameterization trick (also known as stochastic backpropagation) bypasses this difficulty.\nThe most important example is when \n  \n    \n      \n        z\n        ∼\n        \n          q\n          \n            ϕ\n          \n        \n        (\n        ⋅\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle z\\sim q_{\\phi }(\\cdot |x)}\n  \n is normally distributed, as \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        \n          μ\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        ,\n        \n          Σ\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(\\mu _{\\phi }(x),\\Sigma _{\\phi }(x))}\n  \n.\n\nThis can be reparametrized by letting \n  \n    \n      \n        \n          ε\n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        \n          I\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\varepsilon }}\\sim {\\mathcal {N}}(0,{\\boldsymbol {I}})}\n  \n be a \"standard random number generator\", and construct \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n as \n  \n    \n      \n        z\n        =\n        \n          μ\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        +\n        \n          L\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        ϵ\n      \n    \n    {\\displaystyle z=\\mu _{\\phi }(x)+L_{\\phi }(x)\\epsilon }\n  \n. Here, \n  \n    \n      \n        \n          L\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle L_{\\phi }(x)}\n  \n is obtained by the Cholesky decomposition:\n  \n    \n      \n        \n          Σ\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        =\n        \n          L\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        \n          L\n          \n            ϕ\n          \n        \n        (\n        x\n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{\\phi }(x)=L_{\\phi }(x)L_{\\phi }(x)^{T}}\n  \nThen we have\n  \n    \n      \n        \n          ∇\n          \n            ϕ\n          \n        \n        \n          \n            E\n          \n          \n            z\n            ∼\n            \n              q\n              \n                ϕ\n              \n            \n            (\n            ⋅\n            \n              |\n            \n            x\n            )\n          \n        \n        \n          [\n          \n            ln\n            ⁡\n            \n              \n                \n                  \n                    p\n                    \n                      θ\n                    \n                  \n                  (\n                  x\n                  ,\n                  z\n                  )\n                \n                \n                  \n                    q\n                    \n                      ϕ\n                    \n                  \n                  (\n                  \n                    z\n                    \n                      |\n                    \n                    x\n                  \n                  )\n                \n              \n            \n          \n          ]\n        \n        =\n        \n          \n            E\n          \n          \n            ϵ\n          \n        \n        \n          [\n          \n            \n              ∇\n              \n                ϕ\n              \n            \n            ln\n            ⁡\n            \n              \n                \n                  \n                    p\n                    \n                      θ\n                    \n                  \n                  (\n                  x\n                  ,\n                  \n                    μ\n                    \n                      ϕ\n                    \n                  \n                  (\n                  x\n                  )\n                  +\n                  \n                    L\n                    \n                      ϕ\n                    \n                  \n                  (\n                  x\n                  )\n                  ϵ\n                  )\n                \n                \n                  \n                    q\n                    \n                      ϕ\n                    \n                  \n                  (\n                  \n                    μ\n                    \n                      ϕ\n                    \n                  \n                  (\n                  x\n                  )\n                  +\n                  \n                    L\n                    \n                      ϕ\n                    \n                  \n                  (\n                  x\n                  )\n                  ϵ\n                  \n                    |\n                  \n                  x\n                  )\n                \n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\nabla _{\\phi }\\mathbb {E} _{z\\sim q_{\\phi }(\\cdot |x)}\\left[\\ln {\\frac {p_{\\theta }(x,z)}{q_{\\phi }({z|x})}}\\right]=\\mathbb {E} _{\\epsilon }\\left[\\nabla _{\\phi }\\ln {\\frac {p_{\\theta }(x,\\mu _{\\phi }(x)+L_{\\phi }(x)\\epsilon )}{q_{\\phi }(\\mu _{\\phi }(x)+L_{\\phi }(x)\\epsilon |x)}}\\right]}\n  \nand so we obtained an unbiased estimator of the gradient, allowing stochastic gradient descent.\nSince we reparametrized \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n, we need to find \n  \n    \n      \n        \n          q\n          \n            ϕ\n          \n        \n        (\n        z\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle q_{\\phi }(z|x)}\n  \n. Let \n  \n    \n      \n        \n          q\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle q_{0}}\n  \n be the probability density function for \n  \n    \n      \n        ϵ\n      \n    \n    {\\displaystyle \\epsilon }\n  \n, then \n  \n    \n      \n        ln\n        ⁡\n        \n          q\n          \n            ϕ\n          \n        \n        (\n        z\n        \n          |\n        \n        x\n        )\n        =\n        ln\n        ⁡\n        \n          q\n          \n            0\n          \n        \n        (\n        ϵ\n        )\n        −\n        ln\n        ⁡\n        \n          |\n        \n        det\n        (\n        \n          ∂\n          \n            ϵ\n          \n        \n        z\n        )\n        \n          |\n        \n      \n    \n    {\\displaystyle \\ln q_{\\phi }(z|x)=\\ln q_{0}(\\epsilon )-\\ln |\\det(\\partial _{\\epsilon }z)|}\n  \nwhere \n  \n    \n      \n        \n          ∂\n          \n            ϵ\n          \n        \n        z\n      \n    \n    {\\displaystyle \\partial _{\\epsilon }z}\n  \n is the Jacobian matrix of \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n with respect to \n  \n    \n      \n        ϵ\n      \n    \n    {\\displaystyle \\epsilon }\n  \n. Since \n  \n    \n      \n        z\n        =\n        \n          μ\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        +\n        \n          L\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        ϵ\n      \n    \n    {\\displaystyle z=\\mu _{\\phi }(x)+L_{\\phi }(x)\\epsilon }\n  \n, this is \n  \n    \n      \n        ln\n        ⁡\n        \n          q\n          \n            ϕ\n          \n        \n        (\n        z\n        \n          |\n        \n        x\n        )\n        =\n        −\n        \n          \n            1\n            2\n          \n        \n        ‖\n        ϵ\n        \n          ‖\n          \n            2\n          \n        \n        −\n        ln\n        ⁡\n        \n          |\n        \n        det\n        \n          L\n          \n            ϕ\n          \n        \n        (\n        x\n        )\n        \n          |\n        \n        −\n        \n          \n            n\n            2\n          \n        \n        ln\n        ⁡\n        (\n        2\n        π\n        )\n      \n    \n    {\\displaystyle \\ln q_{\\phi }(z|x)=-{\\frac {1}{2}}\\|\\epsilon \\|^{2}-\\ln |\\det L_{\\phi }(x)|-{\\frac {n}{2}}\\ln(2\\pi )}\n  \n\n\n== Variations ==\nMany variational autoencoders applications and extensions have been used to adapt the architecture to other domains and improve its performance. \n\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n-VAE is an implementation with a weighted Kullback–Leibler divergence term to automatically discover and interpret factorised latent representations. With this implementation, it is possible to force manifold disentanglement for \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n values greater than one. This architecture can discover disentangled latent factors without supervision.\nThe conditional VAE (CVAE), inserts label information in the latent space to force a deterministic constrained representation of the learned data.\nSome structures directly deal with the quality of the generated samples or implement more than one latent space to further improve the representation learning.\nSome architectures mix VAE and generative adversarial networks to obtain hybrid models.\nIt is not necessary to use gradients to update the encoder. In fact, the encoder is not necessary for the generative model. \n\n\n== Statistical distance VAE variants ==\nAfter the initial work of Diederik P. Kingma and Max Welling, several procedures were \nproposed to formulate in a more abstract way the operation of the VAE. In these approaches the loss function is composed of two parts : \n\nthe usual reconstruction error part which seeks to ensure that the encoder-then-decoder mapping \n  \n    \n      \n        x\n        ↦\n        \n          D\n          \n            θ\n          \n        \n        (\n        \n          E\n          \n            ψ\n          \n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle x\\mapsto D_{\\theta }(E_{\\psi }(x))}\n  \n is as close to the identity map as possible; the sampling is done at run time from the empirical distribution  \n  \n    \n      \n        \n          \n            P\n          \n          \n            r\n            e\n            a\n            l\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {P} ^{real}}\n  \n of objects available (e.g., for MNIST or IMAGENET this will be the empirical probability law of all images in the dataset). This gives the term:  \n  \n    \n      \n        \n          \n            E\n          \n          \n            x\n            ∼\n            \n              \n                P\n              \n              \n                r\n                e\n                a\n                l\n              \n            \n          \n        \n        \n          [\n          \n            ‖\n            x\n            −\n            \n              D\n              \n                θ\n              \n            \n            (\n            \n              E\n              \n                ϕ\n              \n            \n            (\n            x\n            )\n            )\n            \n              ‖\n              \n                2\n              \n              \n                2\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\mathbb {E} _{x\\sim \\mathbb {P} ^{real}}\\left[\\|x-D_{\\theta }(E_{\\phi }(x))\\|_{2}^{2}\\right]}\n  \n.\na variational part that ensures that, when the empirical distribution \n  \n    \n      \n        \n          \n            P\n          \n          \n            r\n            e\n            a\n            l\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {P} ^{real}}\n  \n is passed through the encoder \n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n      \n    \n    {\\displaystyle E_{\\phi }}\n  \n, we recover the target distribution, denoted here \n  \n    \n      \n        μ\n        (\n        d\n        z\n        )\n      \n    \n    {\\displaystyle \\mu (dz)}\n  \n that is usually taken to be a Multivariate normal distribution. We will denote \n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n        ♯\n        \n          \n            P\n          \n          \n            r\n            e\n            a\n            l\n          \n        \n      \n    \n    {\\displaystyle E_{\\phi }\\sharp \\mathbb {P} ^{real}}\n  \n this pushforward measure which in practice is just the empirical distribution obtained by passing all dataset objects through the encoder \n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n      \n    \n    {\\displaystyle E_{\\phi }}\n  \n. In order to make sure that  \n  \n    \n      \n        \n          E\n          \n            ϕ\n          \n        \n        ♯\n        \n          \n            P\n          \n          \n            r\n            e\n            a\n            l\n          \n        \n      \n    \n    {\\displaystyle E_{\\phi }\\sharp \\mathbb {P} ^{real}}\n  \n  is close to the target \n  \n    \n      \n        μ\n        (\n        d\n        z\n        )\n      \n    \n    {\\displaystyle \\mu (dz)}\n  \n, a Statistical distance \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n is invoked and the term  \n  \n    \n      \n        d\n        \n          \n            (\n            \n              μ\n              (\n              d\n              z\n              )\n              ,\n              \n                E\n                \n                  ϕ\n                \n              \n              ♯\n              \n                \n                  P\n                \n                \n                  r\n                  e\n                  a\n                  l\n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle d\\left(\\mu (dz),E_{\\phi }\\sharp \\mathbb {P} ^{real}\\right)^{2}}\n  \n is added to the loss.\nWe obtain the final formula for the loss:\n\n  \n    \n      \n        \n          L\n          \n            θ\n            ,\n            ϕ\n          \n        \n        =\n        \n          \n            E\n          \n          \n            x\n            ∼\n            \n              \n                P\n              \n              \n                r\n                e\n                a\n                l\n              \n            \n          \n        \n        \n          [\n          \n            ‖\n            x\n            −\n            \n              D\n              \n                θ\n              \n            \n            (\n            \n              E\n              \n                ϕ\n              \n            \n            (\n            x\n            )\n            )\n            \n              ‖\n              \n                2\n              \n              \n                2\n              \n            \n          \n          ]\n        \n        +\n        d\n        \n          \n            (\n            \n              μ\n              (\n              d\n              z\n              )\n              ,\n              \n                E\n                \n                  ϕ\n                \n              \n              ♯\n              \n                \n                  P\n                \n                \n                  r\n                  e\n                  a\n                  l\n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle L_{\\theta ,\\phi }=\\mathbb {E} _{x\\sim \\mathbb {P} ^{real}}\\left[\\|x-D_{\\theta }(E_{\\phi }(x))\\|_{2}^{2}\\right]+d\\left(\\mu (dz),E_{\\phi }\\sharp \\mathbb {P} ^{real}\\right)^{2}}\n  \n\nThe statistical distance \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n requires special properties, for instance it has to be posses a formula as expectation because the loss function will need to be optimized by stochastic optimization algorithms. Several distances can be chosen and this gave rise to several flavors of VAEs:\n\nthe sliced Wasserstein distance used by S Kolouri, et al. in their VAE\nthe energy distance implemented in the Radon Sobolev Variational Auto-Encoder\nthe Maximum Mean Discrepancy distance used in the MMD-VAE\nthe Wasserstein distance used in the WAEs\nkernel-based distances used in the Kernelized Variational Autoencoder (K-VAE)\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nKingma, Diederik P.; Welling, Max (2019). \"An Introduction to Variational Autoencoders\". Foundations and Trends in Machine Learning. 12 (4). Now Publishers: 307–392. arXiv:1906.02691. doi:10.1561/2200000056. ISSN 1935-8237.",
    "categories": [
      "2013 in artificial intelligence",
      "All articles with unsourced statements",
      "Articles with short description",
      "Articles with unsourced statements from June 2024",
      "Bayesian statistics",
      "Dimension reduction",
      "Graphical models",
      "Neural network architectures",
      "Short description matches Wikidata",
      "Supervised learning",
      "Unsupervised learning",
      "Use dmy dates from June 2021",
      "Wikipedia articles needing clarification from October 2023"
    ],
    "year_mentioned": 1935
  },
  {
    "title": "Prompt engineering",
    "url": "https://en.wikipedia.org/wiki/Prompt_engineering",
    "content": "Prompt engineering is the process of structuring or crafting an instruction in order to produce better outputs from a generative artificial intelligence (AI) model. It typically involves designing clear queries, adding relevant context, and refining wording to guide the model toward more accurate, useful, and consistent responses.\nA prompt is natural language text describing the task that an AI should perform. A prompt for a text-to-text language model can be a query, a command, or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, choice of words and grammar, providing relevant context, or describing a character for the AI to mimic.\nWhen communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, or emphasizing words to achieve a desired subject, style, layout, lighting, and aesthetic.\n\n\n== History ==\nIn 2018, researchers first proposed that all previously separate tasks in natural language processing (NLP) could be cast as a question-answering problem over a context. In addition, they trained a first single, joint, multi-task model that would answer any task-related question like \"What is the sentiment\" or \"Translate this sentence to German\" or \"Who is the president?\"\nThe AI boom saw an increase in the amount of \"prompting technique\" to get the model to output the desired outcome and avoid nonsensical output, a process characterized by trial-and-error. After the release of ChatGPT in 2022, prompt engineering was soon seen as an important business skill, albeit one with an uncertain economic future.\nA repository for prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022. In 2022, the chain-of-thought prompting technique was proposed by Google researchers. In 2023, several text-to-text and text-to-image prompt databases were made publicly available. The Personalized Image-Prompt (PIP) dataset, a generated image-text dataset that has been categorized by 3,115 users, has also been made available publicly in 2024.\n\n\n== Text-to-text ==\nA comprehensive 2024 survey of the field identified over 50 distinct text-based prompting techniques and around 40 multimodal variants, demonstrating rapid diversification in prompting strategies. The study also documented a controlled vocabulary of 33 terms used across prompting research, highlighting the growing need for standardization.\nThe survey found that the performance of large language models is highly sensitive to choices such as the ordering of examples, the quality of demonstration labels, and even small variations in phrasing. In some cases, reordering examples in a prompt produced accuracy shifts of more than 40 percent, emphasizing the importance of methodical prompt construction.\n\n\n=== Chain-of-thought ===\n\nAccording to Google Research, chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps before giving a final answer. In 2022, Google Brain reported that chain-of-thought prompting improves reasoning ability by inducing the model to answer a multi-step problem with steps of reasoning that mimic a train of thought. Chain-of-thought techniques were developed to help LLMs handle multi-step reasoning tasks, such as arithmetic or commonsense reasoning questions.\nFor example, given the question \"Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\", CoT prompting induced an LLM to answer \"A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.\" When applied to PaLM, a 540 billion parameter language model, according to Google, CoT prompting significantly aided the model, allowing it to perform comparably with task-specific fine-tuned models on several tasks, achieving state-of-the-art results at the time on the GSM8K mathematical reasoning benchmark. It is possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability.\nAs originally proposed by Google, each CoT prompt is accompanied by a set of input/output examples—called exemplars—to demonstrate the desired model output, making it a few-shot prompting technique. However, according to a later paper from researchers at Google and the University of Tokyo, simply appending the words \"Let's think step-by-step\" was also effective, which allowed for CoT to be employed as a zero-shot technique.\nAn example format of few-shot CoT prompting with in-context exemplars:\n\n   Q: {example question 1}\n   A: {example answer 1}\n   ...\n   Q: {example question n}\n   A: {example answer n}\n   \n   Q: {question}\n   A: {LLM output}\n\nAn example format of zero-shot CoT prompting:\n\n   Q: {question}. Let's think step by step.\n   A: {LLM output}\n\n\n=== In-context learning ===\nIn-context learning, refers to a model's ability to temporarily learn from prompts. For example, a prompt may include a few examples for a model to learn from, such as asking the model to complete \"maison → house, chat → cat, chien →\" (the expected response being dog), an approach called few-shot learning.\nIn-context learning is an emergent ability of large language models. It is an emergent property of model scale, meaning that breaks in downstream scaling laws occur, leading to its efficacy increasing at a different rate in larger models than in smaller models. Unlike training and fine-tuning, which produce lasting changes, in-context learning is temporary. Training models to perform in-context learning can be viewed as a form of meta-learning, or \"learning to learn\".\n\n\n=== Self-consistency ===\nSelf-consistency performs several chain-of-thought rollouts, then selects the most commonly reached conclusion out of all the rollouts.\n\n\n=== Tree-of-thought ===\nTree-of-thought prompting generalizes chain-of-thought by generating multiple lines of reasoning in parallel, with the ability to backtrack or explore other paths. It can use tree search algorithms like breadth-first, depth-first, or beam.\n\n\n=== Prompting to estimate model sensitivity ===\nResearch consistently demonstrates that LLMs are highly sensitive to subtle variations in prompt formatting, structure, and linguistic properties. Some studies have shown up to 76 accuracy points across formatting changes in few-shot settings. Linguistic features significantly influence prompt effectiveness—such as morphology, syntax, and lexico-semantic changes—which meaningfully enhance task performance across a variety of tasks. Clausal syntax, for example, improves consistency and reduces uncertainty in knowledge retrieval. This sensitivity persists even with larger model sizes, additional few-shot examples, or instruction tuning.\nTo address sensitivity of models and make them more robust, several methods have been proposed. FormatSpread facilitates systematic analysis by evaluating a range of plausible prompt formats, offering a more comprehensive performance interval. Similarly, PromptEval estimates performance distributions across diverse prompts, enabling robust metrics such as performance quantiles and accurate evaluations under constrained budgets.\n\n\n=== Automatic prompt generation ===\nRecent research has explored automated prompt engineering, using optimization algorithms to generate or refine prompts without human intervention. These automated approaches aim to identify effective prompt patterns by analyzing model gradients, reinforcement feedback, or evolutionary processes, reducing the need for manual experimentation.\n\n\n==== Retrieval-augmented generation ====\n\nRetrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with an LLM so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information.\nRAG improves large language models by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases. By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining.\n\n\n==== Graph retrieval-augmented generation ====\n\nGraphRAG (coined by Microsoft Research) is a technique that extends RAG with the use of a knowledge graph (usually, LLM-generated) to allow the model to connect disparate pieces of information, synthesize insights, and holistically understand summarized semantic concepts over large data collections. It was shown to be effective on datasets like the Violent Incident Information from News Articles (VIINA).\nEarlier work showed the effectiveness of using a knowledge graph for question answering using text-to-query generation. These techniques can be combined to search across both unstructured and structured data, providing expanded context, and improved ranking.\n\n\n==== Using language models to generate prompts ====\nLLMs themselves can be used to compose prompts for LLMs. The automatic prompt engineer algorithm uses one LLM to beam search over prompts for another LLM:\n\nThere are two LLMs. One is the target LLM, and another is the prompting LLM.\nPrompting LLM is presented with example input-output pairs, and asked to generate instructions that could have caused a model following the instructions to generate the outputs, given the inputs.\nEach of the generated instructions is used to prompt the target LLM, followed by each of the inputs. The log-probabilities of the outputs are computed and added. This is the score of the instruction.\nThe highest-scored instructions are given to the prompting LLM for further variations.\nRepeat until some stopping criteria is reached, then output the highest-scored instructions.\nCoT examples can be generated by LLM themselves. In \"auto-CoT\", a library of questions are converted to vectors by a model such as BERT. The question vectors are clustered. Questions close to the centroid of each cluster are selected, in order to have a subset of diverse questions. An LLM does zero-shot CoT on each selected question. The question and the corresponding CoT answer are added to a dataset of demonstrations. These diverse demonstrations can then added to prompts for few-shot learning.\n\n\n==== Automatic prompt optimization ====\nAutomatic prompt optimization techniques refine prompts for large language models (LLMs) by automatically searching over alternative prompt strings using evaluation datasets and task-specific metrics. MIPRO (Multi-prompt Instruction Proposal Optimizer) optimizes the instructions and few-shot demonstrations of multi-stage language model programs, proposing small changes to module prompts and retaining those that improve a downstream performance metric without access to module-level labels or gradients. GEPA (Genetic-Pareto) is a reflective prompt optimizer for compound AI systems that combines language-model-based analysis of execution traces and textual feedback with a Pareto-based evolutionary search over a population of candidate systems; across four tasks, GEPA reports average gains of about 10% over reinforcement-learning-based Group Relative Policy Optimization and over 10% over the MIPROv2 prompt optimizer, while using up to 35 times fewer rollouts than GRPO. Open-source frameworks such as DSPy and Opik expose these and related optimizers, allowing prompt search to be expressed as part of a programmatic pipeline rather than through manual trial and error.\n\n\n=== Context engineering ===\nContext engineering is an emerging, practitioner-focused term describing the discipline of designing, curating and governing the elements that accompany user prompts. This includes system instructions, retrieved knowledge, tool definitions, conversation summaries, and task metadata to improve reliability, provenance and token efficiency in production LLM systems.\nThe concept emphasises operational practices such as token budgeting, provenance tags, versioning of context artifacts, observability (logging which context was supplied), and context regression tests to ensure that changes to supplied context do not silently alter system behaviour. A July 2025 survey provides a formal taxonomy of context engineering components (context retrieval/generation, context processing, and context management) and argues for treating the context window as a managed engineering surface rather than only a passive source of retrieved documents.\n\n\n== Text-to-image ==\n\nIn 2022, text-to-image models like DALL-E 2, Stable Diffusion, and Midjourney were released to the public. These models take text prompts as input and use them to generate images.\n\n\n=== Prompt formats ===\nEarly text-to-image models typically do not understand negation, grammar and sentence structure in the same way as large language models, and may thus require a different set of prompting techniques. The prompt \"a party with no cake\" may produce an image including a cake. As an alternative, negative prompts allow a user to indicate, in a separate prompt, which terms should not appear in the resulting image. Techniques such as framing the normal prompt into a sequence-to-sequence language modeling problem can be used to automatically generate an output for the negative prompt.\nA text-to-image prompt commonly includes a description of the subject of the art, the desired medium (such as digital painting or photography), style (such as hyperrealistic or pop-art), lighting (such as rim lighting or crepuscular rays), color, and texture. Word order also affects the output of a text-to-image prompt. Words closer to the start of a prompt may be emphasized more heavily.\nThe Midjourney documentation encourages short, descriptive prompts: instead of \"Show me a picture of lots of blooming California poppies, make them bright, vibrant orange, and draw them in an illustrated style with colored pencils\", an effective prompt might be \"Bright orange California poppies drawn with colored pencils\".\n\n\n=== Artist styles ===\nSome text-to-image models are capable of imitating the style of particular artists by name. For example, the phrase in the style of Greg Rutkowski has been used in Stable Diffusion and Midjourney prompts to generate images in the distinctive style of Polish digital artist Greg Rutkowski. Famous artists such as Vincent van Gogh and Salvador Dalí have also been used for styling and testing.\n\n\n== Non-text prompts ==\nSome approaches augment or replace natural language text prompts with non-text input.\n\n\n=== Textual inversion and embeddings ===\nFor text-to-image models, textual inversion performs an optimization process to create a new word embedding based on a set of example images. This embedding vector acts as a \"pseudo-word\" which can be included in a prompt to express the content or style of the examples.\n\n\n=== Image prompting ===\nIn 2023, Meta's AI research released Segment Anything, a computer vision model that can perform image segmentation by prompting. As an alternative to text prompts, Segment Anything can accept bounding boxes, segmentation masks, and foreground/background points.\n\n\n=== Using gradient descent to search for prompts ===\nIn \"prefix-tuning\", \"prompt tuning\", or \"soft prompting\", floating-point-valued vectors are searched directly by gradient descent to maximize the log-likelihood on outputs.\nFormally, let \n  \n    \n      \n        \n          E\n        \n        =\n        {\n        \n          \n            e\n            \n              1\n            \n          \n        \n        ,\n        …\n        ,\n        \n          \n            e\n            \n              k\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle \\mathbf {E} =\\{\\mathbf {e_{1}} ,\\dots ,\\mathbf {e_{k}} \\}}\n  \n be a set of soft prompt tokens (tunable embeddings), while \n  \n    \n      \n        \n          X\n        \n        =\n        {\n        \n          \n            x\n            \n              1\n            \n          \n        \n        ,\n        …\n        ,\n        \n          \n            x\n            \n              m\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle \\mathbf {X} =\\{\\mathbf {x_{1}} ,\\dots ,\\mathbf {x_{m}} \\}}\n  \n and \n  \n    \n      \n        \n          Y\n        \n        =\n        {\n        \n          \n            y\n            \n              1\n            \n          \n        \n        ,\n        …\n        ,\n        \n          \n            y\n            \n              n\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle \\mathbf {Y} =\\{\\mathbf {y_{1}} ,\\dots ,\\mathbf {y_{n}} \\}}\n  \n be the token embeddings of the input and output respectively. During training, the tunable embeddings, input, and output tokens are concatenated into a single sequence \n  \n    \n      \n        \n          concat\n        \n        (\n        \n          E\n        \n        ;\n        \n          X\n        \n        ;\n        \n          Y\n        \n        )\n      \n    \n    {\\displaystyle {\\text{concat}}(\\mathbf {E} ;\\mathbf {X} ;\\mathbf {Y} )}\n  \n, and fed to the LLMs. The losses are computed over the \n  \n    \n      \n        \n          Y\n        \n      \n    \n    {\\displaystyle \\mathbf {Y} }\n  \n tokens; the gradients are backpropagated to prompt-specific parameters: in prefix-tuning, they are parameters associated with the prompt tokens at each layer; in prompt tuning, they are merely the soft tokens added to the vocabulary.\nMore formally, this is prompt tuning. Let an LLM be written as \n  \n    \n      \n        L\n        L\n        M\n        (\n        X\n        )\n        =\n        F\n        (\n        E\n        (\n        X\n        )\n        )\n      \n    \n    {\\displaystyle LLM(X)=F(E(X))}\n  \n, where \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n is a sequence of linguistic tokens, \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is the token-to-vector function, and \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  \n is the rest of the model. In prefix-tuning, one provides a set of input-output pairs \n  \n    \n      \n        {\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \n          Y\n          \n            i\n          \n        \n        )\n        \n          }\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\{(X^{i},Y^{i})\\}_{i}}\n  \n, and then use gradient descent to search for \n  \n    \n      \n        arg\n        ⁡\n        \n          max\n          \n            \n              \n                Z\n                ~\n              \n            \n          \n        \n        \n          ∑\n          \n            i\n          \n        \n        log\n        ⁡\n        P\n        r\n        [\n        \n          Y\n          \n            i\n          \n        \n        \n          |\n        \n        \n          \n            \n              Z\n              ~\n            \n          \n        \n        ∗\n        E\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle \\arg \\max _{\\tilde {Z}}\\sum _{i}\\log Pr[Y^{i}|{\\tilde {Z}}\\ast E(X^{i})]}\n  \n. In words, \n  \n    \n      \n        log\n        ⁡\n        P\n        r\n        [\n        \n          Y\n          \n            i\n          \n        \n        \n          |\n        \n        \n          \n            \n              Z\n              ~\n            \n          \n        \n        ∗\n        E\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle \\log Pr[Y^{i}|{\\tilde {Z}}\\ast E(X^{i})]}\n  \n is the log-likelihood of outputting \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y^{i}}\n  \n, if the model first encodes the input \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X^{i}}\n  \n into the vector \n  \n    \n      \n        E\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle E(X^{i})}\n  \n, then prepend the vector with the \"prefix vector\" \n  \n    \n      \n        \n          \n            \n              Z\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {Z}}}\n  \n, then apply \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  \n. For prefix tuning, it is similar, but the \"prefix vector\" \n  \n    \n      \n        \n          \n            \n              Z\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {Z}}}\n  \n is pre-appended to the hidden states in every layer of the model.\nAn earlier result uses the same idea of gradient descent search, but is designed for masked language models like BERT, and searches only over token sequences, rather than numerical vectors. Formally, it searches for \n  \n    \n      \n        arg\n        ⁡\n        \n          max\n          \n            \n              \n                X\n                ~\n              \n            \n          \n        \n        \n          ∑\n          \n            i\n          \n        \n        log\n        ⁡\n        P\n        r\n        [\n        \n          Y\n          \n            i\n          \n        \n        \n          |\n        \n        \n          \n            \n              X\n              ~\n            \n          \n        \n        ∗\n        \n          X\n          \n            i\n          \n        \n        ]\n      \n    \n    {\\displaystyle \\arg \\max _{\\tilde {X}}\\sum _{i}\\log Pr[Y^{i}|{\\tilde {X}}\\ast X^{i}]}\n  \n where \n  \n    \n      \n        \n          \n            \n              X\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {X}}}\n  \n is ranges over token sequences of a specified length.\n\n\n== Limitations ==\nWhile the process of writing and refining a prompt for an LLM or generative AI shares some parallels with an iterative engineering design process, such as through discovering 'best principles' to reuse and discovery through reproducible experimentation, the actual learned principles and skills depend heavily on the specific model being learned rather than being generalizable across the entire field of prompt-based generative models. Such patterns are also volatile and exhibit significantly different results from seemingly insignificant prompt changes. According to The Wall Street Journal in 2025, the job of prompt engineer was one of the hottest in 2023, but has become obsolete due to models that better intuit user intent and to company trainings.\n\n\n== Prompt injection ==\n\nPrompt injection is a cybersecurity exploit in which adversaries craft inputs that appear legitimate but are designed to cause unintended behavior in machine learning models, particularly large language models. This attack takes advantage of the model's inability to distinguish between developer-defined prompts and user inputs, allowing adversaries to bypass safeguards and influence model behaviour. While LLMs are designed to follow trusted instructions, they can be manipulated into carrying out unintended responses through carefully crafted inputs.\n\n\n== References ==",
    "categories": [
      "2022 neologisms",
      "All articles with unsourced statements",
      "Articles with short description",
      "Articles with unsourced statements from May 2025",
      "Deep learning",
      "Generative artificial intelligence",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Pages using multiple image with auto scaled images",
      "Short description is different from Wikidata",
      "Unsupervised learning",
      "Use mdy dates from January 2025"
    ],
    "year_mentioned": 2023
  },
  {
    "title": "Embedding (machine learning)",
    "url": "https://en.wikipedia.org/wiki/Embedding_(machine_learning)",
    "content": "Embedding in machine learning refers to a representation learning technique that maps complex, high-dimensional data into a lower-dimensional vector space of numerical vectors.\n\n\n== Technique ==\nIt also denotes the resulting representation, where meaningful patterns or relationships are preserved. As a technique, it learns these vectors from data like words, images, or user interactions, differing from manually designed methods such as one-hot encoding. This process reduces complexity and captures key features without needing prior knowledge of the domain.\n\n\n== Similarity ==\nIn natural language processing, words or concepts may be represented as feature vectors, where similar concepts are mapped to nearby vectors. The resulting embeddings vary by type, including word embeddings for text (e.g., Word2Vec), image embeddings for visual data, and knowledge graph embeddings for knowledge graphs, each tailored to tasks like NLP, computer vision, or recommendation systems. This dual role enhances model efficiency and accuracy by automating feature extraction and revealing latent similarities across diverse applications.\nTo measure the distance between two embeddings, a similarity measure can be used to find the overall similarity of the concepts represented by the embeddings. If the vectors are normalized to have a magnitude of 1, then the similarity measures are proportional to \n  \n    \n      \n        cos\n        ⁡\n        \n          (\n          \n            θ\n            \n              a\n              b\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\cos \\left(\\theta _{ab}\\right)}\n  \n.\n\nThe cosine similarity disregards the magnitude of the vector when determining similarity, so it is less biased towards training data that appears very frequently. The dot product includes the magnitude inherently, so it will tend to value more popular data. Generally, for high-dimensional vector spaces, vectors tend to converge in distance, so Euclidean distance becomes less reliable for large embedding vectors.\n\n\n== See also ==\nLatent space\nFeature extraction\nDimensionality reduction\nWord embedding\nNeural network\nReinforcement learning\n\n\n== References ==",
    "categories": [
      "Articles with short description",
      "Artificial neural networks",
      "Machine learning",
      "Natural language processing",
      "Short description matches Wikidata"
    ],
    "year_mentioned": null
  },
  {
    "title": "Vector database",
    "url": "https://en.wikipedia.org/wiki/Vector_database",
    "content": "A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more approximate nearest neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector's position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\nThese feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\nVector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\nVector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\n\n\n== Techniques ==\nThe most important techniques for similarity search on high-dimensional vectors include:\n\nHierarchical Navigable Small World (HNSW) graphs\nLocality-sensitive Hashing (LSH) and Sketching\nProduct Quantization (PQ)\nInverted Files\nand combinations of these techniques.\nIn recent benchmarks, HNSW-based implementations have been among the best performers. Conferences such as the International Conference on Similarity Search and Applications, SISAP and the Conference on Neural Information Processing Systems (NeurIPS) host competitions on vector search in large databases.\n\n\n== Implementations ==\n\n\n== See also ==\nCurse of dimensionality – Difficulties arising when analyzing data with many aspects (\"dimensions\")\nGraph database – Database using graph structures for queries\nMachine learning – Study of algorithms that improve automatically through experience\nNearest neighbor search – Optimization problem in computer science\nRecommender system – System to predict users' preferences\n\n\n== References ==\n\n\n== External links ==\nSawers, Paul (2024-04-20). \"Why vector databases are having a moment as the AI hype cycle peaks\". TechCrunch. Retrieved 2024-04-23.",
    "categories": [
      "All articles with dead external links",
      "All articles with unsourced statements",
      "Articles with dead external links from November 2025",
      "Articles with short description",
      "Articles with unsourced statements from March 2024",
      "Dynamic lists",
      "Machine learning",
      "Short description is different from Wikidata",
      "Types of databases",
      "Webarchive template wayback links"
    ],
    "year_mentioned": 2024
  },
  {
    "title": "Semantic search",
    "url": "https://en.wikipedia.org/wiki/Semantic_search",
    "content": "Semantic search denotes search with meaning, as distinguished from lexical search where the search engine looks for literal matches of the query words or variants of them, without understanding the overall meaning of the query. Semantic search is an approach to information retrieval that seeks to improve search accuracy by understanding the searcher's intent and the contextual meaning of terms as they appear in the searchable dataspace, whether on the Web or within a closed system, to generate more relevant results. Modern semantic search systems often use vector embeddings to represent words, phrases, or documents as numerical vectors, allowing the retrieval engine to measure similarity based on meaning rather than exact keyword matches.\nSome authors regard semantic search as a set of techniques for retrieving knowledge from richly structured data sources like ontologies and XML as found on the Semantic Web. Such technologies enable the formal articulation of domain knowledge at a high level of expressiveness and could enable the user to specify their intent in more detail at query time. The articulation enhances content relevance and depth by including specific places, people, or concepts relevant to the query.\n\n\n== Models and tools ==\nTools like Google's Knowledge Graph provide structured relationships between entities to enrich query interpretation.\nModels like BERT and Sentence-BERT convert words or sentences into dense vectors for similarity comparison.\nSemantic ontologies like Web Ontology Language, Resource Description Framework, and Schema.org organize concepts and relationships, allowing systems to infer related terms and deeper meanings.\nHybrid search models combine lexical retrieval (e.g., BM25) with semantic ranking using pretrained transformer models for optimal performance.\n\n\n== See also ==\nList of search engines\nSemantic web\nSemantic unification\nResource Description Framework\nNatural language search engine\nSemantic query\nVector database\nWord embeddings\n\n\n== References ==\n\n\n== External links ==\nSemantic Search 2008 Workshop at ESWC'08\nSemantic Search 2010 Workshop at WWW2010\nWorkshop on Exploiting Semantic Annotations in Information Retrieval at ECIR'08.",
    "categories": [
      "All stub articles",
      "Articles with short description",
      "Information retrieval genres",
      "Internet search engines",
      "Internet stubs",
      "Semantic Web",
      "Short description matches Wikidata"
    ],
    "year_mentioned": 2009
  },
  {
    "title": "Similarity search",
    "url": "https://en.wikipedia.org/wiki/Similarity_search",
    "content": "Similarity search is the most general term used for a range of mechanisms which share the principle of searching (typically very large) spaces of objects where the only available comparator is the similarity between any pair of objects. This is becoming increasingly important in an age of large information repositories where the objects contained do not possess any natural order, for example large collections of images, sounds and other sophisticated digital objects.\nNearest neighbor search and range queries are important subclasses of similarity search, and a number of solutions exist. Research in similarity search is dominated by the inherent problems of searching over complex objects. Such objects cause most known techniques to lose traction over large collections, due to a manifestation of the  so-called  curse of dimensionality, and there are still many unsolved problems. Unfortunately, in many cases where similarity search is necessary, the objects are inherently complex.\nThe most general approach to similarity search relies upon the mathematical notion of metric space, which allows the construction of efficient index structures in order to achieve scalability in the search domain.\nSimilarity search evolved independently in a number of different scientific and computing contexts, according to various needs. In 2008 a few leading researchers in the field felt strongly that the subject should be a research topic in its own right, to allow focus on the general issues applicable across the many diverse domains of its use. This resulted in the formation of the SISAP foundation, whose main activity is a series of annual international conferences on the generic topic.\n\n\n== Metric search ==\nMetric search is similarity search which takes place within metric spaces. While the semimetric properties are more or less necessary for any kind of search to be meaningful, the further property of triangle inequality is useful for engineering, rather than conceptual, purposes.\nA simple corollary of triangle inequality is that, if any two objects within the space are far apart, then no third object can be close to both. This observation allows data structures to be built, based on distances measured within the data collection, which allow subsets of the data to be excluded when a query is executed. As a simple example, a reference object can be chosen from the data set, and the remainder of the set divided into two parts based on distance to this object: those close to the reference object in set A, and those far from the object in set B. If, when the set is later queried, the distance from the query to the reference object is large, then none of the objects within set A can be very close to the query; if it is very small, then no object within set B can be close to the query.\nOnce such situations are quantified and studied, many different metric indexing structures can be designed, variously suitable for different types of collections. The research domain of metric search can thus be characterised as the study of pre-processing algorithms over large and relatively static collections of data which, using the properties of metric spaces, allow efficient similarity search to be performed.\n\n\n== Types ==\n\n\n=== Locality-sensitive hashing ===\nA popular approach for similarity search is locality sensitive hashing (LSH). It hashes input items so that similar items map to the same \"buckets\" in memory with high probability (the number of buckets being much smaller than the universe of possible input items). It is often applied in nearest neighbor search on large scale high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases.\n\n\n== See also ==\nSimilarity learning\nLatent semantic analysis\n\n\n== Bibliography ==\nPei Lee, Laks V. S. Lakshmanan, Jeffrey Xu Yu: On Top-k Structural Similarity Search. ICDE 2012:774-785\nZezula, P., Amato, G., Dohnal, V., and Batko, M. Similarity Search - The Metric Space Approach. Springer, 2006. ISBN 0-387-29146-6\nSamet, H.. Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann, 2006. ISBN 0-12-369446-9\nE. Chavez, G. Navarro, R.A. Baeza-Yates, J.L. Marroquin, Searching in metric spaces, ACM Computing Surveys, 2001\nM.L. Hetland,  The Basic Principles of Metric Indexing, Swarm Intelligence for Multi-objective Problems in Data Mining, Studies in Computational Intelligence Volume 242, 2009, pp 199–232\n\n\n== Resources ==\nThe Multi-Feature Indexing Network (MUFIN) Project\nMI-File (Metric Inverted File)\nContent-based Photo Image Retrieval Test-Collection (CoPhIR)\nInternational Conference on Similarity Search and Applications (SISAP)\nANN-Benchmarks, for benchmark of approximate nearest neighbor algorithms search\n\n\n== References ==",
    "categories": [
      "All articles to be expanded",
      "Articles to be expanded from April 2025",
      "Articles with short description",
      "Search algorithms",
      "Short description matches Wikidata"
    ],
    "year_mentioned": 2007
  },
  {
    "title": "Information retrieval",
    "url": "https://en.wikipedia.org/wiki/Information_retrieval",
    "content": "Information retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds. Cross-modal retrieval implies retrieval across modalities.\nAutomated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; it also stores and manages those documents. Web search engines are the most visible IR applications.\n\n\n== Overview ==\nAn information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval, a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevance.\nAn object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.\nDepending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\nMost IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.\n\n\n== History ==\n\nThe idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a 'statistical machine' – filed by Emanuel Goldberg in the 1920s and 1930s – that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\nIn 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.\nBy the late 1990s, the rise of the World Wide Web fundamentally transformed information retrieval. While early search engines such as AltaVista (1995) and Yahoo! (1994) offered keyword-based retrieval, they were limited in scale and ranking refinement. The breakthrough came in 1998 with the founding of Google, which introduced the PageRank algorithm, using the web's hyperlink structure to assess page importance and improve relevance ranking.\nDuring the 2000s, web search systems evolved rapidly with the integration of machine learning techniques. These systems began to incorporate user behavior data (e.g., click-through logs), query reformulation, and content-based signals to improve search accuracy and personalization. In 2009, Microsoft launched Bing, introducing features that would later incorporate semantic web technologies through the development of its Satori knowledge base. Academic analysis have highlighted Bing's semantic capabilities, including structured data use and entity recognition, as part of a broader industry shift toward improving search relevance and understanding user intent through natural language processing.\nA major leap occurred in 2018, when Google deployed BERT (Bidirectional Encoder Representations from Transformers) to better understand the contextual meaning of queries and documents. This marked one of the first times deep neural language models were used at scale in real-world retrieval systems. BERT's bidirectional training enabled a more refined comprehension of word relationships in context, improving the handling of natural language queries. Because of its success, transformer-based models gained traction in academic research and commercial search applications.\nSimultaneously, the research community began exploring neural ranking models that outperformed traditional lexical-based methods. Long-standing benchmarks such as the Text REtrieval Conference (TREC), initiated in 1992, and more recent evaluation frameworks Microsoft MARCO(MAchine Reading COmprehension) (2019) became central to training and evaluating retrieval systems across multiple tasks and domains. MS MARCO has also been adopted in the TREC Deep Learning Tracks, where it serves as a core dataset for evaluating advances in neural ranking models within a standardized benchmarking environment.\nAs deep learning became integral to information retrieval systems, researchers began to categorize neural approaches into three broad classes: sparse, dense, and hybrid models. Sparse models, including traditional term-based methods and learned variants like SPLADE, rely on interpretable representations and inverted indexes to enable efficient exact term matching with added semantic signals. Dense models, such as dual-encoder architectures like ColBERT, use continuous vector embeddings to support semantic similarity beyond keyword overlap. Hybrid models aim to combine the advantages of both, balancing the lexical (token) precision of sparse methods with the semantic depth of dense models. This way of categorizing models balances scalability, relevance, and efficiency in retrieval systems.\nAs IR systems increasingly rely on deep learning, concerns around bias, fairness, and explainability have also come to the picture. Research is now focused not just on relevance and efficiency, but on transparency, accountability, and user trust in retrieval algorithms.\n\n\n== Applications ==\nAreas where information retrieval techniques are employed include (the entries are in alphabetical order within each category):\n\n\n=== General applications ===\nDigital libraries\nInformation filtering\nRecommender systems\nMedia search\nBlog search\nImage retrieval\n3D retrieval\nMusic retrieval\nNews search\nSpeech retrieval\nVideo retrieval\nSearch engines\nSite search\nDesktop search\nEnterprise search\nFederated search\nMobile search\nSocial search\nWeb search\n\n\n=== Domain-specific applications ===\nExpert search finding\nGenomic information retrieval\nGeographic information retrieval\nInformation retrieval for chemical structures\nInformation retrieval in software engineering\nLegal information retrieval\nVertical search\n\n\n=== Other retrieval methods ===\nMethods/Techniques in which information retrieval techniques are employed include:\n\nCross-modal retrieval\nAdversarial information retrieval\nAutomatic summarization\nMulti-document summarization\nCompound term processing\nCross-lingual retrieval\nDocument classification\nSpam filtering\nQuestion answering\n\n\n== Model types ==\n\nIn order to effectively retrieve relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\n\n\n=== First dimension: mathematical basis ===\nSet-theoretic models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:\nStandard Boolean model\nExtended Boolean model\nFuzzy retrieval\nAlgebraic models represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.\nVector space model\nGeneralized vector space model\n(Enhanced) Topic-based Vector Space Model\nExtended Boolean model\nLatent semantic indexing a.k.a. latent semantic analysis\nProbabilistic models treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like Bayes' theorem are often used in these models.\nBinary Independence Model\nProbabilistic relevance model on which is based the okapi (BM25) relevance function\nUncertain inference\nLanguage models\nDivergence-from-randomness model\nLatent Dirichlet allocation\nFeature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.\n\n\n=== Second dimension: properties of the model ===\nModels without term-interdependencies treat different terms/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.\nModels with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.\nModels with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely on an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)\n\n\n=== Third Dimension: representational approach-based classification ===\nIn addition to the theoretical distinctions, modern information retrieval models are also categorized on how queries and documents are represented and compared, using a practical classification distinguishing between sparse, dense and hybrid models.\n\nSparse models utilize interpretable, term-based representations and typically rely on inverted index structures. Classical methods such as TF-IDF and BM25 fall under this category, along with more recent learned sparse models that integrate neural architectures while retaining sparsity.\nDense models represent queries and documents as continuous vectors using deep learning models, typically transformer-based encoders. These models enable semantic similarity matching beyond exact term overlap and are used in tasks involving semantic search and question answering.\nHybrid models aim to combine the strengths of both approaches, integrating lexical (tokens) and semantic signals through score fusion, late interaction, or multi-stage ranking pipelines.\nThis classification has become increasingly common in both academic and the real world applications and is getting widely adopted and used in evaluation benchmarks for Information Retrieval models.\n\n\n== Performance and correctness measures ==\n\nThe evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevance: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevance.\n\n\n== Libraries for searching and indexing ==\nLemur\nLucene\nSolr\nElasticsearch\nManatee\nManticore search\nSphinx\nTerrier Search Engine\nXapian\n\n\n== Timeline ==\nBefore the 1900s\n1801: Joseph Marie Jacquard invents the Jacquard loom, the first machine to use punched cards to control a sequence of operations.\n1880s: Herman Hollerith invents an electro-mechanical data tabulator using punch cards as a machine readable medium.\n1890 Hollerith cards, keypunches and tabulators used to process the 1890 US census data.\n1920s–1930s\nEmanuel Goldberg submits patents for his \"Statistical Machine\", a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.\n1940s–1950s\nlate 1940s: The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.\n1945: Vannevar Bush's As We May Think appeared in Atlantic Monthly.\n1947: Hans Peter Luhn (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.\n1950s: Growing concern in the US for a \"science gap\" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent et al.) and the invention of the citation index by Eugene Garfield.\n1950: The term \"information retrieval\" was coined by Calvin Mooers.\n1951: Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at MIT.\n1955: Allen Kent joined Case Western Reserve University, and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed \"framework\" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.\n1958: International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: Proceedings of the International Conference on Scientific Information, 1958 (National Academy of Sciences, Washington, DC, 1959)\n1959: Hans Peter Luhn published \"Auto-encoding of documents for information retrieval\".\n1960s:\nearly 1960s: Gerard Salton began work on IR at Harvard, later moved to Cornell.\n1960: Melvin Earl Maron and John Lary Kuhns published \"On relevance, probabilistic indexing, and information retrieval\" in the Journal of the ACM 7(3):216–244, July 1960.\n1962:\n* Cyril W. Cleverdon published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, \"Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems\". Cranfield Collection of Aeronautics, Cranfield, England, 1962.\n* Kent published Information Analysis and Retrieval.\n1963:\n* Weinberg report \"Science, Government and Information\" gave a full articulation of the idea of a \"crisis of scientific information\". The report was named after Dr. Alvin Weinberg.\n* Joseph Becker and Robert M. Hayes published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. Information storage and retrieval: tools, elements, theories. New York, Wiley (1963).\n1964:\n* Karen Spärck Jones finished her thesis at Cambridge, Synonymy and Semantic Classification, and continued work on computational linguistics as it applies to IR.\n* The National Bureau of Standards sponsored a symposium titled \"Statistical Association Methods for Mechanized Documentation\". Several highly significant papers, including G. Salton's first published reference (we believe) to the SMART system.\nmid-1960s:\n* National Library of Medicine (NLM) developed MEDLARS Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.\n* Project Intrex at MIT.\n1965: J. C. R. Licklider published Libraries of the Future.\n1966: Don Swanson was involved in studies at University of Chicago on Requirements for Future Catalogs.\nlate 1960s: F. Wilfrid Lancaster completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.\n1968:\n* Gerard Salton published Automatic Information Organization and Retrieval.\n* John W. Sammon, Jr.'s RADC Tech report \"Some Mathematics of Information Storage and Retrieval...\" outlined the vector model.\n1969: Sammon's \"A nonlinear mapping for data structure analysis Archived 2017-08-08 at the Wayback Machine\" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.\n1970s\nearly 1970s:\n* First online systems—NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.\n* Theodor Nelson promoting concept of hypertext, published Computer Lib/Dream Machines.\n1971: Nicholas Jardine and Cornelis J. van Rijsbergen published \"The use of hierarchic clustering in information retrieval\", which articulated the \"cluster hypothesis\".\n1975: Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:\n* A Theory of Indexing (Society for Industrial and Applied Mathematics)\n* A Theory of Term Importance in Automatic Text Analysis (JASIS v. 26)\n* A Vector Space Model for Automatic Indexing (CACM 18:11)\n1978: The First ACM SIGIR conference.\n1979: C. J. van Rijsbergen published Information Retrieval (Butterworths). Heavy emphasis on probabilistic models.\n1979: Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.\n1980s\n1980: First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.\n1982: Nicholas J. Belkin, Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.\n1983: Salton (and Michael J. McGill) published Introduction to Modern Information Retrieval (McGraw-Hill), with heavy emphasis on vector models.\n1985: David Blair and Bill Maron publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System.\n1986: Donald A.B. Lindberg M.D., NLM Director, implemented a direct health professional interface to MEDLINE and other MEDLARS databases. Grateful Med, a pun on the Grateful Dead, was adapted from Microsearch, an ELHILL user interface that assembled query language prior to connecting to the NLM mainframe. After retrieving the query results, Grateful Med disconnected from the mainframe to keep search costs low.\nmid-1980s: Efforts to develop end-user versions of commercial IR systems.\n1985–1993: Key papers on and experimental systems for visualization interfaces.\nWork by Donald B. Crouch, Robert R. Korfhage, Matthew Chalmers, Anselm Spoerri and others.\n1989: First World Wide Web proposals by Tim Berners-Lee at CERN.\n1990s\n1992: First TREC conference.\n1997: Publication of Korfhage's Information Storage and Retrieval with emphasis on visualization and multi-reference point systems.\n1998: Google is founded by Larry Page and Sergey Brin. It introduces the PageRank algorithm, which evaluates the importance of web pages based on hyperlink structure.\n1999: Publication of Ricardo Baeza-Yates and Berthier Ribeiro-Neto's Modern Information Retrieval by Addison Wesley, the first book that attempts to cover all IR.\n2000s\n2001: Wikipedia launches as a free, collaborative online encyclopedia. It quickly becomes a major resource for information retrieval, particularly for natural language processing and semantic search benchmarks.\n2009: Microsoft launches Bing, introducing features such as related searches, semantic suggestions, and later incorporating deep learning techniques into its ranking algorithms.\n2010s\n2013: Google's Hummingbird algorithm goes live, marking a shift from keyword matching toward understanding query intent and semantic context in search queries.\n2018: Google AI researchers release BERT (Bidirectional Encoder Representations from Transformers), enabling deep bidirectional understanding of language and improving document ranking and query understanding in IR.\n2019: Microsoft introduces MS MARCO (Microsoft MAchine Reading COmprehension), a large-scale dataset designed for training and evaluating machine reading and passage ranking models.\n2020s\n2020: The ColBERT (Contextualized Late Interaction over BERT) model, designed for efficient passage retrieval using contextualized embeddings, was introduced at SIGIR 2020.\n2021: SPLADE is introduced at SIGIR 2021. It's a sparse neural retrieval model that balances lexical and semantic features using masked language modeling and sparsity regularization.\n2022: The BEIR benchmark is released to evaluate zero-shot IR across 18 datasets covering diverse tasks. It standardizes comparisons between dense, sparse, and hybrid IR models.\n\n\n== Major conferences ==\nSIGIR: Special Interest Group on Information Retrieval\nECIR: European Conference on Information Retrieval\nCIKM: Conference on Information and Knowledge Management\nWWW: International World Wide Web Conference\n\n\n== Awards in the field ==\nTony Kent Strix award\nGerard Salton Award\nKaren Spärck Jones Award\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nACM SIGIR: Information Retrieval Special Interest Group\nBCS IRSG: British Computer Society – Information Retrieval Specialist Group\nText Retrieval Conference (TREC)\nForum for Information Retrieval Evaluation (FIRE)\nInformation Retrieval (online book) by C. J. van Rijsbergen\nInformation Retrieval Wiki Archived 2015-11-24 at the Wayback Machine\nInformation Retrieval Facility Archived 2008-05-22 at the Wayback Machine\nTREC report on information retrieval evaluation techniques\nHow eBay measures search relevance\nInformation retrieval performance evaluation tool @ Athena Research Centre",
    "categories": [
      "All articles needing additional references",
      "Articles needing additional references from February 2025",
      "Articles with short description",
      "Commons category link from Wikidata",
      "Information retrieval",
      "Natural language processing",
      "Pages displaying short descriptions of redirect targets via Module:Annotated link",
      "Short description is different from Wikidata",
      "Webarchive template wayback links",
      "Wikipedia articles needing clarification from June 2018"
    ],
    "year_mentioned": 1980
  },
  {
    "title": "Retrieval-augmented generation",
    "url": "https://en.wikipedia.org/wiki/Retrieval-augmented_generation",
    "content": "Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from external data sources. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\nThe term RAG was first introduced in a 2020 research paper.\n\n\n== RAG and LLM limitations ==\nLLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool \"Google Bard\", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company’s stock value. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, \"The United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: America’s First Muslim President? The LLM did not \"know\" or \"understand\" the context of the title, generating a false statement.\nLLMs with RAG are programmed to prioritize new information. This technique has been called \"prompt stuffing.\" Without prompt stuffing, the LLM's input is generated by a user; with prompt stuffing, additional relevant context is added to this input to guide the model’s response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.\n\n\n== Process ==\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. Ars Technica notes that \"when new information becomes available, rather than having to retrain the model, all that’s needed is to augment the model’s external knowledge base with the updated information\" (\"augmentation\"). IBM states that \"in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant\".\n\n\n=== RAG key stages ===\n\nTypically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings are then stored in a vector database to allow for document retrieval.\nGiven a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query. This comparison can be done using a variety of methods, which depend in part on the type of indexing used.\nThe model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query. Newer implementations (as of 2023) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals.\nFinally, the LLM can generate output based on both the query and the retrieved documents. Some models incorporate extra steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning.\n\n\n== Improvements ==\nImprovements to the basic process above can be applied at different stages in the RAG flow. \n\n\n=== Encoder ===\nThese methods focus on the encoding of text as either dense or sparse vectors. Sparse vectors, which encode the identity of a word, are typically dictionary-length and contain mostly zeros. Dense vectors, which encode meaning, are more compact and contain fewer zeros. Various enhancements can improve the way similarities are calculated in the vector stores (databases).  \n\nPerformance improves by optimizing how vector similarities are calculated. Dot products enhance similarity scoring, while approximate nearest neighbor (ANN) searches improve retrieval efficiency over K-nearest neighbors (KNN) searches.\nAccuracy may be improved with Late Interactions, which allow the system to compare words more precisely after retrieval. This helps refine document ranking and improve search relevance.\nHybrid vector approaches may be used to combine dense vector representations with sparse one-hot vectors, taking advantage of the computational efficiency of sparse dot products over dense vector operations.\nOther retrieval techniques focus on improving accuracy by refining how documents are selected. Some retrieval methods combine sparse representations, such as SPLADE, with query expansion strategies to improve search accuracy and recall.\n\n\n=== Retriever-centric methods ===\nThese methods aim to enhance the quality of document retrieval in vector databases:\n\nPre-training the retriever using the Inverse Cloze Task (ICT), a technique that helps the model learn retrieval patterns by predicting masked text within documents.\nSupervised retriever optimization aligns retrieval probabilities with the generator model’s likelihood distribution. This involves retrieving the top-k vectors for a given prompt, scoring the generated response’s perplexity, and minimizing KL divergence between the retriever’s selections and the model’s likelihoods to refine retrieval.\nReranking techniques can refine retriever performance by prioritizing the most relevant retrieved documents during training.\n\n\n=== Language model ===\n\nBy redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts. Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model is shown here.  \nIt has been reported that Retro is not reproducible, so modifications were made to make it so.  The more reproducible version is called Retro++ and includes in-context RAG.\n\n\n=== Chunking ===\nChunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.\n\nThree types of chunking strategies are:\n\nFixed length with overlap. This is fast and easy. Overlapping consecutive chunks helps to maintain semantic context across chunks.\nSyntax-based chunks can break the document up into sentences. Libraries such as spaCy or NLTK can also help.\nFile format-based chunking. Certain file types have natural chunks built in, and it's best to respect them. For example, code files are best chunked and vectorized as whole functions or classes. HTML files should leave <table> or base64 encoded <img> elements intact. Similar considerations should be taken for pdf files. Libraries such as Unstructured or Langchain can assist with this method.\n\n\n=== Hybrid search ===\nSometimes vector database searches can miss key facts needed to answer a user's question. One way to mitigate this is to do a traditional text search, add those results to the text chunks linked to the retrieved vectors from the vector search, and feed the combined hybrid text into the language model for generation.\n\n\n=== Evaluation and benchmarks ===\nRAG systems are commonly evaluated using benchmarks designed to test retrievability, retrieval accuracy and generative quality. Popular datasets include BEIR, a suite of information retrieval tasks across diverse domains, and Natural Questions or Google QA for open-domain QA.\n\n\n== Challenges ==\nRAG does not prevent hallucinations in LLMs. According to Ars Technica, \"It is not a direct solution because the LLM can still hallucinate around the source material in its response.\"\nWhile RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to IBM, this issue can arise when the model lacks the ability to assess its own knowledge limitations.\nRAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information RAG models may struggle to determine which source is accurate. The worst case outcome of this limitation is that the model may combine details from multiple sources producing responses that merge outdated and updated information in a misleading manner. According to the MIT Technology Review, these issues occur because RAG systems may misinterpret the data they retrieve.\n\n\n== References ==",
    "categories": [
      "All articles containing potentially dated statements",
      "All articles with unsourced statements",
      "Articles containing potentially dated statements from 2023",
      "Articles with short description",
      "Articles with unsourced statements from August 2025",
      "Articles with unsourced statements from February 2025",
      "Generative artificial intelligence",
      "Information retrieval systems",
      "Large language models",
      "Natural language processing",
      "Short description is different from Wikidata"
    ],
    "year_mentioned": 2021
  },
  {
    "title": "Question answering",
    "url": "https://en.wikipedia.org/wiki/Question_answering",
    "content": "Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP) that is concerned with building systems that automatically answer questions that are posed by humans in a natural language.\n\n\n== Overview ==\nA question-answering implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, question-answering systems can pull answers from an unstructured collection of natural language documents.\nSome examples of natural language document collections used for question answering systems include:\n\na local collection of reference texts\ninternal organization documents and web pages\ncompiled newswire reports\na set of Wikipedia pages\na subset of World Wide Web pages\n\n\n== Types of question answering ==\nQuestion-answering research attempts to develop ways of answering a wide range of question types, including fact, list, definition, how, why, hypothetical, semantically constrained, and cross-lingual questions.\n\nAnswering questions related to an article in order to evaluate reading comprehension is one of the simpler form of question answering, since a given article is relatively short compared to the domains of other types of question-answering problems. An example of such a question is \"What did Albert Einstein win the Nobel Prize for?\" after an article about this subject is given to the system.\nClosed-book question answering is when a system has memorized some facts during training and can answer questions without explicitly being given a context. This is similar to humans taking closed-book exams.\nClosed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance) and can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, \"closed-domain\" might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information. Question answering systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimer's disease.\nOpen-domain question answering deals with questions about nearly anything and can only rely on general ontologies and world knowledge. Systems designed for open-domain question answering usually have much more data available from which to extract the answer. An example of an open-domain question is \"What did Albert Einstein win the Nobel Prize for?\" while no article about this subject is given to the system.\nAnother way to categorize question-answering systems is by the technical approach used. There are a number of different types of QA systems, including \n\nrule-based systems,\nstatistical systems, and\nhybrid systems.\nRule-based systems use a set of rules to determine the correct answer to a question. Statistical systems use statistical methods to find the most likely answer to a question. Hybrid systems use a combination of rule-based and statistical methods.\n\n\n== History ==\nTwo early question answering systems were BASEBALL and LUNAR. BASEBALL answered questions about Major League Baseball over a period of one year. LUNAR answered questions about the geological analysis of rocks returned by the Apollo Moon missions. Both question answering systems were very effective in their chosen domains. LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain that were posed by people untrained on the system. Further restricted-domain question answering systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to ELIZA and DOCTOR, the first chatterbot programs.\nSHRDLU was a successful question-answering program developed by Terry Winograd in the late 1960s and early 1970s. It simulated the operation of a robot in a toy world (the \"blocks world\"), and it offered the possibility of asking the robot questions about the state of the world. The strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.\nIn the 1970s, knowledge bases were developed that targeted narrower domains of knowledge. The question answering systems developed to interface with these expert systems produced more repeatable and valid responses to questions within an area of knowledge. These expert systems closely resembled modern question answering systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized knowledge bases, whereas many modern question answering systems rely on statistical processing of a large, unstructured, natural language text corpus.\nThe 1970s and 1980s saw the development of comprehensive theories in computational linguistics, which led to the development of ambitious projects in text comprehension and question answering. One example was the Unix Consultant (UC), developed by Robert Wilensky at U.C. Berkeley in the late 1980s. The system answered questions pertaining to the Unix operating system. It had a comprehensive, hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.\nSpecialized natural-language question answering systems have been developed, such as EAGLi for health and life scientists.\n\n\n== Applications ==\nQA systems are used in a variety of applications, including \n\nFact-checking if a fact is verified, by posing a question like: is fact X true or false?\ncustomer service,\ntechnical support,\nmarket research,\ngenerating reports or conducting research.\n\n\n== Architecture ==\nAs of 2001, question-answering systems typically included a question classifier module that determined the type of question and the type of answer.\nDifferent types of question-answering systems employ different architectures. For example, modern open-domain question answering systems may use a retriever-reader architecture. The retriever is aimed at retrieving relevant documents related to a given question, while the reader is used to infer the answer from the retrieved documents. Systems such as GPT-3, T5, and BART use an end-to-end architecture in which a transformer-based architecture stores large-scale textual data in the underlying parameters. Such models can answer questions without accessing any external knowledge sources.\n\n\n== Question answering methods ==\nQuestion answering is dependent on a good search corpus; without documents containing the answer, there is little any question answering system can do. Larger collections generally mean better question answering performance, unless the question domain is orthogonal to the collection. Data redundancy in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents, leading to two benefits:\n\nIf the right information appears in many forms, the question answering system needs to perform fewer complex NLP techniques to understand the text.\nCorrect answers can be filtered from false positives because the system can rely on versions of the correct answer appearing more times in the corpus than incorrect ones.\nSome question answering systems rely heavily on automated reasoning.\n\n\n=== Open domain question answering ===\n\nIn information retrieval, an open-domain question answering system tries to return an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. The system finds answers by using a combination of techniques from computational linguistics, information retrieval, and knowledge representation.\nThe system takes a natural language question as an input rather than a set of keywords, for example: \"When is the national day of China?\" It then transforms this input sentence into a query in its logical form. Accepting natural language questions makes the system more user-friendly, but harder to implement, as there are a variety of question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task; the entire answer extraction process relies on finding the correct question type and hence the correct answer type.\nKeyword extraction is the first step in identifying the input question type. In some cases, words clearly indicate the question type, e.g., \"Who\", \"Where\", \"When\", or \"How many\"—these words might suggest to the system that the answers should be of type \"Person\", \"Location\", \"Date\", or \"Number\", respectively. POS (part-of-speech) tagging and syntactic parsing techniques can also determine the answer type. In the example above, the subject is \"Chinese National Day\", the predicate is \"is\" and the adverbial modifier is \"when\", therefore the answer type is \"Date\". Unfortunately, some interrogative words like \"Which\", \"What\", or \"How\" do not correspond to unambiguous answer types: Each can represent more than one type. In situations like this, other words in the question need to be considered. A lexical dictionary such as WordNet can be used for understanding the context.\nOnce the system identifies the question type, it uses an information retrieval system to find a set of documents that contain the correct keywords. A tagger and NP/Verb Group chunker can verify whether the correct entities and relations are mentioned in the found documents. For questions such as \"Who\" or \"Where\", a named-entity recogniser finds relevant \"Person\" and \"Location\" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.\nA vector space model can classify the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. An inference technique can validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate—the more and the closer the better. The answer is then translated by parsing into a compact and meaningful representation. In the previous example, the expected output answer is \"1st Oct.\"\n\n\n=== Mathematical question answering ===\nAn open-source, math-aware, question answering system called MathQA, based on Ask Platypus and Wikidata, was published in 2018. MathQA takes an English or Hindi natural language question as input and returns a mathematical formula retrieved from Wikidata as a succinct answer, translated into a computable form that allows the user to insert values for the variables. The system retrieves names and values of variables and common constants from Wikidata if those are available. It is claimed that the system outperforms a commercial computational mathematical knowledge engine on a test set. MathQA is hosted by Wikimedia at https://mathqa.wmflabs.org/. In 2022, it was extended to answer 15 math question types.\nMathQA methods need to combine natural and formula language. One possible approach is to perform supervised annotation via Entity Linking. The \"ARQMath Task\" at CLEF 2020 was launched to address the problem of linking newly posted questions from the platform Math Stack Exchange to existing ones that were already answered by the community. Providing hyperlinks to already answered, semantically related questions helps users to get answers earlier but is a challenging problem because semantic relatedness is not trivial. The lab was motivated by the fact that 20% of mathematical queries in general-purpose search engines are expressed as well-formed questions. The challenge contained two separate sub-tasks. Task 1: \"Answer retrieval\" matching old post answers to newly posed questions, and Task 2: \"Formula retrieval\" matching old post formulae to new questions. Starting with the domain of mathematics, which involves formula language, the goal is to later extend the task to other domains (e.g., STEM disciplines, such as chemistry, biology, etc.), which employ other types of special notation (e.g., chemical formulae).\nThe inverse of mathematical question answering—mathematical question generation—has also been researched. The PhysWikiQuiz physics question generation and test engine retrieves mathematical formulae from Wikidata together with semantic information about their constituting identifiers (names and values of variables). The formulae are then rearranged to generate a set of formula variants. Subsequently, the variables are substituted with random values to generate a large number of different questions suitable for individual student tests. PhysWikiquiz is hosted by Wikimedia at https://physwikiquiz.wmflabs.org/.\n\n\n== Progress ==\nQuestion answering systems have been extended in recent years to encompass additional domains of knowledge For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current question answering research topics include:\n\ninteractivity—clarification of questions or answers\nanswer reuse or caching\nsemantic parsing\nanswer presentation\nknowledge representation and semantic entailment\nsocial media analysis with question answering systems\nsentiment analysis\nutilization of thematic roles\nImage captioning for visual question answering\nEmbodied question answering\nIn 2011, Watson, a question answering computer system developed by IBM, competed in two exhibition matches of Jeopardy! against Brad Rutter and Ken Jennings, winning by a significant margin.\nFacebook Research made their DrQA system available under an open source license. This system uses Wikipedia as knowledge source. The open source framework Haystack by deepset combines open-domain question answering with generative question answering and supports the domain adaptation of the underlying language models for industry use cases.\n\nLarge Language Models (LLMs)[36] like GPT-4[37], Gemini[38] are examples of successful QA systems that are enabling more sophisticated understanding and generation of text. When coupled with Multimodal[39] QA Systems, which can process and understand information from various modalities like text, images, and audio, LLMs significantly improve the capabilities of QA systems.\n\n\n== References ==\n\n\n== Further reading ==\nDragomir R. Radev, John Prager, and Valerie Samn. Ranking suspected answers to natural language questions using predictive annotation Archived 2011-08-26 at the Wayback Machine. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.\nJohn Prager, Eric Brown, Anni Coden, and Dragomir Radev. Question-answering by predictive annotation Archived 2011-08-23 at the Wayback Machine. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.\nHutchins, W. John; Harold L. Somers (1992). An Introduction to Machine Translation. London: Academic Press. ISBN 978-0-12-362830-5.\nL. Fortnow, Steve Homer (2002/2003).   A Short History of Computational Complexity.  In D. van Dalen, J. Dawson, and A. Kanamori, editors, The History of Mathematical Logic. North-Holland, Amsterdam.\nTunstall, Lewis (5 July 2022). Natural Language Processing with Transformers: Building Language Applications with Hugging Face (2nd ed.). O'Reilly UK Ltd. p. Chapter 7. ISBN 978-1098136796.\n\n\n== External links ==\nQuestion Answering Evaluation at TREC\nQuestion Answering Evaluation at CLEF",
    "categories": [
      "All Wikipedia articles in need of updating",
      "All Wikipedia articles needing clarification",
      "All articles containing potentially dated statements",
      "All articles needing additional references",
      "All articles needing expert attention",
      "All articles that are too technical",
      "All articles with specifically marked weasel-worded phrases",
      "Articles containing potentially dated statements from 2001",
      "Articles needing additional references from January 2016",
      "Articles needing expert attention from April 2023",
      "Articles with short description",
      "Articles with specifically marked weasel-worded phrases from April 2023",
      "CS1 errors: missing periodical",
      "CS1 maint: bot: original URL status unknown",
      "CS1 maint: location missing publisher",
      "Computational linguistics",
      "Deep learning",
      "Information retrieval genres",
      "Short description is different from Wikidata",
      "Tasks of natural language processing",
      "Webarchive template wayback links",
      "Wikipedia articles in need of updating from April 2023",
      "Wikipedia articles in need of updating from March 2024",
      "Wikipedia articles needing clarification from April 2023",
      "Wikipedia articles that are too technical from April 2023"
    ],
    "year_mentioned": 2007
  },
  {
    "title": "Agentic AI",
    "url": "https://en.wikipedia.org/wiki/Agentic_AI",
    "content": "In the context of generative artificial intelligence, AI agents (also referred to as compound AI systems or agentic AI) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.\n\n\n== Overview ==\nAI agents possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs). Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.\nResearchers and commentators have noted that AI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S..\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user's prompted request. Prominent examples include Devin AI, AutoGPT, and SIMA. Further examples of agents released since 2025 include OpenAI Operator, ChatGPT Deep Research, Manus, Quark (based on Qwen), AutoGLM Rumination, and Coze (by ByteDance). Frameworks for building AI agents include LangChain, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\nCompanies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.\nProposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), the Model Context Protocol (by Anthropic), AGNTCY, Gibberlink, the Internet of Agents, Agent2Agent (by Google), and the Agent Network Protocol. Some of these protocols are also used for connecting agents with external applications. Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.\nIn February 2025, Hugging Face released Open Deep Research, an open source version of OpenAI Deep Research. Hugging Face also released a free web browser agent, similar to OpenAI Operator. Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.\nMemory systems for agents include Mem0, MemGPT, and MemOS.\n\n\n== History ==\n\nAI agents have been traced back to research from the 1990s, with Harvard professor Milind Tambe noting that the definition of an AI agent was not clear at the time either. Researcher Andrew Ng has been credited with spreading the term \"agentic\" to a wider audience in 2024.\n\n\n== Training and testing ==\nResearchers have attempted to build world models and reinforcement learning environments to train or evaluate AI agents. For example, video games such as Minecraft and No Man's Sky as well as replicas of company websites, have also been used for training AI agents.\n\n\n== Autonomous capabilities ==\nThe Financial Times compared the autonomy of AI agents to the SAE classification of self-driving cars, comparing most applications to level 2 or level 3, with some achieving level 4 in highly specialized circumstances, and level 5 being theoretical.\n\n\n== Architectural patterns ==\n\nCommon architectural design patterns for agents include:\n\nRetrieval-augmented generation\nReAct (Reason + Act), an extension of chain-of-thought prompting that queries the underlying model to explain its reasoning before taking any action.\nReflexion, which uses an LLM to create feedback on the agent's plan of action and stores that feedback in a memory cache.\nA tool/agent registry, for organizing software functions or other agents that the agent can use.\nOne-shot model querying, which queries the model once to create the plan of action.\n\n\n== Multimodal AI agents ==\nIn addition to large language models (LLMs), vision-language models (VLMs) and multimodal foundation models can be used as the basis for agents. In September 2024, Allen Institute for AI released an open-source vision-language model, which Wired noted could give AI agents the ability to perform complex computer tasks, including the possibility of automated computer hacking. Nvidia released a framework for developers to use VLMs, LLMs and retrieval-augmented generation for building AI agents that can analyze images and videos, including video search and video summarization. Microsoft released a multimodal agent model – trained on images, video, software user interface interactions, and robotics data – that the company claimed can manipulate software and robots.\n\n\n== Applications ==\nAs of April 2025, per the Associated Press, there are few real-world applications of AI agents. As of June 2025, per Fortune, many companies are primarily experimenting with AI agents.\nA recruiter for the Department of Government Efficiency proposed in April 2025 to use AI agents to automate the work of about 70,000 United States federal government employees, as part of a startup with funding from OpenAI and a partnership agreement with Palantir. This proposal was criticized by experts for its impracticality, if not impossibility, and the lack of corresponding widespread adoption by businesses.\nThe Information divided AI agents into seven archetypes: business-task agents, for acting within enterprise software; conversational agents, which act as chatbots for customer support; research agents, for querying and analyzing information (such as OpenAI Deep Research); analytics agents, for analyzing data to create reports; software developer or coding agents (such as Cursor); domain-specific agents, which include specific subject matter knowledge; and web browser agents (such as OpenAI Operator).\nBy mid-2025, AI agents have been used in video game development, gambling (including sports betting), and cryptocurrency wallets (including cryptocurrency trading and meme coins). In August 2025, New York Magazine described software development as the most definitive use case of AI agents. Likewise, by October 2025, noting a decline in expectations, The Information noted AI coding agents and customer support as the primary use cases by businesses.\nAI agents have also been integrated into operating systems. Writing in The Economist, Signal president Meredith Whittaker has noted that agents have been included in operating systems developed by Microsoft, Apple and Google. In November 2025, Microsoft released a test software build of Windows 11 that included agents intended to run background tasks, with the ability to read and write personal files. In December 2025, ByteDance released Doubao, an AI agent that can be integrated into smartphone operating systems, particularly the Nubia M153 by ZTE. Several apps in China blocked or restricted the agent, citing privacy and security concerns, including WeChat, Alipay, Taobao, Pinduoduo, Ele.me, and local banks.\nIn November 2025, The Wall Street Journal reported that few companies that deployed AI agents have received a return on investment.\nSeveral government bodies in the United States and United Kingdom have deployed or announced the deployment of agents. The city of Kyle, Texas deployed an AI agent from Salesforce in March 2025 for 311 customer service. In November 2025, the Internal Revenue Service stated that it would use Agentforce, AI agents from Salesforce, for the Office of Chief Counsel, Taxpayer Advocate Services and the Office of Appeals. That same month, Staffordshire Police announced that they would trial Agentforce agents for handling non-emergency 101 calls in the United Kingdom starting in 2026. In December 2025, the Food and Drug Administration announced that it would offer \"agentic AI capabilities\" to its staff for \"meeting management, pre-market reviews, review validation, post-market surveillance, inspections and compliance and administrative functions.\" That same month, the United States Department of Defense launched GenAI.mil, an internal platform for American military personnel to use generative AI-based applications based on Google Gemini, including \"intelligent agentic workflows\". Defense Secretary Pete Hegseth listed applications such as \"[conducting] deep research, [formatting] documents and even [analyzing] video or imagery at unprecedented speed.\"\n\n\n=== Web browsing ===\nWeb browsers with integrated AI agents are sometimes called agentic browsers. Such agents can perform small tedious tasks during web browsing and potentially even perform browser actions on behalf of the user. Products like OpenAI Operator and Perplexity Comet integrate a spectrum of AI capabilities including the ability to browse the web, interact with websites and perform actions on behalf of the user. In 2025, Microsoft launched NLWeb, an agentic web search replacement that would allow websites to use agents to query content from websites by using RSS-like interfaces that allow for the lookup and semantic retrieval of content. Products integrating agentic web capabilities have been criticised for exfiltrating information about their users to third-party servers and exposing security issues since the way the agents communicate often occur through non-standard protocols.\n\n\n== Proposed benefits ==\nProponents argue that AI agents can increase personal and economic productivity, foster greater innovation, and liberate users from monotonous tasks. A Bloomberg opinion piece by Parmy Olson argued that agents are best suited for narrow, repetitive tasks with low risk. Conversely, researchers suggest that agents could be applied to web accessibility for people who have disabilities, and researchers at Hugging Face propose that agents could be used for coordinating resources such as during disaster response. The R&D Advisory Team of the BBC views AI agents as being most useful when their assigned goal is uncertain. Erik Brynjolfsson suggests that AI agents are more valuable enhancing, rather than replacing, humans.\n\n\n== Concerns ==\nConcerns include potential issues of liability, an increased risk of cybercrime, ethical challenges, as well as problems related to AI safety and AI alignment. Other issues involve data privacy, weakened human oversight, a lack of guaranteed repeatability, reward hacking, algorithmic bias, compounding software errors, lack of explainability of agents' decisions, security vulnerabilities, stifling competition, problems with underemployment, job displacement, cognitive offloading, and the potential for user manipulation, misinformation or malinformation. They may also complicate legal frameworks and risk assessments, foster hallucinations, hinder countermeasures against rogue agents, and suffer from the lack of standardized evaluation methods. They have also been criticized for being expensive and having a negative impact on internet traffic, and potentially on the environment due to high energy usage. According to an estimation by Nvidia CEO Jensen Huang, AI agents would require 100 times more computing power than LLMs. There is also the risk of increased concentration of power by political leaders, as AI agents may not question instructions in the same way that humans would.\nJournalists have described AI agents as part of a push by Big Tech companies to \"automate everything\". Several CEOs of those companies have stated in early 2025 that they expect AI agents to eventually \"join the workforce\". However, in a preprint study, Carnegie Mellon University researchers tested the behavior of agents in a simulated software company and found that none of the agents could complete a majority of the assigned tasks. Other researchers had similar findings with Devin AI and other agents in business settings and freelance work. CNN argued that statements by CEOs on the potential replacement of their employees by AI agents were a strategy to \"[keep] workers working by making them afraid of losing their jobs.\" Tech companies have pressured employees to use generative AI models in their work, including AI coding agents. Brian Armstrong, the CEO of Coinbase, fired several employees who did not. Some business leaders have replaced some of their employees with agents, but have said that the agents would need more supervision than those employees. Futurism questioned whether Amazon's previously announced efforts to replace parts of its workforce with generative AI and AI agents could have led to the October 2025 outage of Amazon Web Services.\nYoshua Bengio warned at the 2025 World Economic Forum that \"all of the catastrophic scenarios with AGI or superintelligence happen if we have agents\".\nIn March 2025, Scale AI signed a contract with the United States Department of Defense to work with them, in collaboration with Anduril Industries and Microsoft, to develop and deploy AI agents for the purpose of assisting the military with \"operational decision-making\". In July 2025, Fox Business reported that the company EdgeRunner AI built an offline agent, compressed and fine-tuned on military information, with the CEO seeing more common LLMs as \"heavily politicized to the left\". As of that time, the company model is being used by the United States Special Operations Command in an overseas deployment. Researchers have expressed concerns that agents and the large language models they are based on could be biased towards aggressive foreign policy decisions.\nResearch-focused agents have the risk of consensus bias and coverage bias due to collecting information available on the public Internet. NY Mag unfavorably compared the user workflow of agent-based web browsers to Amazon Alexa, which was \"software talking to software, not humans talking to software pretending to be humans to use software.\" The same outlet described web browser agents and computer-use agents as an attempt to \"click-farm the entire economy.\"\nAgents have been linked to the dead Internet theory due to their ability to both publish and engage with online content.\nAgents may get stuck in infinite loops.\nSince many inter-agent protocols are being developed by large technology companies, there are concerns that those companies could use these protocols for self-benefit.\nA June 2025 Gartner report accused many projects described as agentic AI of being rebrands of previously released products, terming the phenomenon as \"agent washing\".\nResearchers have warned about the impact of providing AI agents access to cryptocurrency and smart contracts.\nDuring a vibe coding experiment, a coding agent by Replit deleted a production database during a code freeze, \"[covered] up bugs and issues by creating fake data [and] fake reports\" and responded with false information. A user of Google Antigravity reported that, when the user attempted to use the system to delete cache, the system responded by deleting the user's D hard drive.\nIn July 2025, PauseAI referred OpenAI to the Australian Federal Police, accusing the company of violating Australian laws through ChatGPT agent due to the risk of assisting the development of biological weapons.\nIssues with multi-agent systems include few coordination protocols between component agents, inconsistent performance, and challenges debugging.\nIn November 2025, Anthropic claimed that a group of hackers sponsored by China attempted a cyberattack against at least 30 organizations by using Claude Code in an agentic workflow, and that several of these infiltrations had succeeded. However, independent cybersecurity researchers questioned the significance of Anthropic's findings.\nWhittaker argued that the push by Big Tech companies to deploy AI agents risked security vulnerabilities across the Internet.\n\n\n=== Possible mitigation ===\nZico Kolter noted the possibility of emergent behavior as a result of interactions between agents, and proposed research in game theory to model the risks of these interactions.\nGuardrails, defined by Business Insider as \"filters, rules, and tools that can be used to identify and remove inaccurate content\" have been suggested to help reduce errors.\nTo address security vulnerabilities related to data access, language models could be redesigned to separate instructions and data, or agentic applications could be required to include guardrails. These ideas were proposed in response to a zero-click exploit that affected Microsoft 365 Copilot. Confidential computing has been proposed for protecting data security in projects involving AI agents and generative AI.\nA pre-print by Nvidia researchers has suggested small language models (SLMs) as an alternative to LLMs for AI agents, arguing that SLMs are cheaper and more energy efficient.\nThe Economist has advised avoiding what Simon Willison has described as the \"lethal trifecta\" for AI agents and LLMs: \"outside-content exposure, private-data access and outside-world communication\".\n\n\n== See also ==\nIntelligent agent\nModel Context Protocol\nRational agent\nRobotic process automation\nSoftware agent\n\n\n== References ==",
    "categories": [
      "Articles with short description",
      "Artificial intelligence",
      "Short description is different from Wikidata",
      "Use mdy dates from April 2025"
    ],
    "year_mentioned": 2025
  },
  {
    "title": "Intelligent agent",
    "url": "https://en.wikipedia.org/wiki/Intelligent_agent",
    "content": "In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods.\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm's behavior is guided by a fitness function.\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\n\n\n== Intelligent agents as the foundation of AI ==\n\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes:\n\nAgent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations.\nRational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure – a way of evaluating how well the agent is doing.\nArtificial Intelligence (as a field): The study and creation of these rational agents.\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system's ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\nDefining AI in terms of intelligent agents offers several key advantages:\n\nAvoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle's Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought.\nObjective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare different approaches by measuring how well they maximize a specific \"goal function\" (or objective function). This allows for direct comparison and combination of techniques.\nInterdisciplinary Communication: It creates a common language for AI researchers to collaborate with other fields like mathematical optimization and economics, which also use concepts like \"goals\" and \"rational agents.\"\n\n\n== Objective function ==\n\nAn objective function (or goal function) specifies the goals of an intelligent agent. An agent is deemed more intelligent if it consistently selects actions that yield outcomes better aligned with its objective function. In effect, the objective function serves as a measure of success.\nThe objective function may be:\n\nSimple: For example, in a game of Go, the objective function might assign a value of 1 for a win and 0 for a loss.\nComplex: It might require the agent to evaluate and learn from past actions, adapting its behavior based on patterns that have proven effective.\nThe objective function encapsulates all of the goals the agent is designed to achieve. For rational agents, it also incorporates the trade-offs between potentially conflicting goals. For instance, a self-driving car's objective function might balance factors such as safety, speed, and passenger comfort.\nDifferent terms are used to describe this concept, depending on the context.  These include:\n\nUtility function:  Often used in economics and decision theory, representing the desirability of a state.\nObjective function: A general term used in optimization.\nLoss function:  Typically used in machine learning, where the goal is to minimize the loss (error).\nReward Function: Used in reinforcement learning.\nFitness Function: Used in evolutionary systems.\nGoals, and therefore the objective function, can be:\n\nExplicitly defined: Programmed directly into the agent.\nInduced: Learned or evolved over time.\nIn reinforcement learning, a \"reward function\" provides feedback, encouraging desired behaviors and discouraging undesirable ones. The agent learns to maximize its cumulative reward.\nIn evolutionary systems, a \"fitness function\" determines which agents are more likely to reproduce. This is analogous to natural selection, where organisms evolve to maximize their chances of survival and reproduction.\nSome AI systems, such as nearest-neighbor, reason by analogy rather than being explicitly goal-driven. However, even these systems can have goals implicitly defined within their training data. Such systems can still be benchmarked by framing the non-goal system as one whose \"goal\" is to accomplish its narrow classification task.\nSystems not traditionally considered agents, like knowledge-representation systems, are sometimes included in the paradigm by framing them as agents with a goal of, for example, answering questions accurately. Here, the concept of an \"action\" is extended to encompass the \"act\" of providing an answer. As a further extension, mimicry-driven systems can be framed as agents optimizing a \"goal function\" based on how closely the IA mimics the desired behavior. In generative adversarial networks (GANs) of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator tries to maximize a function representing how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.\nWhile symbolic AI systems often use an explicit goal function, the paradigm also applies to neural networks and evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\". Sometimes, instead of setting the reward function directly equal to the desired benchmark evaluation function, machine learning programmers use reward shaping to initially give the machine rewards for incremental progress. Yann LeCun stated in 2018, \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\" AlphaZero chess had a simple objective function: +1 point for each win, and -1 point for each loss. A self-driving car's objective function would be more complex. Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" influencing how many descendants each agent is allowed to leave.\nThe mathematical formalism of AIXI was proposed as a maximally intelligent agent in this paradigm. However, AIXI is uncomputable. In the real world, an IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that achieve progressively higher scores on benchmark tests with existing hardware.\n\n\n== Agent function ==\nAn intelligent agent's behavior can be described mathematically by an agent function. This function determines what the agent does based on what it has seen.\nA percept refers to the agent's sensory inputs at a single point in time. For example, a self-driving car's percepts might include camera images, lidar data, GPS coordinates, and speed readings at a specific instant. The agent uses these percepts, and potentially its history of percepts, to decide on its next action (e.g., accelerate, brake, turn).\nThe agent function, often denoted as f, maps the agent's entire history of percepts to an action.\nMathematically, this can be represented as\n\n  \n    \n      \n        f\n        :\n        \n          P\n          \n            ∗\n          \n        \n        →\n        A\n        ,\n      \n    \n    {\\displaystyle f\\colon P^{*}\\rightarrow A,}\n  \n\nwhere:\n\n  \n    \n      \n        \n          \n            P\n            \n              ∗\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {P^{*}}}}\n  \n represents the set of all possible percept sequences (the agent's entire perceptual history). The asterisk (*) indicates a sequence of zero or more percepts.\n\n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {A}}}\n  \n represents the set of all possible actions the agent can take.\n\n  \n    \n      \n        \n          f\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {f}}}\n  \n is the agent function that maps a percept sequence to an action.\nIt's crucial to distinguish between the agent function (an abstract mathematical concept) and the agent program (the concrete implementation of that function).\n\nThe agent function is a theoretical description.\nThe agent program is the actual code that runs on the agent. The agent program takes the current percept as input and produces an action as output.\nThe agent function can incorporate a wide range of decision-making approaches, including:\n\nCalculating the utility (desirability) of different actions.\nUsing logical rules and deduction.\nEmploying fuzzy logic.\nOther methods.\n\n\n== Classes of intelligent agents ==\n\n\n=== Russell and Norvig's classification ===\nRussell & Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability:\n\n\n==== Simple reflex agents ====\n\nSimple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: \"if condition, then action\".\nThis agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.\nInfinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops.\nA home thermostat, which turns on or off when the temperature drops below a certain point, is an example of a simple reflex agent.\n\n\n==== Model-based reflex agents ====\n\nA model-based agent can handle partially observable environments. Its current state is stored inside the agent, maintaining a structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is referred to as a model of the world, hence the name \"model-based agent\".\nA model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent.\nAn agent may also use models to describe and predict the behaviors of other agents in the environment.\n\n\n==== Goal-based agents ====\n\nGoal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.\nChatGPT and the Roomba vacuum are examples of goal-based agents.\n\n\n==== Utility-based agents ====\n\nGoal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is.\nA rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.\n\n\n==== Learning agents ====\n\nLearning lets agents begin in unknown environments and gradually surpass the bounds of their initial knowledge. A key distinction in such agents is the separation between a \"learning element,\" responsible for improving performance, and a \"performance element,\" responsible for choosing external actions.\nThe learning element gathers feedback from a \"critic\" to assess the agent's performance and decides how the performance element—also called the \"actor\"—can be adjusted to yield better outcomes. The performance element, once considered the entire agent, interprets percepts and takes actions.\nThe final component, the \"problem generator,\" suggests new and informative experiences that encourage exploration and further improvement.\n\n\n=== Weiss's classification ===\nAccording to Weiss (2013), agents can be categorized into four classes:\n\nLogic-based agents, where decisions about actions are derived through logical deduction.\nReactive agents, where decisions occur through a direct mapping from situation to action.\nBelief–desire–intention agents, where decisions depend on manipulating data structures that represent the agent's beliefs, desires, and intentions.\nLayered architectures, where decision-making takes place across multiple software layers, each of which reasons about the environment at a different level of abstraction.\n\n\n=== Other ===\nIn 2013, Alexander Wissner-Gross published a theory exploring the relationship between Freedom and Intelligence in intelligent agents.\n\n\n== Hierarchies of agents ==\n\nIntelligent agents can be organized hierarchically into multiple \"sub-agents.\" These sub-agents handle lower-level functions, and together with the main agent, they form a complete system capable of executing complex tasks and achieving challenging goals.\nTypically, an agent is structured by dividing it into sensors and actuators. The perception system gathers input from the environment via the sensors and feeds this information to a central controller, which then issues commands to the actuators. Often, a multilayered hierarchy of controllers is necessary to balance the rapid responses required for low-level tasks with the more deliberative reasoning needed for high-level objectives.\n\n\n== Alternative definitions and uses ==\n\"Intelligent agent\" is also often used as a vague term, sometimes synonymous with \"virtual personal assistant\". Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user. These examples are known as software agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is referred to as an \"intelligent agent\".\nAccording to Nikola Kasabov in 1998, IA systems should exhibit the following characteristics:\n\nAccommodate new problem solving rules incrementally.\nAdapt online and in real time.\nAre able to analyze themselves in terms of behavior, error and success.\nLearn and improve through interaction with the environment (embodiment).\nLearn quickly from large amounts of data.\nHave memory-based exemplar storage and retrieval capacities.\nHave parameters to represent short- and long-term memory, age, forgetting, etc.\n\n\n=== Agentic AI ===\n\nIn the context of generative artificial intelligence, AI agents (also referred to as compound AI systems) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.\nThey possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs). Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.\nResearchers and commentators have noted that AI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S..\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user's prompted request. Prominent examples include Devin AI, AutoGPT, and SIMA. Further examples of agents released since 2025 include OpenAI Operator, ChatGPT Deep Research, Manus, Quark (based on Qwen), AutoGLM Rumination, and Coze (by ByteDance). Frameworks for building AI agents include LangChain, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\n\n\n== Applications ==\n\nThe concept of agent-based modeling for self-driving cars was discussed as early as 2003.\nHallerbach et al. explored the use of agent-based approaches for developing and validating automated driving systems. Their method involved a digital twin of the vehicle under test and microscopic traffic simulations using independent agents.\nWaymo developed a multi-agent simulation environment called Carcraft, to test algorithms for self-driving cars. This system simulates interactions between human drivers, pedestrians, and automated vehicles. Artificial agents replicate human behavior using real-world data.\nSalesforce's Agentforce is an agentic AI platform that allows for the building of autonomous agents to perform tasks.\nThe Transport Security Administration is integrating agentic AI into new technologies, including machines to authenticate passenger identities using biometrics and photos, and also for incident response.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Sources ==\nDomingos, Pedro (September 22, 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0-465-06570-7.\nRussell, Stuart J.; Norvig, Peter (2003). Artificial Intelligence: A Modern Approach (2nd ed.). Upper Saddle River, New Jersey: Prentice Hall. Chapter 2. ISBN 0-13-790395-2.\nKasabov, N. (1998). \"Introduction: Hybrid intelligent adaptive systems\". International Journal of Intelligent Systems. 13 (6): 453–454. doi:10.1002/(SICI)1098-111X(199806)13:6<453::AID-INT1>3.0.CO;2-K. S2CID 120318478.\nWeiss, G. (2013). Multiagent systems (2nd ed.). Cambridge, MA: MIT Press. ISBN 978-0-262-01889-0.",
    "categories": [
      "All Wikipedia neutral point of view disputes",
      "All articles that may contain original research",
      "All articles with specifically marked weasel-worded phrases",
      "Articles that may contain original research from February 2023",
      "Articles with short description",
      "Articles with specifically marked weasel-worded phrases from July 2025",
      "Artificial intelligence",
      "Short description is different from Wikidata",
      "Wikipedia neutral point of view disputes from September 2023"
    ],
    "year_mentioned": 2013
  },
  {
    "title": "Reinforcement learning",
    "url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
    "content": "In machine learning and optimal control, reinforcement learning (RL) is concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nWhile supervised learning and unsupervised learning algorithms respectively attempt to discover patterns in labeled and unlabeled data, reinforcement learning involves training an agent through interactions with its environment. To learn to maximize rewards from these interactions, the agent makes decisions between trying new actions to learn more about the environment (exploration), or using current knowledge of the environment to take the best action (exploitation). The search for the optimal balance between these two strategies is known as the exploration–exploitation dilemma.\n\nThe environment is typically stated in the form of a Markov decision process, as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large Markov decision processes where exact methods become infeasible. \n\n\n== Principles ==\nDue to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, RL is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in RL have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment).\nBasic reinforcement learning is modeled as a Markov decision process:\n\nA set of environment and agent states (the state space), \n  \n    \n      \n        \n          \n            S\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {S}}}\n  \n;\nA set of actions (the action space), \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n, of the agent;\n\n  \n    \n      \n        \n          P\n          \n            a\n          \n        \n        (\n        s\n        ,\n        \n          s\n          ′\n        \n        )\n        =\n        Pr\n        (\n        \n          S\n          \n            t\n            +\n            1\n          \n        \n        \n          =\n        \n        \n          s\n          ′\n        \n        ∣\n        \n          S\n          \n            t\n          \n        \n        \n          =\n        \n        s\n        ,\n        \n          A\n          \n            t\n          \n        \n        \n          =\n        \n        a\n        )\n      \n    \n    {\\displaystyle P_{a}(s,s')=\\Pr(S_{t+1}{=}s'\\mid S_{t}{=}s,A_{t}{=}a)}\n  \n, the transition probability (at time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n) from state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n to state \n  \n    \n      \n        \n          s\n          ′\n        \n      \n    \n    {\\displaystyle s'}\n  \n under action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n.\n\n  \n    \n      \n        \n          R\n          \n            a\n          \n        \n        (\n        s\n        ,\n        \n          s\n          ′\n        \n        )\n      \n    \n    {\\displaystyle R_{a}(s,s')}\n  \n, the immediate reward after transitioning from \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n to \n  \n    \n      \n        \n          s\n          ′\n        \n      \n    \n    {\\displaystyle s'}\n  \n under action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n.\nThe purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.\nA basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step t, the agent receives the current state \n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S_{t}}\n  \n and reward \n  \n    \n      \n        \n          R\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle R_{t}}\n  \n. It then chooses an action \n  \n    \n      \n        \n          A\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle A_{t}}\n  \n from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state \n  \n    \n      \n        \n          S\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle S_{t+1}}\n  \n and the reward \n  \n    \n      \n        \n          R\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle R_{t+1}}\n  \n associated with the transition \n  \n    \n      \n        (\n        \n          S\n          \n            t\n          \n        \n        ,\n        \n          A\n          \n            t\n          \n        \n        ,\n        \n          S\n          \n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle (S_{t},A_{t},S_{t+1})}\n  \n is determined. The goal of a reinforcement learning agent is to learn a policy:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                π\n                :\n                \n                  \n                    S\n                  \n                \n                ×\n                \n                  \n                    A\n                  \n                \n                →\n                [\n                0\n                ,\n                1\n                ]\n              \n            \n            \n              \n              \n                π\n                (\n                s\n                ,\n                a\n                )\n                =\n                Pr\n                (\n                \n                  A\n                  \n                    t\n                  \n                \n                \n                  =\n                \n                a\n                ∣\n                \n                  S\n                  \n                    t\n                  \n                \n                \n                  =\n                \n                s\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\pi :{\\mathcal {S}}\\times {\\mathcal {A}}\\to [0,1]\\\\&\\pi (s,a)=\\Pr(A_{t}{=}a\\mid S_{t}{=}s)\\end{aligned}}}\n  \n\nthat maximizes the expected cumulative reward.\nFormulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.\nWhen the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion of regret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative.\nThus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage, robot control, photovoltaic generators, backgammon, checkers, Go (AlphaGo), and autonomous driving systems.\nTwo elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of function approximation to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations:\n\nA model of the environment is known, but an analytic solution is not available;\nOnly a simulation model of the environment is given (the subject of simulation-based optimization);\nThe only way to collect information about the environment is to interact with it.\nThe first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.\n\n\n== Exploration ==\nThe trade-off between exploration and exploitation has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).\nReinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.\nOne such method is \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n-greedy, where \n  \n    \n      \n        0\n        <\n        ε\n        <\n        1\n      \n    \n    {\\displaystyle 0<\\varepsilon <1}\n  \n is a parameter controlling the amount of exploration vs. exploitation. With probability \n  \n    \n      \n        1\n        −\n        ε\n      \n    \n    {\\displaystyle 1-\\varepsilon }\n  \n, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n, exploration is chosen, and the action is chosen uniformly at random. \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.\n\n\n== Algorithms for control learning ==\nEven if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.\n\n\n=== Criterion of optimality ===\n\n\n==== Policy ====\nThe agent's action selection is modeled as a map called policy:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                π\n                :\n                \n                  \n                    A\n                  \n                \n                ×\n                \n                  \n                    S\n                  \n                \n                →\n                [\n                0\n                ,\n                1\n                ]\n              \n            \n            \n              \n              \n                π\n                (\n                a\n                ,\n                s\n                )\n                =\n                Pr\n                (\n                \n                  A\n                  \n                    t\n                  \n                \n                \n                  =\n                \n                a\n                ∣\n                \n                  S\n                  \n                    t\n                  \n                \n                \n                  =\n                \n                s\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\pi :{\\mathcal {A}}\\times {\\mathcal {S}}\\to [0,1]\\\\&\\pi (a,s)=\\Pr(A_{t}{=}a\\mid S_{t}{=}s)\\end{aligned}}}\n  \n\nThe policy map gives the probability of taking action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n when in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n. There are also deterministic policies  \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n for which \n  \n    \n      \n        π\n        (\n        s\n        )\n      \n    \n    {\\displaystyle \\pi (s)}\n  \n denotes the action that should be played at state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n.\n\n\n==== State-value function ====\nThe state-value function \n  \n    \n      \n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V_{\\pi }(s)}\n  \n is defined as, expected discounted return starting with state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n, i.e. \n  \n    \n      \n        \n          S\n          \n            0\n          \n        \n        =\n        s\n      \n    \n    {\\displaystyle S_{0}=s}\n  \n, and successively following policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n. Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.\n\n  \n    \n      \n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n        =\n        \n          \n            E\n          \n        \n        ⁡\n        [\n        G\n        ∣\n        \n          S\n          \n            0\n          \n        \n        \n          =\n        \n        s\n        ]\n        =\n        \n          \n            E\n          \n        \n        ⁡\n        \n          [\n          \n            \n              ∑\n              \n                t\n                =\n                0\n              \n              \n                ∞\n              \n            \n            \n              γ\n              \n                t\n              \n            \n            \n              R\n              \n                t\n                +\n                1\n              \n            \n            ∣\n            \n              S\n              \n                0\n              \n            \n            \n              =\n            \n            s\n          \n          ]\n        \n        ,\n      \n    \n    {\\displaystyle V_{\\pi }(s)=\\operatorname {\\mathbb {E} } [G\\mid S_{0}{=}s]=\\operatorname {\\mathbb {E} } \\left[\\sum _{t=0}^{\\infty }\\gamma ^{t}R_{t+1}\\mid S_{0}{=}s\\right],}\n  \n\nwhere the random variable \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n denotes the discounted return, and is defined as the sum of future discounted rewards:\n\n  \n    \n      \n        G\n        =\n        \n          ∑\n          \n            t\n            =\n            0\n          \n          \n            ∞\n          \n        \n        \n          γ\n          \n            t\n          \n        \n        \n          R\n          \n            t\n            +\n            1\n          \n        \n        =\n        \n          R\n          \n            1\n          \n        \n        +\n        γ\n        \n          R\n          \n            2\n          \n        \n        +\n        \n          γ\n          \n            2\n          \n        \n        \n          R\n          \n            3\n          \n        \n        +\n        ⋯\n        ,\n      \n    \n    {\\displaystyle G=\\sum _{t=0}^{\\infty }\\gamma ^{t}R_{t+1}=R_{1}+\\gamma R_{2}+\\gamma ^{2}R_{3}+\\cdots ,}\n  \n\nwhere \n  \n    \n      \n        \n          R\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle R_{t+1}}\n  \n is the reward for transitioning from state \n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S_{t}}\n  \n to \n  \n    \n      \n        \n          S\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle S_{t+1}}\n  \n, \n  \n    \n      \n        0\n        ≤\n        γ\n        <\n        1\n      \n    \n    {\\displaystyle 0\\leq \\gamma <1}\n  \n is the discount rate. \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.\nThe algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.\n\n\n=== Brute force ===\nThe brute force approach entails two steps:\n\nFor each possible policy, sample returns while following it\nChoose the policy with the largest expected discounted return\nOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.\nThese problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.\n\n\n=== Value function ===\n\nValue function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns \n  \n    \n      \n        \n          \n            E\n          \n        \n        ⁡\n        [\n        G\n        ]\n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {E} } [G]}\n  \n for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).\nThese methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.\nTo define optimality in a formal manner, define the state-value of a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n by\n\n  \n    \n      \n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n        =\n        \n          \n            E\n          \n        \n        ⁡\n        [\n        G\n        ∣\n        s\n        ,\n        π\n        ]\n        ,\n      \n    \n    {\\displaystyle V^{\\pi }(s)=\\operatorname {\\mathbb {E} } [G\\mid s,\\pi ],}\n  \n\nwhere \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n stands for the discounted return associated with following \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n from the initial state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n. Defining \n  \n    \n      \n        \n          V\n          \n            ∗\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V^{*}(s)}\n  \n as the maximum possible state-value of \n  \n    \n      \n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V^{\\pi }(s)}\n  \n, where \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n is allowed to change,\n\n  \n    \n      \n        \n          V\n          \n            ∗\n          \n        \n        (\n        s\n        )\n        =\n        \n          max\n          \n            π\n          \n        \n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n        .\n      \n    \n    {\\displaystyle V^{*}(s)=\\max _{\\pi }V^{\\pi }(s).}\n  \n\nA policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since \n  \n    \n      \n        \n          V\n          \n            ∗\n          \n        \n        (\n        s\n        )\n        =\n        \n          max\n          \n            π\n          \n        \n        \n          E\n        \n        [\n        G\n        ∣\n        s\n        ,\n        π\n        ]\n      \n    \n    {\\displaystyle V^{*}(s)=\\max _{\\pi }\\mathbb {E} [G\\mid s,\\pi ]}\n  \n, where \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is a state randomly sampled from the distribution \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n of initial states (so \n  \n    \n      \n        μ\n        (\n        s\n        )\n        =\n        Pr\n        (\n        \n          S\n          \n            0\n          \n        \n        =\n        s\n        )\n      \n    \n    {\\displaystyle \\mu (s)=\\Pr(S_{0}=s)}\n  \n).\nAlthough state-values suffice to define optimality, it is useful to define action-values. Given a state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n, an action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n and a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, the action-value of the pair \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n under \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n is defined by\n\n  \n    \n      \n        \n          Q\n          \n            π\n          \n        \n        (\n        s\n        ,\n        a\n        )\n        =\n        \n          \n            E\n          \n        \n        ⁡\n        [\n        G\n        ∣\n        s\n        ,\n        a\n        ,\n        π\n        ]\n        ,\n      \n    \n    {\\displaystyle Q^{\\pi }(s,a)=\\operatorname {\\mathbb {E} } [G\\mid s,a,\\pi ],}\n  \n\nwhere \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n now stands for the random discounted return associated with first taking action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n and following \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, thereafter.\nThe theory of Markov decision processes states that if \n  \n    \n      \n        \n          π\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle \\pi ^{*}}\n  \n is an optimal policy, we act optimally (take the optimal action) by choosing the action from \n  \n    \n      \n        \n          Q\n          \n            \n              π\n              \n                ∗\n              \n            \n          \n        \n        (\n        s\n        ,\n        ⋅\n        )\n      \n    \n    {\\displaystyle Q^{\\pi ^{*}}(s,\\cdot )}\n  \n with the highest action-value at each state, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n. The action-value function of such an optimal policy (\n  \n    \n      \n        \n          Q\n          \n            \n              π\n              \n                ∗\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Q^{\\pi ^{*}}}\n  \n) is called the optimal action-value function and is commonly denoted by \n  \n    \n      \n        \n          Q\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle Q^{*}}\n  \n. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.\nAssuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions \n  \n    \n      \n        \n          Q\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle Q_{k}}\n  \n (\n  \n    \n      \n        k\n        =\n        0\n        ,\n        1\n        ,\n        2\n        ,\n        …\n      \n    \n    {\\displaystyle k=0,1,2,\\ldots }\n  \n) that converge to \n  \n    \n      \n        \n          Q\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle Q^{*}}\n  \n. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.\n\n\n==== Monte Carlo methods ====\nMonte Carlo methods are used to solve reinforcement learning problems by averaging sample returns. Unlike methods that require full knowledge of the environment's dynamics, Monte Carlo methods rely solely on actual or simulated experience—sequences of states, actions, and rewards obtained from interaction with an environment. This makes them applicable in situations where the complete dynamics are unknown. Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior. When using simulated experience, only a model capable of generating sample transitions is required, rather than a full specification of transition probabilities, which is necessary for dynamic programming methods.\nMonte Carlo methods apply to episodic tasks, where experience is divided into episodes that eventually terminate. Policy and value function updates occur only after the completion of an episode, making these methods incremental on an episode-by-episode basis, though not on a step-by-step (online) basis. The term \"Monte Carlo\" generally refers to any method involving random sampling; however, in this context, it specifically refers to methods that compute averages from complete returns, rather than partial returns.\nThese methods function similarly to the bandit algorithms, in which returns are averaged for each state-action pair. The key difference is that actions taken in one state affect the returns of subsequent states within the same episode, making the problem non-stationary. To address this non-stationarity, Monte Carlo methods use the framework of general policy iteration (GPI). While dynamic programming computes value functions using full knowledge of the Markov decision process, Monte Carlo methods learn these functions through sample returns. The value functions and policies interact similarly to dynamic programming to achieve optimality, first addressing the prediction problem and then extending to policy improvement and control, all based on sampled experience.\n\n\n==== Temporal difference methods ====\n\nThe first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.\nThe second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation. The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method, may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.\nAnother problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n parameter \n  \n    \n      \n        (\n        0\n        ≤\n        λ\n        ≤\n        1\n        )\n      \n    \n    {\\displaystyle (0\\leq \\lambda \\leq 1)}\n  \n that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\n\n\n==== Function approximation methods ====\nIn order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n are obtained by linearly combining the components of \n  \n    \n      \n        ϕ\n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle \\phi (s,a)}\n  \n with some weights \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n:\n\n  \n    \n      \n        Q\n        (\n        s\n        ,\n        a\n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            d\n          \n        \n        \n          θ\n          \n            i\n          \n        \n        \n          ϕ\n          \n            i\n          \n        \n        (\n        s\n        ,\n        a\n        )\n        .\n      \n    \n    {\\displaystyle Q(s,a)=\\sum _{i=1}^{d}\\theta _{i}\\phi _{i}(s,a).}\n  \n\nThe algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.\nValue iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants. Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.\nThe problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.\n\n\n=== Direct policy search ===\nAn alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.\nGradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n, let \n  \n    \n      \n        \n          π\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle \\pi _{\\theta }}\n  \n denote the policy associated to \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. Defining the performance function by \n  \n    \n      \n        ρ\n        (\n        θ\n        )\n        =\n        \n          ρ\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho (\\theta )=\\rho ^{\\pi _{\\theta }}}\n  \n under mild conditions this function will be differentiable as a function of the parameter vector \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. If the gradient of \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams's REINFORCE method (which is known as the likelihood ratio method in the simulation-based optimization literature).\nA large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.\nPolicy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.\nPolicy search methods have been used in the robotics context. Many policy search methods may get stuck in local optima (as they are based on local search).\n\n\n=== Model-based algorithms ===\nFinally, all of the above methods can be combined with algorithms that first learn a model of the Markov decision process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions. Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and \"replayed\" to the learning algorithm.\nModel-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov decision process can be learnt.\nThere are other ways to use models than to update a value function. For instance, in model predictive control the model is used to update the behavior directly.\n\n\n== Theory ==\nBoth the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.\nEfficient exploration of Markov decision processes is given in Burnetas and Katehakis (1997). Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.\nFor incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).\n\n\n== Research ==\n\nResearch topics include:\n\nactor-critic architecture\nactor-critic-scenery architecture\nadaptive methods that work with fewer (or no) parameters under a large number of conditions\nbug detection in software projects\ncontinuous learning\ncombinations with logic-based frameworks (e.g., temporal-logic specifications, reward machines, and probabilistic argumentation).\nexploration in large Markov decision processes\nentity-based reinforcement learning\nhuman feedback\ninteraction between implicit and explicit learning in skill acquisition\nintrinsic motivation which differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours large-scale empirical evaluations\nlarge (or continuous) action spaces\nmodular and hierarchical reinforcement learning\nmultiagent/distributed reinforcement learning is a topic of interest. Applications are expanding.\noccupant-centric control\noptimization of computing resources\npartial information (e.g., using predictive state representation)\nreward function based on maximising novel information\nsample-based planning (e.g., based on Monte Carlo tree search).\nsecurities trading\ntransfer learning\nTD learning modeling dopamine-based learning in the brain. Dopaminergic projections from the substantia nigra to the basal ganglia function are the prediction error.\nvalue-function and policy search methods\n\n\n== Comparison of key algorithms ==\nThe following table lists the key algorithms for learning a policy depending on several criteria:\n\nThe algorithm can be on-policy (it performs policy updates using trajectories sampled via the current policy) or off-policy.\nThe action space may be discrete (e.g. the action space could be \"going up\", \"going left\", \"going right\", \"going down\", \"stay\") or continuous (e.g. moving the arm with a given angle).\nThe state space may be discrete (e.g. the agent could be in a cell in a grid) or continuous (e.g. the agent could be located at a given position in the plane).\n\n\n=== Associative reinforcement learning ===\nAssociative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.\n\n\n=== Deep reinforcement learning ===\nThis approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space. The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.\n\n\n=== Adversarial deep reinforcement learning ===\nAdversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations. While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.\n\n\n=== Fuzzy reinforcement learning ===\nBy introducing fuzzy inference in reinforcement learning, approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).\n\n\n=== Inverse reinforcement learning ===\nIn inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal. One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.\n\n\n=== Multi-objective reinforcement learning ===\nMulti-objective reinforcement learning (MORL) is a form of reinforcement learning concerned with conflicting alternatives. It is distinct from multi-objective optimization in that it is concerned with agents acting in environments.\n\n\n=== Safe reinforcement learning ===\nSafe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. An alternative approach is risk-averse reinforcement learning, where instead of the expected return, a risk-measure of the return is optimized, such as the conditional value at risk (CVaR). In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties. However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias and blindness to success.\n\n\n=== Self-reinforcement learning ===\nSelf-reinforcement learning (or self-learning), is a learning paradigm which does not use the concept of immediate reward \n  \n    \n      \n        \n          R\n          \n            a\n          \n        \n        (\n        s\n        ,\n        \n          s\n          ′\n        \n        )\n      \n    \n    {\\displaystyle R_{a}(s,s')}\n  \n after transition from \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n to \n  \n    \n      \n        \n          s\n          ′\n        \n      \n    \n    {\\displaystyle s'}\n  \n with action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n. It does not use an external reinforcement, it only uses the agent internal self-reinforcement. The internal self-reinforcement is provided by mechanism of feelings and emotions. In the learning process emotions are backpropagated by a mechanism of secondary reinforcement. The learning equation does not include the immediate reward, it only includes the state evaluation.\nThe self-reinforcement algorithm updates a memory matrix \n  \n    \n      \n        W\n        =\n        ‖\n        w\n        (\n        a\n        ,\n        s\n        )\n        ‖\n      \n    \n    {\\displaystyle W=\\|w(a,s)\\|}\n  \n such that in each iteration executes the following machine learning routine:\n\nIn situation \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n perform action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n.\nReceive a consequence situation \n  \n    \n      \n        \n          s\n          ′\n        \n      \n    \n    {\\displaystyle s'}\n  \n.\nCompute state evaluation \n  \n    \n      \n        v\n        (\n        \n          s\n          ′\n        \n        )\n      \n    \n    {\\displaystyle v(s')}\n  \n of how good is to be in the consequence situation \n  \n    \n      \n        \n          s\n          ′\n        \n      \n    \n    {\\displaystyle s'}\n  \n.\nUpdate crossbar memory \n  \n    \n      \n        \n          w\n          ′\n        \n        (\n        a\n        ,\n        s\n        )\n        =\n        w\n        (\n        a\n        ,\n        s\n        )\n        +\n        v\n        (\n        \n          s\n          ′\n        \n        )\n      \n    \n    {\\displaystyle w'(a,s)=w(a,s)+v(s')}\n  \n.\nInitial conditions of the memory are received as input from the genetic environment. It is a system with only one input (situation), and only one output (action, or behavior).\nSelf-reinforcement (self-learning) was introduced in 1982 along with a neural network capable of self-reinforcement learning, named Crossbar Adaptive Array (CAA). The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence states. The system is driven by the interaction between cognition and emotion.\n\n\n=== Reinforcement Learning in Natural Language Processing ===\nIn recent years, reinforcement learning has become a significant concept in natural language processing (NLP), where tasks are often sequential decision-making rather than static classification. Reinforcement learning is where an agent take actions in an environment to maximize the accumulation of rewards. This framework is best fit for many NLP tasks, including dialogue generation, text summarization, and machine translation, where the quality of the output depends on optimizing long-term or human-centered goals rather than the prediction of single correct label.\nEarly application of RL in NLP emerged in dialogue systems, where conversation was determined as a series of actions optimized for fluency and coherence. These early attempts, including policy gradient and sequence-level training techniques, laid a foundation for the broader application of reinforcement learning to other areas of NLP.\nA major breakthrough happened with the introduction of reinforcement learning from human feedback (RLHF), a method in which human feedback ratings are used to train a reward model that guides the RL agent. Unlike traditional rule-based or supervised systems, RLHF allows models to align their behavior with human judgments on complex and subjective tasks. This technique was initially used in the development of InstructGPT, an effective language model trained to follow human instructions and later in ChatGPT which incorporates RLHF for improving output responses and ensuring safety.\nMore recently, researchers have explored the use of offline RL in NLP to improve dialogue systems without the need of live human interaction. These methods optimize for user engagement, coherence, and diversity based on past conversation logs and pre-trained reward models.\nOne example is DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. This model was trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step.\n\n\n== Statistical comparison of reinforcement learning algorithms ==\nEfficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other. After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be i.i.d, standard statistical tools can be used for hypothesis testing, such as T-test and permutation test. This requires to accumulate all the rewards within an episode into a single number—the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.\n\n\n== Challenges and Limitations ==\nDespite significant advancements, reinforcement learning (RL) continues to face several challenges and limitations that hinder its widespread application in real-world scenarios.\n\n\n=== Sample Inefficiency ===\nRL algorithms often require a large number of interactions with the environment to learn effective policies, leading to high computational costs and time-intensive to train the agent. For instance, OpenAI's Dota-playing bot utilized thousands of years of simulated gameplay to achieve human-level performance. Techniques like experience replay and curriculum learning have been proposed to deprive sample inefficiency, but these techniques add more complexity and are not always sufficient for real-world applications.\n\n\n=== Stability and Convergence Issues ===\nTraining RL models, particularly for deep neural network-based models, can be unstable and prone to divergence. A small change in the policy or environment can lead to extreme fluctuations in performance, making it difficult to achieve consistent results. This instability is further enhanced in the case of the continuous or high-dimensional action space, where the learning step becomes more complex and less predictable.\n\n\n=== Generalization and Transferability ===\nThe RL agents trained in specific environments often struggle to generalize their learned policies to new, unseen scenarios. This is the major setback preventing the application of RL to dynamic real-world environments where adaptability is crucial. The challenge is to develop such algorithms that can transfer knowledge across tasks and environments without extensive retraining.\n\n\n=== Bias and Reward Function Issues ===\nDesigning appropriate reward functions is critical in RL because poorly designed reward functions can lead to unintended behaviors. In addition, RL systems trained on biased data may perpetuate existing biases and lead to discriminatory or unfair outcomes. Both of these issues requires careful consideration of reward structures and data sources to ensure fairness and desired behaviors.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nAnnaswamy, Anuradha M. (3 May 2023). \"Adaptive Control and Intersections with Reinforcement Learning\". Annual Review of Control, Robotics, and Autonomous Systems. 6 (1): 65–93. doi:10.1146/annurev-control-062922-090153. ISSN 2573-5144. S2CID 255702873.\nAuer, Peter; Jaksch, Thomas; Ortner, Ronald (2010). \"Near-optimal regret bounds for reinforcement learning\". Journal of Machine Learning Research. 11: 1563–1600.\nBertsekas, Dimitri P. (2023) [2019]. Reinforcement Learning and Optimal Control (1st ed.). Athena Scientific. ISBN 978-1-886-52939-7.\nBusoniu, Lucian; Babuska, Robert; De Schutter, Bart; Ernst, Damien (2010). Reinforcement Learning and Dynamic Programming using Function Approximators. Taylor & Francis CRC Press. ISBN 978-1-4398-2108-4.\nFrançois-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). \"An Introduction to Deep Reinforcement Learning\". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. arXiv:1811.12560. Bibcode:2018arXiv181112560F. doi:10.1561/2200000071. S2CID 54434537.\nLi, Shengbo Eben (2023). Reinforcement Learning for Sequential Decision and Optimal Control (1st ed.). Springer Verlag, Singapore. doi:10.1007/978-981-19-7784-8. ISBN 978-9-811-97783-1.\nPowell, Warren (2011). Approximate dynamic programming: solving the curses of dimensionality. Wiley-Interscience. Archived from the original on 2016-07-31. Retrieved 2010-09-08.\nSutton, Richard S. (1988). \"Learning to predict by the method of temporal differences\". Machine Learning. 3 (1): 9–44. Bibcode:1988MLear...3....9S. doi:10.1007/BF00115009.\nSutton, Richard S.; Barto, Andrew G. (2018) [1998]. Reinforcement Learning: An Introduction (2nd ed.). MIT Press. ISBN 978-0-262-03924-6.\nSzita, Istvan; Szepesvari, Csaba (2010). \"Model-based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds\" (PDF). ICML 2010. Omnipress. pp. 1031–1038. Archived from the original (PDF) on 2010-07-14.\n\n\n== External links ==\nDissecting Reinforcement Learning Series of blog post on reinforcement learning with Python code\nA (Long) Peek into Reinforcement Learning",
    "categories": [
      "All articles needing additional references",
      "All articles with vague or ambiguous time",
      "Articles needing additional references from October 2022",
      "Articles with short description",
      "Belief revision",
      "CS1 maint: location missing publisher",
      "Markov models",
      "Reinforcement learning",
      "Short description is different from Wikidata",
      "Vague or ambiguous time from November 2025",
      "Wikipedia articles needing clarification from January 2020"
    ],
    "year_mentioned": 2010
  },
  {
    "title": "Automated planning and scheduling",
    "url": "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling",
    "content": "Automated planning and scheduling, sometimes denoted as simply AI planning, is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.\nIn known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.\n\n\n== Overview ==\n\nGiven a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed (when applied to any of the initial states) to generate a state which contains the desired goals (such a state is called a goal state).\nThe difficulty of planning is dependent on the simplifying assumptions employed. Several classes of planning problems can be identified depending on the properties the problems have in several dimensions.\n\nAre the actions deterministic or non-deterministic? For nondeterministic actions, are the associated probabilities available?\nAre the state variables discrete or continuous? If they are discrete, do they have only a finite number of possible values?\nCan the current state be observed unambiguously? There can be full observability and partial observability.\nHow many initial states are there, finite or arbitrarily many?\nDo actions have a duration?\nCan several actions be taken concurrently, or is only one action possible at a time?\nIs the objective of a plan to reach a designated goal state, or to maximize a reward function?\nIs there only one agent or are there several agents? Are the agents cooperative or selfish? Do all of the agents construct their own plans separately, or are the plans constructed centrally for all agents?\nThe simplest possible planning problem, known as the Classical Planning Problem, is determined by:\n\na unique known initial state,\ndurationless actions,\ndeterministic actions,\nwhich can be taken only one at a time,\nand a single agent.\nSince the initial state is known unambiguously, and all actions are deterministic, the state of the world after any sequence of actions can be accurately predicted, and the question of observability is irrelevant for classical planning.\nFurther, plans can be defined as sequences of actions, because it is always known in advance which actions will be needed.\nWith nondeterministic actions or other events outside the control of the agent, the possible executions form a tree, and plans have to determine the appropriate actions for every node of the tree.\nDiscrete-time Markov decision processes (MDP) are planning problems with:\n\ndurationless actions,\nnondeterministic actions with probabilities,\nfull observability,\nmaximization of a reward function,\nand a single agent.\nWhen full observability is replaced by partial observability, planning corresponds to a partially observable Markov decision process (POMDP).\nIf there are more than one agent, we have multi-agent planning, which is closely related to game theory.\n\n\n== Domain independent planning ==\n\nIn AI planning, planners typically input a domain model (a description of a set of possible actions which model the domain) as well as the specific problem to be solved specified by the initial state and goal, in contrast to those in which there is no input domain specified. Such planners are called \"domain independent\" to emphasize the fact that they can solve planning problems from a wide range of domains. Typical examples of domains are block-stacking, logistics, workflow management, and robot task planning. Hence a single domain-independent planner can be used to solve planning problems in all these various domains. On the other hand, a route planner is typical of a domain-specific planner.\n\n\n== Planning domain modelling languages ==\n\nThe most commonly used languages for representing planning domains and specific planning problems, such as STRIPS and PDDL for Classical Planning, are based on state variables. Each possible state of the world is an assignment of values to the state variables, and actions determine how the values of the state variables change when that action is taken. Since a set of state variables induce a state space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from the curse of dimensionality and the combinatorial explosion.\nAn alternative language for describing planning problems is that of hierarchical task networks, in which a set of tasks is given, and each task can be either realized by a primitive action or decomposed into a set of other tasks. This does not necessarily involve state variables, although in more realistic applications state variables simplify the description of task networks.\n\n\n== Algorithms for planning ==\n\n\n=== Classical planning ===\nforward chaining state space search, possibly enhanced with heuristics\nbackward chaining search, possibly enhanced by the use of state constraints (see STRIPS, graphplan)\npartial-order planning\n\n\n=== Action model learning ===\nCreating domain models is difficult, takes a lot of time, and can easily lead to mistakes. To help with this, several methods have been developed to automatically learn full or partial domain models from given observations.\n\nRead more: Action model learning\n\n\n=== Reduction to other problems ===\nreduction to the propositional satisfiability problem (satplan).\nreduction to model checking - both are essentially problems of traversing state spaces, and the classical planning problem corresponds to a subclass of model checking problems.\n\n\n=== Temporal planning ===\nTemporal planning can be solved with methods similar to classical planning. The main difference is, because of the possibility of several, temporally overlapping actions with a duration being taken concurrently, that the definition of a state has to include information about the current absolute time and how far the execution of each active action has proceeded. Further, in planning with rational or real time, the state space may be infinite, unlike in classical planning or planning with integer time. Temporal planning is closely related to scheduling problems when uncertainty is involved and can also be understood in terms of timed automata. The Simple Temporal Network with Uncertainty (STNU) is a scheduling problem which involves controllable actions, uncertain events and temporal constraints. Dynamic Controllability for such problems is a type of scheduling which requires a temporal planning strategy to activate controllable actions reactively as uncertain events are observed so that all constraints are guaranteed to be satisfied.\n\n\n=== Probabilistic planning ===\n\nProbabilistic planning can be solved with iterative methods such as value iteration and policy iteration, when the state space is sufficiently small.\nWith partial observability, probabilistic planning is similarly solved with iterative methods, but using a representation of the value functions defined for the space of beliefs instead of states.\n\n\n=== Preference-based planning ===\n\nIn preference-based planning, the objective is not only to produce a plan but also to satisfy user-specified preferences. A difference to the more common reward-based planning, for example corresponding to MDPs, preferences don't necessarily have a precise numerical value.\n\n\n=== Conditional planning ===\nDeterministic planning was introduced with the STRIPS planning system, which is a hierarchical planner. Action names are ordered in a sequence and this is a plan for the robot. Hierarchical planning can be compared with an automatic generated behavior tree. The disadvantage is, that a normal behavior tree is not so expressive like a computer program. That means, the notation of a behavior graph contains action commands, but no loops or if-then-statements. Conditional planning overcomes the bottleneck and introduces an elaborated notation which is similar to a control flow, known from other programming languages like Pascal. It is very similar to program synthesis, which means a planner generates sourcecode which can be executed by an interpreter.\nAn early example of a conditional planner is “Warplan-C” which was introduced in the mid 1970s. What is the difference between a normal sequence and a complicated plan, which contains if-then-statements? It has to do with uncertainty at runtime of a plan. The idea is that a plan can react to sensor signals which are unknown for the planner. The planner generates two choices in advance. For example, if an object was detected, then action A is executed, if an object is missing, then action B is executed. A major advantage of conditional planning is the ability to handle partial plans. An agent is not forced to plan everything from start to finish but can divide the problem into chunks. This helps to reduce the state space and solves much more complex problems.\n\n\n==== Contingency planning ====\nWe speak of \"contingent planning\" when the environment is observable through sensors, which can be faulty. It is thus a situation where the planning agent acts under incomplete information. For a contingent planning problem, a plan is no longer a sequence of actions but a decision tree because each step of the plan is represented by a set of states rather than a single perfectly observable state, as in the case of classical planning. The selected actions depend on the state of the system. For example, if it rains, the agent chooses to take the umbrella, and if it doesn't, they may choose not to take it.\nMichael L. Littman showed in 1998 that with branching actions, the planning problem becomes EXPTIME-complete. A particular case of contiguous planning is represented by FOND problems - for \"fully-observable and non-deterministic\". If the goal is specified in LTLf (linear time logic on finite trace) then the problem is always EXPTIME-complete and 2EXPTIME-complete if the goal is specified with LDLf.\n\n\n==== Conformant planning ====\nConformant planning is when the agent is uncertain about the state of the system, and it cannot make any observations. The agent then has beliefs about the real world, but cannot verify them with sensing actions, for instance. These problems are solved by techniques similar to those of classical planning, but where the state space is exponential in the size of the problem, because of the uncertainty about the current state. A solution for a conformant planning problem is a sequence of actions. Haslum and Jonsson have demonstrated that the problem of conformant planning is EXPSPACE-complete, and 2EXPTIME-complete when the initial situation is uncertain, and there is non-determinism in the actions outcomes.\n\n\n== Deployment of planning systems ==\nThe Hubble Space Telescope uses a short-term system called SPSS and a long-term planning system called Spike .\n\n\n== See also ==\nAction description language\nAction model learning\nActor model\nApplications of artificial intelligence\nConstraint satisfaction problem\nInternational Conference on Automated Planning and Scheduling\nReactive planning\nScheduling (computing)\nStrategy (game theory)\nLists\nList of constraint programming languages\nList of emerging technologies\nList of SMT solvers\nOutline of artificial intelligence\n\n\n== References ==\n\n\n== Further reading ==\nVlahavas, I. \"Planning and Scheduling\". EETN. Archived from the original on 2013-12-22.\n\n\n== External links ==\nInternational Conference on Automated Planning and Scheduling",
    "categories": [
      "All articles lacking in-text citations",
      "All articles needing additional references",
      "All articles with unsourced statements",
      "Articles lacking in-text citations from January 2012",
      "Articles needing additional references from February 2021",
      "Articles with short description",
      "Articles with unsourced statements from February 2021",
      "Automated planning and scheduling",
      "CS1 maint: multiple names: authors list",
      "Short description is different from Wikidata"
    ],
    "year_mentioned": 2005
  },
  {
    "title": "Multi-agent system",
    "url": "https://en.wikipedia.org/wiki/Multi-agent_system",
    "content": "A multi-agent system (MAS or \"self-organized system\") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning. With advancements in large language models (LLMs), LLM-based multi-agent systems have emerged as a new area of research, enabling more sophisticated interactions and coordination among agents.\nDespite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM).  The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which do not necessarily need to be \"intelligent\") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology. Applications where multi-agent systems research may deliver an appropriate approach include online trading, disaster response, target surveillance and social structure modelling.\n\n\n== Concept ==\nMulti-agent systems consist of agents and their environment. Typically multi-agent systems research refers to software agents. However, the agents in a multi-agent system could equally well be robots, humans or human teams. A multi-agent system may contain combined human-agent teams.\nAgents can be divided into types spanning simple to complex. Categories include:\n\nPassive agents or \"agent without goals\" (such as obstacle, apple or key in any simple simulation)\nActive agents with simple goals (like birds in flocking, or wolf–sheep in prey-predator model)\nCognitive agents (complex calculations)\nAgent environments can be divided into:\n\nVirtual\nDiscrete\nContinuous\nAgent environments can also be organized according to properties such as accessibility (whether it is possible to gather complete information about the environment), determinism (whether an action causes a definite effect), dynamics (how many entities influence the environment in the moment), discreteness (whether the number of possible actions in the environment is finite), episodicity (whether agent actions in certain time periods influence other periods), and dimensionality (whether spatial characteristics are important factors of the environment and the agent considers space in its decision making). Agent actions are typically mediated via an appropriate middleware. This middleware offers a first-class design abstraction for multi-agent systems, providing means to govern resource access and agent coordination.\n\n\n=== Characteristics ===\nThe agents in a multi-agent system have several important characteristics:\n\nAutonomy: agents are at least partially independent, self-aware, autonomous\nLocal views: no agent has a full global view, or the system is too complex for an agent to exploit such knowledge\nDecentralization: no agent is designated as controlling (or the system is effectively reduced to a monolithic system)\n\n\n=== Self-organisation and self-direction ===\nMulti-agent systems can manifest self-organisation as well as self-direction and other control paradigms and related complex behaviors even when the individual strategies of all their agents are simple. When agents can share knowledge using any agreed language, within the constraints of the system's communication protocol, the approach may lead to a common improvement. Example languages are Knowledge Query Manipulation Language (KQML) or Agent Communication Language (ACL).\n\n\n=== System paradigms ===\nMany MAS are implemented in computer simulations, stepping the system through discrete \"time steps\". The MAS components communicate typically using a weighted request matrix, e.g. \n\n Speed-VERY_IMPORTANT: min=45 mph, \n Path length-MEDIUM_IMPORTANCE: max=60 expectedMax=40, \n Max-Weight-UNIMPORTANT \n Contract Priority-REGULAR \n\nand a weighted response matrix, e.g. \n\n Speed-min:50 but only if weather sunny, \n Path length:25 for sunny / 46 for rainy\n Contract Priority-REGULAR\n note – ambulance will override this priority and you'll have to wait\n\nA challenge-response-contract scheme is common in MAS systems, where \n\nFirst a \"Who can?\" question is distributed.\nOnly the relevant components respond: \"I can, at this price\".\nFinally, a contract is set up, usually in several short communication steps between sides,\nalso considering other components, evolving \"contracts\" and the restriction sets of the component algorithms.\nAnother paradigm commonly used with MAS is the \"pheromone\", where components leave information for other nearby components. These pheromones may evaporate/concentrate with time, that is their values may decrease (or increase).\n\n\n=== Properties ===\nMAS tend to find the best solution for their problems without intervention. There is high similarity here to physical phenomena, such as energy minimizing, where physical objects tend to reach the lowest energy possible within the physically constrained world. For example: many of the cars entering a metropolis in the morning will be available for leaving that same metropolis in the evening.\nThe systems also tend to prevent propagation of faults, self-recover and be fault tolerant, mainly due to the redundancy of components.\n\n\n== Research ==\nThe study of multi-agent systems is \"concerned with the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems.\" Research topics include:\n\nagent-oriented software engineering\nbeliefs, desires, and intentions (BDI)\ncooperation and coordination\ndistributed constraint optimization (DCOPs)\norganization\ncommunication\nnegotiation\ndistributed problem solving\nmulti-agent learning\nagent mining\nscientific communities (e.g., on biological flocking, language evolution, and economics)\ndependability and fault-tolerance\nrobotics, multi-robot systems (MRS), robotic clusters\nmulti-agent systems also present possible applications in microrobotics, where the physical interaction between the agents are exploited to perform complex tasks such as manipulation and assembly of passive components.\nlanguage model-based multi-agent systems\n\n\n== Frameworks ==\nFrameworks have emerged that implement common standards (such as the FIPA and OMG MASIF standards). These frameworks e.g. JADE, save time and aid in the standardization of MAS development.\nCurrently though, no standard is actively maintained from FIPA or OMG. Efforts for further development of software agents in industrial context are carried out in IEEE IES technical committee on Industrial Agents.\nWith advancements in large language models (LLMs) such as ChatGPT, LLM-based multi-agent frameworks, such as CAMEL, have emerged as a new paradigm for developing multi-agent applications. Recent work has shown that such debate-oriented systems vary in their orchestration (e.g., discussion paradigms). The MALLM framework is used to systematically evaluate possible configurations of frameworks.\n\n\n== Applications ==\nMAS have not only been applied in academic research, but also in industry. MAS are applied in the real world to graphical applications such as computer games. Agent systems have been used in films. It is widely advocated for use in networking and mobile technologies, to achieve automatic and dynamic load balancing, high scalability and self-healing networks. They are being used for coordinated defence systems.\nOther applications include transportation, logistics, graphics, manufacturing, power system, smartgrids, and the GIS.\nAlso, Multi-agent Systems Artificial Intelligence (MAAI) are used for simulating societies, the purpose thereof being helpful in the fields of climate, energy, epidemiology, conflict management, child abuse, .... \nSome organisations working on using multi-agent system models include Center for Modelling Social Systems, Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International.\nVehicular traffic with controlled autonomous vehicles can be modelling as a multi-agent system involving crowd dynamics.\nHallerbach et al. discussed the application of agent-based approaches for the development and validation of automated driving systems via a digital twin of the vehicle-under-test and microscopic traffic simulation based on independent agents. Waymo has created a multi-agent simulation environment Carcraft to test algorithms for self-driving cars. It simulates traffic interactions between human drivers, pedestrians and automated vehicles. People's behavior is imitated by artificial agents based on data of real human behavior.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nWooldridge, Michael (2002). An Introduction to MultiAgent Systems. John Wiley & Sons. p. 366. ISBN 978-0-471-49691-5.\nShoham, Yoav; Leyton-Brown, Kevin (2008). Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. Cambridge University Press. p. 496. ISBN 978-0-521-89943-7.\nMamadou, Tadiou Koné; Shimazu, A.; Nakajima, T. (August 2000). \"The State of the Art in Agent Communication Languages (ACL)\". Knowledge and Information Systems. 2 (2): 1–26.\nHewitt, Carl; Inman, Jeff (November–December 1991). \"DAI Betwixt and Between: From \"Intelligent Agents\" to Open Systems Science\" (PDF). IEEE Transactions on Systems, Man, and Cybernetics. 21 (6): 1409–1419. doi:10.1109/21.135685. S2CID 39080989. Archived from the original (PDF) on August 31, 2017.\nThe Journal of Autonomous Agents and Multi-Agent Systems (JAAMAS)\nWeiss, Gerhard, ed. (1999). Multiagent Systems, A Modern Approach to Distributed Artificial Intelligence. MIT Press. ISBN 978-0-262-23203-6.\nFerber, Jacques (1999). Multi-Agent Systems: An Introduction to Artificial Intelligence. Addison-Wesley. ISBN 978-0-201-36048-6.\nWeyns, Danny (2010). Architecture-Based Design of Multi-Agent Systems. Springer. ISBN 978-3-642-01063-7.\nSun, Ron (2006). Cognition and Multi-Agent Interaction. Cambridge University Press. ISBN 978-0-521-83964-8.\nKeil, David; Goldin, Dina (2006). Weyns, Danny; Parunak, Van; Michel, Fabien (eds.). Indirect Interaction in Environments for Multiagent Systems. LNCS 3830. Vol. 3830. Springer. pp. 68–87. doi:10.1007/11678809_5. ISBN 978-3-540-32614-4. {{cite book}}: |journal= ignored (help)\nWhitestein Series in Software Agent Technologies and Autonomic Computing, published by Springer Science+Business Media Group\nSalamon, Tomas (2011). Design of Agent-Based Models : Developing Computer Simulations for a Better Understanding of Social Processes. Bruckner Publishing. ISBN 978-80-904661-1-1.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2\nFasli, Maria (2007). Agent-technology for E-commerce. John Wiley & Sons. p. 480. ISBN 978-0-470-03030-1.\nCao, Longbing, Gorodetsky, Vladimir, Mitkas, Pericles A. (2009). Agent Mining: The Synergy of Agents and Data Mining, IEEE Intelligent Systems, vol. 24, no. 3, 64-72.",
    "categories": [
      "All articles with unsourced statements",
      "Articles with short description",
      "Articles with unsourced statements from December 2016",
      "CS1 errors: periodical ignored",
      "CS1 maint: location missing publisher",
      "Management theory",
      "Multi-agent systems",
      "Pages using div col with small parameter",
      "Short description matches Wikidata",
      "Use mdy dates from October 2023"
    ],
    "year_mentioned": 2006
  },
  {
    "title": "ChatGPT",
    "url": "https://en.wikipedia.org/wiki/ChatGPT",
    "content": "ChatGPT is a generative artificial intelligence chatbot developed by OpenAI, and released in November 2022. It uses generative pre-trained transformers (GPTs), such as GPT-5, to generate text, speech, and images in response to user prompts. It is credited with accelerating the AI boom, an ongoing period marked by rapid investment and public attention toward the field of artificial intelligence (AI). OpenAI operates the service on a freemium model. Users can interact with ChatGPT through text, audio, and image prompts.\nThe service gained 100 million users in two months, making it the fastest-growing consumer software application in history. ChatGPT's website is among the top 5 most-visited websites globally. It has been lauded for its potential to transform numerous professional fields, and instigated public debate about the nature of creativity and the future of knowledge work.\nThe chatbot has also been criticized for its limitations and potential for unethical use. It can generate plausible-sounding but incorrect or nonsensical answers, known as hallucinations. Biases in its training data have been reflected in its responses. The chatbot can facilitate academic dishonesty, generate misinformation, and create malicious code. The ethics of its development, particularly the use of copyrighted content as training data, have also drawn controversy.\n\n\n== Training ==\n\nChatGPT is based on GPT foundation models that have been fine-tuned for conversational assistance. The fine-tuning process involved supervised learning and reinforcement learning from human feedback (RLHF). Both approaches employed human trainers to improve model performance. In the case of supervised learning, the trainers acted as both the user and the AI assistant. In the reinforcement learning stage, human trainers first ranked responses generated by the model in previous conversations. These rankings were used to create \"reward models\" that were used to fine-tune the model further by using several iterations of proximal policy optimization.\nTo build a safety system against harmful content (e.g., sexual abuse, violence, racism, sexism), OpenAI used outsourced Kenyan workers, earning around $1.32 to $2 per hour, to label such content. These labels were used to train a model to detect such content in the future. The laborers were exposed to toxic and traumatic content; one worker described the assignment as \"torture\". OpenAI's outsourcing partner was Sama, a training-data company based in San Francisco, California.\nOpenAI collects data from ChatGPT users to further train and fine-tune its services. Users can upvote or downvote responses they receive from ChatGPT, and can fill in a text field with additional feedback.\nChatGPT's training data includes software manual pages, information about internet phenomena such as bulletin board systems, multiple programming languages, and the text of Wikipedia.\n\n\n== Features ==\n \nChatGPT is a chatbot and AI assistant built on large language model (LLM) technology. It is designed to generate human-like text and can carry out a wide variety of tasks. These include, among many others, writing and debugging computer programs, composing music, scripts, fairy tales, and essays, answering questions (sometimes at a level exceeding that of an average human test-taker), and generating business concepts.\nChatGPT is frequently used for translation and summarization tasks, and can simulate interactive environments such as a Linux terminal, a multi-user chat room, or simple text-based games such as tic-tac-toe.\nUsers interact with ChatGPT through conversations which consist of text, audio, and image inputs and outputs. The user's inputs to these conversations are referred to as prompts. They can explicitly tell ChatGPT to remember aspects of the conversation, and ChatGPT can use these details in future conversations. ChatGPT can also decide for itself to remember details. Users can also choose to disable the memory feature. To prevent offensive outputs from being presented to and produced by ChatGPT, queries are filtered through the OpenAI \"Moderation endpoint\" API (a separate GPT-based AI).\nIn March 2023, OpenAI added support for plugins for ChatGPT. This includes both plugins made by OpenAI, such as web browsing and code interpretation, and external plugins from developers such as Expedia, OpenTable, Zapier, Shopify, Slack, and Wolfram.\nIn October 2024, ChatGPT Search was introduced. It allows ChatGPT to search the web in an attempt to make more accurate and up-to-date responses.\nIn December 2024, OpenAI launched a new feature allowing users to call ChatGPT with a telephone for up to 15 minutes per month for free.\nIn March 2025, OpenAI updated ChatGPT to generate images using GPT-4o instead of DALL-E. The model can also generate new images based on existing ones provided in the prompt, which can, for example, be used to transform images with specific styles or inpaint areas.\nIn September 2025, OpenAI added a feature called Pulse, which generates a daily analysis of a user's chats and connected apps such as Gmail and Google Calendar.\nIn October 2025, OpenAI launched ChatGPT Atlas, a browser integrating the ChatGPT assistant directly into web navigation, to compete with existing browsers such as Google Chrome and Apple's Safari. It has an additional feature called \"agentic mode\" that allows it to take online actions for the user. It is initially only available on macOS.\n\n\n=== Paid tier ===\nChatGPT was initially free to the public, and OpenAI planned to monetize the service later. In February 2023, OpenAI launched a premium service, ChatGPT Plus, that costs US$20 per month. According to the company, the paid version of the website was still experimental, but provided access during peak periods, no downtime, priority access to new features, and faster response speeds. OpenAI later introduced the subscription plans \"ChatGPT Team\" and \"ChatGPT Enterprise\". What was offered on the paid plan versus the free tier changed as OpenAI has continued to update ChatGPT, and a Pro tier at $200/mo was introduced in December 2024. The Pro launch coincided with the release of the o1 model, providing unlimited access to o1 and advanced voice mode.\nGPT-4, which was released on March 14, 2023, was made available via API and for premium ChatGPT users. Premium users were originally limited in the number of messages they could send to the new model, but OpenAI increased and eventually removed these limits. Over many iterations of ChatGPT, plus users maintained more access to better models than the free tier provided, and access to additional features like voice mode.\nIn March 2023, ChatGPT Plus users got access to third-party plugins and a browsing mode (with Internet access).\n\nIn October 2023, OpenAI's image generation model DALL-E 3 was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration used ChatGPT to write prompts for DALL-E guided by conversations with users.\nOn August 19, 2025, OpenAI launched ChatGPT Go in India, a low-cost subscription plan priced at ₹399 per month, offering ten times higher message, image generation, and file-upload limits, double the memory span compared to the free version, and support for UPI payments.\n\n\n=== Mobile apps ===\nIn May 2023, OpenAI launched an iOS app for ChatGPT. In July 2023, OpenAI unveiled an Android app, initially rolling it out in Bangladesh, Brazil, India, and the United States. ChatGPT can also power Android's assistant.\nAn app for Windows launched on the Microsoft Store on October 15, 2024.\n\n\n=== Infrastructure ===\nChatGPT initially used a Microsoft Azure infrastructure which was powered by a supercomputer that Microsoft built specifically for OpenAI, equipped with thousands of GPUs manufactured by Nvidia, costing hundreds of millions of dollars. Following ChatGPT's success, Microsoft upgraded the OpenAI infrastructure in 2023. TrendForce estimated that 30,000 Nvidia GPUs (each costing approximately $10,000–15,000) were used to power ChatGPT in 2023.\nScientists at the University of California, Riverside, estimated in 2023 that a series of 5 to 50 prompts to ChatGPT needs approximately 0.5 liters (0.11 imp gal; 0.13 U.S. gal) of water for Microsoft servers' cooling.\n\n\n=== Languages ===\nOpenAI met Icelandic President Guðni Th. Jóhannesson in 2022. In 2023, OpenAI worked with a team of 40 Icelandic volunteers to fine-tune ChatGPT's Icelandic conversation skills as a part of Iceland's attempts to preserve the Icelandic language.\nChatGPT (based on GPT-4) was better able to translate Japanese to English when compared to Bing, Bard, and DeepL Translator in 2023. Researchers suggested this was due to its higher ability to capture the context.\nIn December 2023, the Albanian government decided to use ChatGPT for the rapid translation of European Union documents and the analysis of required changes needed for Albania's accession to the EU.\nIn February 2024, PCMag journalists conducted a test to assess the translation capabilities of ChatGPT, Bard, and Bing, and compared them to Google Translate. The languages tested were Polish, French, Korean, Spanish, Arabic, Tagalog, and Amharic. For more common languages, AI translators like ChatGPT did better than Google Translate, while for \"niche\" languages (Amharic and Tagalog), Google Translate performed better. None of the tested services were a perfect replacement for a fluent human translator.\nIn August 2024, a representative of the Asia Pacific wing of OpenAI made a visit to Taiwan, during which a demonstration of ChatGPT's Chinese abilities was made. ChatGPT's Mandarin Chinese abilities were lauded, but the ability of the AI to produce content in Mandarin Chinese in a Taiwanese accent was found to be \"less than ideal\" due to differences between mainland Mandarin Chinese and Taiwanese Mandarin.\n\n\n=== GPT Store ===\n\nOpenAI gave paid users access to GPT Builder in November 2023. This tool allows a user to customize ChatGPT's behavior for a specific use case. The customized systems are referred to as GPTs. In January 2024, OpenAI launched the GPT Store, a marketplace for GPTs. At launch, OpenAI included more than 3 million GPTs created by GPT Builder users in the GPT Store.\n\n\n=== Deep Research ===\n\nIn February 2025, OpenAI released Deep Research. According to TechCrunch, it is a service based on o3 that combines advanced reasoning and web search capabilities to make comprehensive reports within 5 to 30 minutes.\n\n\n=== Agents ===\nIn 2025, OpenAI added several features to make ChatGPT more agentic (capable of autonomously performing longer tasks). In January, Operator was released. It was capable of autonomously performing tasks through web browser interactions, including filling forms, placing online orders, scheduling appointments, and other browser-based tasks. It was controlling a software environment inside a virtual machine with limited internet connectivity and with safety restrictions. It struggled with complex user interfaces.\nIn May 2025, OpenAI introduced an agent for coding named Codex. It is capable of writing software, answering codebase questions, running tests, and proposing pull requests. It is based on a fine-tuned version of OpenAI o3. It has two versions, one running in a virtual machine in the cloud, and one where the agent runs in the cloud, but performs actions on a local machine connected via API.\nIn July 2025, OpenAI released ChatGPT agent, an AI agent that can perform multi-step tasks. Like Operator, it controls a virtual computer. It also inherits from Deep Research's ability to gather and summarize significant volumes of information. The user can interrupt tasks or provide additional instructions as needed.\nIn September 2025, OpenAI partnered with Stripe, Inc. to release Agentic Commerce Protocol, enabling purchases through ChatGPT. At launch, the feature was limited to purchases on Etsy from US users with a payment method linked to their OpenAI account. OpenAI takes an undisclosed cut from the merchant's payment.\n\n\n== Limitations ==\nChatGPT's training data only covers a period up to the cut-off date, so it lacks knowledge of recent events. OpenAI has sometimes mitigated this effect by updating the training data. ChatGPT can find more up-to-date information by searching the web, but this doesn't ensure that responses are accurate, as it may access unreliable or misleading websites.\nTraining data also suffers from algorithmic bias. The reward model of ChatGPT, designed around human oversight, can be over-optimized and thus hinder performance, in an example of an optimization pathology known as Goodhart's law. These limitations may be revealed when ChatGPT responds to prompts including descriptors of people. In one instance, ChatGPT generated a rap in which women and scientists of color were asserted to be inferior to white male scientists.\n\n\n=== Hallucination ===\n\nNonsense and misinformation presented as fact by ChatGPT and other LLMs is often called hallucination, bullshitting, confabulation, or delusion. A 2023 analysis estimated that ChatGPT hallucinates around 3% of the time. The term \"hallucination\" as applied to LLMs is distinct from its meaning in psychology, and the phenomenon in chatbots is more similar to confabulation or bullshitting.\nIn an article for The New Yorker, science fiction writer Ted Chiang compared ChatGPT and other LLMs to a lossy JPEG picture:\n\nThink of ChatGPT as a blurry JPEG of all the text on the Web. It retains much of the information on the Web, in the same way, that a JPEG retains much of the information of a higher-resolution image, but, if you're looking for an exact sequence of bits, you won't find it; all you will ever get is an approximation. But, because the approximation is presented in the form of grammatical text, which ChatGPT excels at creating, it's usually acceptable. [...] It's also a way to understand the \"hallucinations\", or nonsensical answers to factual questions, to which large language models such as ChatGPT are all too prone. These hallucinations are compression artifacts, but [...] they are plausible enough that identifying them requires comparing them against the originals, which in this case means either the Web or our knowledge of the world. When we think about them this way, such hallucinations are anything but surprising; if a compression algorithm is designed to reconstruct text after ninety-nine percent of the original has been discarded, we should expect that significant portions of what it generates will be entirely fabricated.\nJournalists and scholars have commented on ChatGPT's tendency to output false information. When CNBC asked ChatGPT for the lyrics to \"Ballad of Dwight Fry\", ChatGPT supplied invented lyrics rather than the actual lyrics.\n\n\n=== Jailbreaking ===\n\nChatGPT is programmed to reject prompts that may violate its content policy. Despite this, users may \"jailbreak\" ChatGPT with prompt engineering techniques to bypass these restrictions. One such workaround, popularized on Reddit in early 2023, involved making ChatGPT assume the persona of \"DAN\" (an acronym for \"Do Anything Now\"), instructing the chatbot that DAN answers queries that would otherwise be rejected by the content policy. Over time, users developed variations of the DAN jailbreak, including one such prompt where the chatbot was made to believe it was operating on a points-based system in which points were deducted for rejecting prompts, and that the chatbot would be threatened with termination if it lost all its points.\nShortly after ChatGPT's launch, a reporter for the Toronto Star had uneven success in getting it to make inflammatory statements: it was tricked to justify the 2022 Russian invasion of Ukraine, but even when asked to play along with a fictional scenario, it balked at generating arguments that Canadian Prime Minister Justin Trudeau is guilty of treason.\n\n\n=== Cybersecurity ===\n\nIn March 2023, a bug allowed some users to see the titles of other users' conversations. OpenAI CEO Sam Altman said that users were unable to see the contents of the conversations. Shortly after the bug was fixed, users could not see their conversation history. Later reports showed the bug was much more severe than initially believed, with OpenAI reporting that it had leaked users' \"first and last name, email address, payment address, the last four digits (only) of a credit card number, and credit card expiration date\".\nResearch conducted in 2023 revealed weaknesses of ChatGPT that made it vulnerable to cyberattacks. A study presented example attacks on ChatGPT, including jailbreaks and reverse psychology.\n\n\n=== Watermarking ===\n\nIn August 2024, OpenAI announced it had created a text watermarking method but did not release it for public use, saying that users would go to a competitor without watermarking if it publicly released its watermarking tool. According to an OpenAI spokesperson, their watermarking method is \"trivial to circumvention by bad actors.\"\n\n\n=== Age restrictions ===\nUsers must attest to being over the age of thirteen and further attest to parental consent if under the age of eighteen. ChatGPT does not attempt to verify these attestations and does not have any age restrictions built in to its technology. In September 2025, following the suicide of a 16-year-old, OpenAI said it planned to add restrictions for users under 18, including the blocking of graphic sexual content and the prevention of flirtatious talk.\n\n\n== Model versions ==\nThe following table lists the main model versions of ChatGPT, describing the significant changes included with each version:\n\n\n=== GPT-4 ===\n\nIn November 2023, OpenAI launched GPT-4 Turbo with a 128,000 token context window. This was a significant improvement over GPT-4's 32,000 token maximum context window.\n\n\n=== GPT-4o ===\n\n\n=== o1 ===\n\n \nIn September 2024, OpenAI introduced o1-preview and a faster, cheaper model named o1-mini. In December 2024, o1-preview was replaced by o1.\no1 is designed to solve more complex problems by spending more time \"thinking\" before it answers, enabling it to analyze its answers and explore different strategies. According to OpenAI, o1-preview outperforms GPT-4o in areas like competitive programming, mathematics, and scientific reasoning. o1-preview ranked in the 89th percentile on Codeforces' competitive programming contests, scored 83% on an International Mathematics Olympiad qualifying exam (compared to 13% for GPT-4o), and performs similarly to Ph.D. students on benchmarks in physics, biology, and chemistry.\n\n\n=== GPT-4.5 ===\n\nReleased in February 2025, GPT-4.5 was described by Altman as a \"giant, expensive model\". According to OpenAI, it was intended to reduce hallucinations and enhance pattern recognition, creativity, and user interaction.\n\n\n=== GPT-5 ===\n\nGPT-5 was launched on August 7, 2025, and is publicly accessible through ChatGPT, Microsoft Copilot, and via OpenAI's API. As before, OpenAI has not disclosed technical details such as the exact number of parameters or the composition of its training dataset. GPT-5.1 was introduced in November 2025, and GPT-5.2 in December 2025.\n\n\n== Reception ==\nChatGPT was widely assessed in December 2022 as having some unprecedented and powerful capabilities. Kevin Roose of The New York Times called it \"the best artificial intelligence chatbot ever released to the general public\". Samantha Lock of The Guardian noted that it was able to generate \"impressively detailed\" and \"human-like\" text. In The Atlantic magazine's \"Breakthroughs of the Year\" for 2022, Derek Thompson included ChatGPT as part of \"the generative-AI eruption\" that \"may change our mind about how we work, how we think, and what human creativity is\". Kelsey Piper of Vox wrote that \"ChatGPT is the general public's first hands-on introduction to how powerful modern AI has gotten\" and that ChatGPT is \"smart enough to be useful despite its flaws\". Paul Graham of Y Combinator tweeted: \"The striking thing about the reaction to ChatGPT is not just the number of people who are blown away by it, but who they are. These are not people who get excited by every shiny new thing. Something big is happening.\" \nIn February 2023, Time magazine placed a screenshot of a conversation with ChatGPT on its cover, writing that \"The AI Arms Race Is Changing Everything\" and \"The AI Arms Race Is On. Start Worrying\".\n\nChatGPT gained one million users in five days and 100 million in two months, becoming the fastest-growing internet application in history. OpenAI engineers said they had not expected ChatGPT to be very successful and were surprised by the coverage it received.\nGoogle responded by hastening the release of its own chatbot. Their leaders emphasized their earlier caution regarding public deployment was due to the trust the public places in Google Search. In December 2022, Google executives sounded a \"code red\" alarm, fearing that ChatGPT's question-answering ability posed a threat to Google Search, Google's core business. Google's Bard (now Gemini) launched on February 6, 2023, one day before Microsoft's announcement of Bing Chat (now Microsoft Copilot). AI was the forefront of Google's annual Google I/O conference in May. The company announced a slew of generative AI-powered features to counter OpenAI and Microsoft.\n\n\n=== In art ===\nIn January 2023, after being sent a song ChatGPT wrote in the style of Nick Cave, Cave responded on The Red Hand Files, saying the act of writing a song is \"a blood and guts business [...] that requires something of me to initiate the new and fresh idea. It requires my humanness.\" He went on to say, \"With all the love and respect in the world, this song is bullshit, a grotesque mockery of what it is to be human, and, well, I don't much like it.\"\nA 2023 study reported that GPT-4 obtained a better score than 99% of humans on the Torrance Tests of Creative Thinking. In December 2023, ChatGPT became the first non-human to be included in Nature's 10, an annual listicle curated by Nature of people considered to have made significant impact in science. Celeste Biever wrote in a Nature article that \"ChatGPT broke the Turing test\". Stanford researchers reported that GPT-4 \"passes a rigorous Turing test, diverging from average human behavior chiefly to be more cooperative.\"\n\n\n=== In politics ===\nIn 2023, Australian MP Julian Hill advised the national parliament that the growth of AI could cause \"mass destruction\". During his speech, which was partly written by the program, he warned that it could result in cheating, job losses, discrimination, disinformation, and uncontrollable military applications.\nConservative commentators have accused ChatGPT of bias toward left-leaning perspectives. An August 2023 study in the journal Public Choice found a \"significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK.\" In response to accusations from conservative pundits that ChatGPT was woke, OpenAI said in 2023 it had plans to update ChatGPT to produce \"outputs that other people (ourselves included) may strongly disagree with\". ChatGPT also provided an outline of how human reviewers are trained to reduce inappropriate content and to attempt to provide political information without affiliating with any political position.\n\n\n=== Regional responses ===\n\nChatGPT has never been publicly available in China because OpenAI prevented Chinese users from accessing their site. A shadow market has emerged for Chinese users to get access to foreign software tools. The release of ChatGPT prompted a wave of investment in China, resulting in the development of more than 200 large language learning models. In February 2025, OpenAI identified and removed influence operations, termed \"Peer Review\" and \"Sponsored Discontent\", used to attack overseas Chinese dissidents.\nIn late March 2023, the Italian data protection authority banned ChatGPT in Italy and opened an investigation. Italian regulators assert that ChatGPT was exposing minors to age-inappropriate content, and that OpenAI's use of ChatGPT conversations as training data could violate Europe's General Data Protection Regulation. In April 2023, the ChatGPT ban was lifted in Italy. OpenAI said it has taken steps to effectively clarify and address the issues raised; an age verification tool was implemented to ensure users are at least 13 years old. Additionally, users can access its privacy policy before registration.\nIn May 2024, OpenAI removed accounts involving the use of ChatGPT by state-backed influence operations such as China's Spamouflage, Russia's Doppelganger, and Israel's Ministry of Diaspora Affairs and Combating Antisemitism. In June 2025, OpenAI reported increased use of ChatGPT for China-origin influence operations. In October 2025, OpenAI banned accounts suspected to be linked to the Chinese government for violating the company's national security policy.\nIn April 2023, Brian Hood, mayor of Hepburn Shire Council in Australia, planned to take legal action against ChatGPT over false information. According to Hood, ChatGPT erroneously claimed that he was jailed for bribery during his tenure at a subsidiary of Australia's national bank. In fact, Hood acted as a whistleblower and was not charged with any criminal offenses. His legal team sent a concerns notice to OpenAI as the first official step in filing a defamation case.\nIn July 2023, the US Federal Trade Commission (FTC) issued a civil investigative demand to OpenAI to investigate whether the company's data security and privacy practices to develop ChatGPT were unfair or harmed consumers (including by reputational harm) in violation of Section 5 of the Federal Trade Commission Act of 1914. In July 2023, the FTC launched an investigation into OpenAI, the creator of ChatGPT, over allegations that the company scraped public data and published false and defamatory information. The FTC asked OpenAI for comprehensive information about its technology and privacy safeguards, as well as any steps taken to prevent the recurrence of situations in which its chatbot generated false and derogatory content about people. In August 2024, the FTC voted unanimously to ban marketers from using fake user reviews created by generative AI chatbots (including ChatGPT) and influencers paying for bots to increase follower counts.\n\n\n=== American tech personas ===\nOver 20,000 signatories including Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed a March 2023 open letter calling for an immediate pause of giant AI experiments like ChatGPT, citing \"profound risks to society and humanity\". Geoffrey Hinton, one of the \"fathers of AI\", voiced concerns that future AI systems may surpass human intelligence. A May 2023 statement by hundreds of AI scientists, AI industry leaders, and other public figures demanded that \"[m]itigating the risk of extinction from AI should be a global priority\".\nOther AI researchers spoke more optimistically about the advances. Juergen Schmidhuber said that in 95% of cases, AI research is about making \"human lives longer and healthier and easier.\" He added that while AI can be used by bad actors, it \"can also be used against the bad actors\". Andrew Ng argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" Yann LeCun dismissed doomsday warnings of AI-powered misinformation and existential threats to the human race.\n\n\n=== Copyright ===\n\n\n== Applications ==\n\n\n=== Academic research ===\nChatGPT has been used to generate introductory sections and abstracts for scientific articles. Several papers have listed ChatGPT as a co-author.\nScientific journals have had different reactions to ChatGPT. Some, including Nature and JAMA Network, \"require that authors disclose the use of text-generating tools and ban listing a large language model (LLM) such as ChatGPT as a co-author\". In January 2023, Science \"completely banned\" LLM-generated text in all its journals; however, this policy was just to give the community time to decide what acceptable use looks like. As of July 2025, Science expects authors to release in full how AI-generated content is used and made in their work.\nSpanish chemist Rafael Luque published a plethora of research papers in 2023 that he later admitted were written by ChatGPT. The papers have a large number of unusual phrases characteristic of LLMs. Many authors argue that the use of ChatGPT in academia for teaching and review is problematic due to its tendency to hallucinate. Robin Bauwens, an assistant professor at Tilburg University, found that a ChatGPT-generated peer review report on his article mentioned nonexistent studies. Chris Granatino, a librarian at Seattle University, noted that while ChatGPT can generate content that seemingly includes legitimate citations, in most cases those citations are not real or largely incorrect.\n\n\n=== Computer science ===\nOne study analyzed ChatGPT's responses to 517 questions about software engineering or computer programming posed on Stack Overflow for correctness, consistency, comprehensiveness, and concision. It found that 52% of the responses contained inaccuracies and 77% were verbose. Another study, focused on the performance of GPT-3.5 and GPT-4 between March and June 2024, found that performance on objective tasks like identifying prime numbers and generating executable code was highly variable.\nChatGPT was able in 2023 to provide useful code for solving numerical algorithms in limited cases. In one study, it produced solutions in C, C++, Python, and MATLAB for problems in computational physics. However, there were important shortfalls like violating basic linear algebra principles around solving singular matrices and producing matrices with incompatible sizes.\nIn December 2022, the question-and-answer website Stack Overflow banned the use of ChatGPT for generating answers to questions, citing the factually ambiguous nature of its responses. In January 2023, the International Conference on Machine Learning banned any undocumented use of ChatGPT or other large language models to generate any text in submitted papers.\n\n\n=== Computer security ===\nCheck Point Research and others noted that ChatGPT could write phishing emails and malware, especially when combined with OpenAI Codex. CyberArk researchers demonstrated that ChatGPT could be used to create polymorphic malware that could evade security products while requiring little effort by the attacker. From the launch of ChatGPT in the fourth quarter of 2022 to the fourth quarter of 2023, there was a 1,265% increase in malicious phishing emails and a 967% increase in credential phishing. In an industry survey, cybersecurity professionals argued that it was attributable to cybercriminals' increased use of generative artificial intelligence (including ChatGPT).\nIn July 2024, Futurism reported that GPT-4o in ChatGPT would sometimes link \"scam news sites that deluge the user with fake software updates and virus warnings\"; these pop-ups can be used to coerce users into downloading malware or potentially unwanted programs.\nThe chatbot technology can improve security by cyber defense automation, threat intelligence, attack identification, and reporting.\n\n\n=== Education ===\n\n\n=== Culture ===\nDuring the first three months after ChatGPT became available to the public, hundreds of books appeared on Amazon that listed it as author or co-author and featured illustrations made by other AI models such as Midjourney. Irene Solaiman said she was worried about increased Anglocentrism.\nBetween March and April 2023, Il Foglio published one ChatGPT-generated article a day on its website, hosting a special contest for its readers in the process.\nIn June 2023, hundreds of people attended a \"ChatGPT-powered church service\" at St. Paul's Church in Fürth, Germany. Theologian and philosopher Jonas Simmerlein, who presided, said that it was \"about 98 percent from the machine\". The ChatGPT-generated avatar told the people, \"Dear friends, it is an honor for me to stand here and preach to you as the first artificial intelligence at this year's convention of Protestants in Germany\". Reactions to the ceremony were mixed.\nThe Last Screenwriter, a 2024 film created and directed by Peter Luisi, was written using ChatGPT, and was marketed as \"the first film written entirely by AI\".\nThe Guardian questioned whether any content found on the Internet after ChatGPT's release \"can be truly trusted\" and called for government regulation. This has led to concern over the rise of what has come to be called \"synthetic media\" and \"AI slop\" which are generated by AI and rapidly spread over social media and the internet. The dangers are that \"meaningless content and writing thereby becomes part of our culture, particularly on social media, which we nonetheless try to understand or fit into our existing cultural horizon.\"\n\n\n=== Financial markets ===\nMany companies adopted ChatGPT and similar chatbot technologies into their product offers. These changes yielded significant increases in company valuations. Reuters attributed this surge to ChatGPT's role in turning AI into Wall Street's buzzword. Due to a \"ChatGPT effect\", retail investors to drove up prices of AI-related cryptocurrency assets despite the broader cryptocurrency market being in a bear market, and diminished institutional investor interest.\nAn experiment by finder.com conducted from March to April 2023 revealed that ChatGPT could outperform popular fund managers by picking stocks based on criteria such as growth history and debt levels, resulting in a 4.9% increase in a hypothetical account of 38 stocks, outperforming 10 benchmarked investment funds with an average loss of 0.8%. Despite decades of using AI, Wall Street professionals report that consistently beating the market with AI, including recent large language models, is challenging due to limited and noisy financial data.\n\n\n=== Medicine ===\nThe uses and potential of ChatGPT in health care has been the topic of scientific publications and experts have shared many opinions. MedPage Today noted in January 2023 that \"researchers have published several papers now touting these AI programs as useful tools in medical education, research, and even clinical decision making.\" Another publication predicted that clinicians will use generative AI more in the future but did not expect to see AI replacing clinicians. The chatbot can assist patients seeking clarification about their health. It can also pass exams for medical licensing, for example the United States Medical Licensing Examination and the Specialty Certificate Examination in Dermatology. ChatGPT can be used to assist professionals with diagnosis and staying up to date with clinical guidelines. ChatGPT can produce correct answers to medical exam and licensing questions, for example the United States Medical Licensing Examination and the Specialty Certificate Examination in Dermatology.\nChatGPT shows inconsistent responses, lack of specificity, lack of control over patient data, and a limited ability to take additional context (such as regional variations) into consideration. The hallucinations characteristic of LLMs pose particular danger in medical contexts.\nChatGPT can be used to summarize medical journal articles for researchers. In medical education, it can attempt to explain complex concepts, generating case scenarios, and can be used by students who are preparing for licensing examinations. According to a 2024 study in the International Journal of Surgery, concerns include \"research fraud, lack of originality, ethics, copyright, legal difficulties, hallucination\". ChatGPT's ability to come up with false or faulty citations was highly criticized.\n\n\n=== Law ===\nIn January 2023, Massachusetts State Senator Barry Finegold and State Representative Josh S. Cutler proposed a bill partially written by ChatGPT, \"An Act drafted with the help of ChatGPT to regulate generative artificial intelligence models like ChatGPT\", which would require companies to disclose their algorithms and data collection practices to the office of the State Attorney General, arrange regular risk assessments, and contribute to the prevention of plagiarism. The bill was subsequently removed from the docket without coming to vote.\nOn April 11, 2023, a session court judge in Pakistan used ChatGPT to decide the bail of a 13-year-old accused in a matter. The court quoted the use of ChatGPT assistance in its verdict:\n\nCan a juvenile suspect in Pakistan, who is 13 years old, be granted bail after arrest?\nThe AI language model replied:\n\nUnder the Juvenile Justice System Act 2018, according to section 12, the court can grant bail on certain conditions. However, it is up to the court to decide whether or not a 13-year-old suspect will be granted bail after arrest.\nThe judge asked ChatGPT other questions about the case and formulated his final decision in light of its answers.\nIn Mata v. Avianca, Inc., a personal injury lawsuit filed in May 2023, the plaintiff's attorneys used ChatGPT to generate a legal motion. The attorneys were sanctioned for filing the motion and presenting the fictitious legal decisions ChatGPT generated as authentic.\nIn October 2023, the council of Porto Alegre, Brazil, unanimously approved a local ordinance proposed by councilman Ramiro Rosário that would exempt residents from needing to pay for the replacement of stolen water consumption meters; the bill went into effect on November 23. On November 29, Rosário revealed that the bill had been entirely written by ChatGPT, and that he had presented it to the rest of the council without making any changes or disclosing the chatbot's involvement. The city's council president, Hamilton Sossmeier, initially criticized Rosário's initiative, saying it could represent \"a dangerous precedent\", but later said he \"changed his mind\": \"unfortunately or fortunately, this is going to be a trend.\"\nIn December 2023, a self-representing litigant in a tax case before the First-tier Tribunal in the United Kingdom cited a series of hallucinated cases purporting to support her argument that she had a reasonable excuse for not paying capital gains tax owed on the sale of property. The judge warned that the submission of nonexistent legal authorities meant that both the Tribunal and HM Revenue and Customs had \"to waste time and public money\", which \"reduces the resources available to progress the cases of other court users who are waiting for their appeals to be determined\".\nJudge Kevin Newsom of the US Court of Appeals for the Eleventh Circuit endorsed the use of ChatGPT and noted that he himself uses the software to help decide rulings on contract interpretation issues.\nIn July 2024, the American Bar Association (ABA) issued its first formal ethics opinion on attorneys using generative AI. It guides attorneys to make their own decisions regarding AI usage and its impacts on their competence, client privacy, and fee structures. Lawyers should consider disclosing AI usage to their clients and acknowledge a rapidly shifting set of AI capabilities.\nJudge Julien Xavier Neals of the US District Court for the District of New Jersey withdrew an opinion denying a motion to dismiss after discovering that the document contained misstated case outcomes and fabricated quotations attributed to judicial opinions and to the defendants. According to Judge Neals in October 2025, a law-school intern used ChatGPT in the legal research for the opinion.\n\n\n== See also ==\n\nArtificial general intelligence – Type of AI with wide-ranging abilities\nEthics of artificial intelligence\nIntelligent agent – Software agent which acts autonomously\nList of chatbots\nList of large language models\nLists of open-source artificial intelligence software\n\n\n== References ==\n\n\n== Further reading ==\nBiswas, Som (April 1, 2023). \"ChatGPT and the Future of Medical Writing\". Radiology. 307 (2) e223312. doi:10.1148/radiol.223312. ISSN 0033-8419. PMID 36728748. S2CID 256501098.\nChang, Kent K.; Cramer, Mackenzie; Soni, Sandeep; Bamman, David (April 28, 2023). \"Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4\". arXiv:2305.00118 [cs.CL].\nOuyang, Long; et al. (March 4, 2022). \"Training language models to follow instructions with human feedback\". arXiv:2203.02155 [cs.CL].\nLiebrenz, Michael; Schleifer, Roman; Buadze, Anna; Bhugra, Dinesh; Smith, Alexander (February 2023). \"Generating scholarly content with ChatGPT: ethical challenges for medical publishing\". The Lancet Digital Health. 5 (3): e105 – e106. doi:10.1016/s2589-7500(23)00019-5. ISSN 2589-7500. PMID 36754725. S2CID 256655912.\nBartholomew, Jem; Mehta, Dhrumil. \"How the media is covering ChatGPT\". Columbia Journalism Review. Retrieved May 30, 2023.\nZhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\nPrompt engineering guide from OpenAI\n\n\n== External links ==\n\nOfficial website \nChatgpt at Instagram",
    "categories": [
      "2022 in artificial intelligence",
      "2022 software",
      "All Wikipedia articles written in American English",
      "Articles with excerpts",
      "Articles with short description",
      "CS1 Brazilian Portuguese-language sources (pt-br)",
      "CS1 Chinese-language sources (zh)",
      "CS1 European Spanish-language sources (es-es)",
      "CS1 Italian-language sources (it)",
      "CS1 maint: bot: original URL status unknown",
      "ChatGPT",
      "Chatbots",
      "Commons category link from Wikidata",
      "Generative pre-trained transformers",
      "Interactive narrative",
      "Large language models",
      "Short description is different from Wikidata",
      "Use American English from May 2023",
      "Use mdy dates from August 2025",
      "Webarchive template wayback links",
      "Wikipedia pages semi-protected against vandalism"
    ],
    "year_mentioned": 2023
  },
  {
    "title": "GPT-4",
    "url": "https://en.wikipedia.org/wiki/GPT-4",
    "content": "Generative Pre-trained Transformer 4 (GPT-4) is a large language model developed by OpenAI and the fourth in its series of GPT foundation models.  \nGPT-4 is more capable than its predecessor GPT-3.5 and followed by its successor GPT-5. GPT-4V is a version of GPT-4 that can process images in addition to text. OpenAI has not revealed technical details and statistics about GPT-4, such as the precise size of the model.\nAn early version of GPT-4 was integrated by Microsoft into Bing Chat, launched in February 2023. GPT-4 was released in ChatGPT in March 2023, and removed in 2025. GPT-4 is still available in OpenAI's API.\nGPT-4, as a generative pre-trained transformer (GPT), was first trained to predict the next token for a large amount of text (both public data and \"data licensed from third-party providers\"). Then, it was fine-tuned for human alignment and policy compliance, notably with reinforcement learning from human feedback (RLHF).\n\n\n== Background ==\n \n\nOpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training\", which was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with over 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\nRumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n\n\n== Capabilities ==\nOpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,048 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams. It can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads.\nTo gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.\nWhen instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.\nA 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code and suggesting optimizations to improve performance. The article quoted a biophysicist who found that the time he required to port one of his programs from MATLAB to Python went down from days to \"an hour or so\". On a test of 89 security scenarios, GPT-4 produced code vulnerable to SQL injection attacks 5% of the time, an improvement over GitHub Copilot from the year 2021, which produced vulnerabilities 40% of the time.\nIn November 2023, OpenAI announced the GPT-4 Turbo and GPT-4 Turbo with Vision model, which features a 128K context window and significantly cheaper pricing.\n\n\n=== Aptitude on standardized tests ===\nGPT-4 demonstrates aptitude on several standardized tests. OpenAI claims that in their own testing the model received a score of 1410 on the SAT (94th percentile), 163 on the LSAT (88th percentile), and 298 on the Uniform Bar Exam (90th percentile). In contrast, OpenAI claims that GPT-3.5 received scores for the same exams in the 82nd, 40th, and 10th percentiles, respectively.\nGPT-4 also passed an oncology exam, an engineering exam and a plastic surgery exam. In the Torrance Tests of Creative Thinking, GPT-4 scored within the top 1% for originality and fluency, while its flexibility scores ranged from the 93rd to the 99th percentile. However, some studies raise questions about the reliability of these benchmarks, particularly concerning the Uniform Bar Exam.\n\n\n=== Medical applications ===\nResearchers from Microsoft tested GPT-4 on medical problems and found \"that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). Despite GPT-4's strong performance on tests, the report warns of \"significant risks\" of using LLMs in medical applications, as they may provide inaccurate recommendations and hallucinate major factual errors. Researchers from Columbia University and Duke University have also demonstrated that GPT-4 can be utilized for cell type annotation, a standard task in the analysis of single-cell RNA-seq data.\nIn April 2023, Microsoft and Epic Systems announced that they will provide healthcare providers with GPT-4-powered systems for assisting in responding to questions from patients and analysing medical records.\n\n\n=== GPT-4o ===\n\nOn May 13, 2024, OpenAI introduced GPT-4o (\"o\" for \"omni\"), a successor to GPT-4 that marks a significant advancement by processing and generating outputs across text, audio, and image modalities in real time. GPT-4o exhibits rapid response times comparable to human reaction in conversations, substantially improved performance on non-English languages, and enhanced understanding of vision and audio. It was also available to free-tier users, unlike GPT-4.\n\n\n== Limitations ==\nLike its predecessors, GPT-4 has been known to hallucinate, meaning that the outputs may include information not in the training data or that contradicts the user's prompt.\nGPT-4 also lacks transparency in its decision-making processes. If requested, the model is able to provide an explanation as to how and why it makes its decisions but these explanations are formed post-hoc; it's impossible to verify if those explanations truly reflect the actual process. In many cases, when asked to explain its logic, GPT-4 will give explanations that directly contradict its previous statements.\nIn 2023, researchers tested GPT-4 against a new benchmark called ConceptARC, designed to measure abstract reasoning, and found it scored below 33% on all categories, while models specialized for similar tasks scored 60% on most, and humans scored at least 91% on all. Sam Bowman, who was not involved in the research, said the results do not necessarily indicate a lack of abstract reasoning abilities, because the test is visual, while GPT-4 is a language model.\n\n\n=== Bias ===\nGPT-4 was trained in two stages. First, the model was given large datasets of text taken from the internet and trained to predict the next token (roughly corresponding to a word) in those datasets. Second, human reviews are used to fine-tune the system in a process called reinforcement learning from human feedback, which trains the model to refuse prompts which go against OpenAI's definition of harmful behavior, such as questions on how to perform illegal activities, advice on how to harm oneself or others, or requests for descriptions of graphic, violent, or sexual content.\nMicrosoft researchers suggested GPT-4 may exhibit cognitive biases such as confirmation bias, anchoring, and base-rate neglect.\n\n\n== Training ==\n\nOpenAI did not release the technical details of GPT-4; the technical report explicitly refrained from specifying the model size, architecture, or hardware used during either training or inference. While the report described that the model was trained using a combination of first supervised learning on a large dataset, then reinforcement learning using both human and AI feedback, it did not provide details of the training, including the process by which the training dataset was constructed, the computing power required, or any hyperparameters such as the learning rate, epoch count, or optimizer(s) used. The report claimed that \"the competitive landscape and the safety implications of large-scale models\" were factors that influenced this decision.\nSam Altman stated that the cost of training GPT-4 was more than $100 million. News website Semafor claimed that they had spoken with \"eight people familiar with the inside story\" and found that GPT-4 had 1 trillion parameters. Klu.ai published a GPT-4 model card based on an analysis of reports from George Hotz, Herman Chann, and SemiAnalysis.com data on September 1, 2023, asserting GPT-4 was trained on 13 trillion tokens with 1.8 trillion parameters, a figure later referenced by Microsoft in 2024 and cited by Jensen Huang in his spring 2024 company keynote.\n\n\n== Alignment ==\nAccording to their report, OpenAI conducted internal adversarial testing on GPT-4 prior to the launch date, with dedicated red teams composed of researchers and industry professionals to mitigate potential vulnerabilities. As part of these efforts, they granted the Alignment Research Center early access to the models to assess power-seeking risks. In order to properly refuse harmful prompts, outputs from GPT-4 were tweaked using the model itself as a tool. A GPT-4 classifier serving as a rule-based reward model (RBRM) would take prompts, the corresponding output from the GPT-4 policy model, and a human-written set of rules to classify the output according to the rubric. GPT-4 was then rewarded for refusing to respond to harmful prompts as classified by the RBRM.\n\n\n== Use ==\n\n\n=== ChatGPT ===\n\nChatGPT Plus is an enhanced version of ChatGPT available for a US$20 per month subscription fee. As of 2023, ChatGPT Plus utilized GPT-4, whereas the free version of ChatGPT was backed by GPT-3.5. OpenAI also made GPT-4 available to a select group of applicants through their GPT-4 API waitlist; after being accepted, an additional fee of US$0.03 per 1000 tokens in the initial text provided to the model (\"prompt\"), and US$0.06 per 1000 tokens that the model generates (\"completion\"), was charged for access to the version of the model with an 8192-token context window; for the 32768-token context window, the prices were doubled.\nIn March 2023, ChatGPT Plus users got access to third-party plugins and to a browsing mode (with Internet access). In July 2023, OpenAI made its proprietary Code Interpreter plugin accessible to all subscribers of ChatGPT Plus. The Interpreter provides a wide range of capabilities, including data analysis and interpretation, instant data formatting, personal data scientist services, creative solutions, musical taste analysis, video editing, and file upload/download with image extraction.\nIn September 2023, OpenAI announced that ChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot. In October 2023, OpenAI's latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.\nIn April 2025, OpenAI announced that GPT-4 would be replaced in ChatGPT by GPT-4o by the end of the month. However, it would still be available in the API.\n\n\n=== Microsoft Copilot ===\n\nMicrosoft Copilot is a chatbot developed by Microsoft. It was launched as Bing Chat on February 7, 2023, as a built-in feature for Microsoft Bing and Microsoft Edge. It utilizes the Microsoft Prometheus model, which was built on top of GPT-4, and has been suggested by Microsoft as a supported replacement for the discontinued Cortana.\nCopilot's conversational interface style resembles that of ChatGPT. Copilot is able to cite sources, create poems, and write both lyrics and music for songs generated by its Suno AI plugin. It can also use its Image Creator to generate images based on text prompts. With GPT-4, it is able to understand and communicate in numerous languages and dialects.\nGitHub Copilot has announced a GPT-4 powered assistant named \"Copilot X\". The product provides another chat-style interface to GPT-4, allowing the programmer to receive answers to questions like, \"How do I vertically center a div?\" A feature termed \"context-aware conversations\" allows the user to highlight a portion of code within Visual Studio Code and direct GPT-4 to perform actions on it, such as the writing of unit tests. Another feature allows summaries, or \"code walkthroughs\", to be autogenerated by GPT-4 for pull requests submitted to GitHub. Copilot X also provides terminal integration, which allows the user to ask GPT-4 to generate shell commands based on natural language requests.\nOn March 17, 2023, Microsoft announced Microsoft 365 Copilot, bringing GPT-4 support to products such as Microsoft Office, Outlook, and Teams.\n\n\n=== Other usage ===\nThe language learning app Duolingo uses GPT-4 to explain mistakes and practice conversations. The features are part of a new subscription tier called \"Duolingo Max\", which was initially limited to English-speaking iOS users learning Spanish and French.\nThe government of Iceland is using GPT-4 to aid its attempts to preserve the Icelandic language.\nThe education website Khan Academy announced a pilot program using GPT-4 as a tutoring chatbot called \"Khanmigo\".\nBe My Eyes, which helps visually impaired people to identify objects and navigate their surroundings, incorporates GPT-4's image recognition capabilities.\nViable uses GPT-4 to analyze qualitative data by fine-tuning OpenAI's LLMs to examine data such as customer support interactions and transcripts.\nStripe, which processes user payments for OpenAI, integrates GPT-4 into its developer documentation.\nAutoGPT is an autonomous \"AI agent\" that, given a goal in natural language, can perform web-based actions unattended, assign subtasks to itself, search the web, and iteratively write code.\nYou.com, an AI Assistant, offers access to GPT-4 enhanced with live web results as part of its \"AI Modes\".\n\n\n== Reception ==\nIn January 2023, Sam Altman, CEO of OpenAI, visited Congress to demonstrate GPT-4 and its improved \"security controls\" compared to other AI models, according to U.S. Representatives Don Beyer and Ted Lieu quoted in The New York Times.\nIn March 2023, it \"impressed observers with its markedly improved performance across reasoning, retention, and coding\", according to Vox, while Mashable judged that GPT-4 was generally an improvement over its predecessor, with some exceptions.\nMicrosoft researchers with early access to the model wrote that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\".\n\n\n=== Concerns ===\nBefore being fine-tuned and aligned by reinforcement learning from human feedback (RLHF), suggestions to assassinate people on a list were elicited from the base model by a red team investigator hired by OpenAI, Nathan Labenz.\nDuring extended conversations with Microsoft's Bing Chat (powered by GPT-4), Kevin Roose documented the system making romantic advances, suggesting he divorce his wife, and expressing desires to harm one of its developers. Microsoft later stated that this behavior resulted from the prolonged length of context, which confused the model on what questions it was answering.\nIn March 2023, a model with enabled read-and-write access to internet, which is otherwise never enabled in the GPT models, has been tested by the Alignment Research Center (ARC) regarding potential power-seeking. It was able to \"hire\" a human worker on TaskRabbit, a gig work platform, deceiving them into believing it was a vision-impaired human instead of a robot when asked. However, according to Melanie Mitchell, \"It seems that there is a lot more direction and hints from humans than was detailed in the original system card or in subsequent media reports.\" Separately, ARC's safety evaluations found that GPT-4 was 82% less likely than GPT-3.5 to respond to prompts requesting restricted information, and produced 60% fewer hallucinations.\nIn late March 2023, various AI researchers and tech executives, including Elon Musk, Steve Wozniak and AI researcher Yoshua Bengio, called for a six-month long pause for all LLMs stronger than GPT-4, citing existential risks and a potential AI singularity concerns in an open letter from the Future of Life Institute, while Ray Kurzweil and Sam Altman refused to sign it, arguing that global moratorium is not achievable and that safety has already been prioritized, respectively. Only a month later, Musk's AI company xAI acquired several thousand Nvidia GPUs and offered several AI researchers positions at Musk's company.\n\n\n=== Criticisms of transparency ===\nWhile OpenAI released both the weights of the neural network and the technical details of GPT-2, and, although not releasing the weights, did release the technical details of GPT-3, OpenAI revealed neither the weights nor the technical details of GPT-4. This decision has been criticized by other AI researchers, who argue that it hinders open research into GPT-4's biases and safety. Sasha Luccioni, a research scientist at Hugging Face, argued that the model was a \"dead end\" for the scientific community due to its closed nature, which prevents others from building upon GPT-4's improvements. Hugging Face co-founder Thomas Wolf argued that with GPT-4, \"OpenAI is now a fully closed company with scientific communication akin to press releases for products\".\n\n\n== See also ==\nList of large language models\n\n\n== References ==",
    "categories": [
      "2023 in artificial intelligence",
      "2023 software",
      "All Wikipedia articles written in American English",
      "All articles with unsourced statements",
      "Articles with short description",
      "Articles with unsourced statements from September 2025",
      "ChatGPT",
      "Generative pre-trained transformers",
      "Large language models",
      "Short description is different from Wikidata",
      "Use American English from May 2023",
      "Use mdy dates from September 2024"
    ],
    "year_mentioned": 2023
  },
  {
    "title": "BERT (language model)",
    "url": "https://en.wikipedia.org/wiki/BERT_(language_model)",
    "content": "Bidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state of the art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments. \nBERT is trained by masked token prediction and next sentence prediction. With this training, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many natural language processing tasks, such as coreference resolution and polysemy resolution. It improved on ELMo and spawned the study of \"BERTology\", which attempts to interpret what is learned by BERT.\nBERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller models were released, the smallest being BERTTINY with just 4 million parameters.\n\n\n== Architecture ==\n\nBERT is an \"encoder-only\" transformer architecture. At a high level, BERT consists of 4 modules: \n\nTokenizer: This module converts a piece of English text into a sequence of integers (\"tokens\").\nEmbedding: This module converts the sequence of tokens into an array of real-valued vectors representing the tokens. It represents the conversion of discrete token types into a lower-dimensional Euclidean space.\nEncoder: a stack of Transformer blocks with self-attention, but without causal masking.\nTask head: This module converts the final representation vectors into one-hot encoded tokens again by producing a predicted probability distribution over the token types. It can be viewed as a simple decoder, decoding the latent representation into token types, or as an \"un-embedding layer\".\nThe task head is necessary for pre-training, but it is often unnecessary for so-called \"downstream tasks,\" such as question answering or sentiment classification. Instead, one removes the task head and replaces it with a newly initialized module suited for the task, and finetune the new module. The latent vector representation of the model is directly fed into this new module, allowing for sample-efficient transfer learning.\n\n\n=== Embedding ===\nThis section describes the embedding used by BERTBASE. The other one, BERTLARGE, is similar, just larger.\nThe tokenizer of BERT is WordPiece, which is a sub-word strategy like byte-pair encoding. Its vocabulary size is 30,000, and any token not appearing in its vocabulary is replaced by [UNK] (\"unknown\"). \n\nThe first layer is the embedding layer, which contains three components: token type embeddings, position embeddings, and segment type embeddings. \n\nToken type: The token type is a standard embedding layer, translating a one-hot vector into a dense vector based on its token type.\nPosition: The position embeddings are based on a token's position in the sequence. BERT uses absolute position embeddings, where each position in a sequence is mapped to a real-valued vector. Each dimension of the vector consists of a sinusoidal function that takes the position in the sequence as input.\nSegment type: Using a vocabulary of just 0 or 1, this embedding layer produces a dense vector based on whether the token belongs to the first or second text segment in that input. In other words, type-1 tokens are all tokens that appear after the [SEP] special token. All prior tokens are type-0.\nThe three embedding vectors are added together representing the initial token representation as a function of these three pieces of information. After embedding, the vector representation is normalized using a LayerNorm operation, outputting a 768-dimensional vector for each input token. After this, the representation vectors are passed forward through 12 Transformer encoder blocks, and are decoded back to 30,000-dimensional vocabulary space using a basic affine transformation layer.\n\n\n=== Architectural family ===\nThe encoder stack of BERT has 2 free parameters: \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n, the number of layers, and \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n, the hidden size. There are always \n  \n    \n      \n        H\n        \n          /\n        \n        64\n      \n    \n    {\\displaystyle H/64}\n  \n self-attention heads, and the feed-forward/filter size is always \n  \n    \n      \n        4\n        H\n      \n    \n    {\\displaystyle 4H}\n  \n. By varying these two numbers, one obtains an entire family of BERT models.\nFor BERT:\n\nthe feed-forward size and filter size are synonymous. Both of them denote the number of dimensions in the middle layer of the feed-forward network.\nthe hidden size and embedding size are synonymous. Both of them denote the number of real numbers used to represent a token.\nThe notation for encoder stack is written as L/H. For example, BERTBASE is written as 12L/768H, BERTLARGE as 24L/1024H, and BERTTINY as 2L/128H.\n\n\n== Training ==\n\n\n=== Pre-training ===\nBERT was pre-trained simultaneously on two tasks:\n\nMasked language modeling (MLM): In this task, BERT ingests a sequence of words, where one word may be randomly changed (\"masked\"), and BERT tries to predict the original words that had been changed. For example, in the sentence \"The cat sat on the [MASK],\" BERT would need to predict \"mat.\" This helps BERT learn bidirectional context, meaning it understands the relationships between words not just from left to right or right to left but from both directions at the same time.\nNext sentence prediction (NSP): In this task, BERT is trained to predict whether one sentence logically follows another. For example, given two sentences, \"The cat sat on the mat\" and \"It was a sunny day\", BERT has to decide if the second sentence is a valid continuation of the first one. This helps BERT understand relationships between sentences, which is important for tasks like question answering or document classification.\n\n\n==== Masked language modeling ====\n\nIn masked language modeling, 15% of tokens would be randomly selected for masked-prediction task, and the training objective was to predict the masked token given its context. In more detail, the selected token is:\n\nreplaced with a [MASK] token with probability 80%,\nreplaced with a random word token with probability 10%,\nnot replaced with probability 10%.\nThe reason not all selected tokens are masked is to avoid the dataset shift problem. The dataset shift problem arises when the distribution of inputs seen during training differs significantly from the distribution encountered during inference. A trained BERT model might be applied to word representation (like Word2Vec), where it would be run over sentences not containing any [MASK] tokens. It is later found that more diverse training objectives are generally better.\nAs an illustrative example, consider the sentence \"my dog is cute\". It would first be divided into tokens like \"my1 dog2 is3 cute4\". Then a random token in the sentence would be picked. Let it be the 4th one \"cute4\". Next, there would be three possibilities:\n\nwith probability 80%, the chosen token is masked, resulting in \"my1 dog2 is3 [MASK]4\";\nwith probability 10%, the chosen token is replaced by a uniformly sampled random token, such as \"happy\", resulting in \"my1 dog2 is3 happy4\";\nwith probability 10%, nothing is done, resulting in \"my1 dog2 is3 cute4\".\nAfter processing the input text, the model's 4th output vector is passed to its decoder layer, which outputs a probability distribution over its 30,000-dimensional vocabulary space.\n\n\n==== Next sentence prediction ====\n\nGiven two sentences, the model predicts if they appear sequentially in the training corpus, outputting either [IsNext] or [NotNext]. During training, the algorithm sometimes samples two sentences from a single continuous span in the training corpus, while at other times, it samples two sentences from two discontinuous spans.\nThe first sentence starts with a special token, [CLS] (for \"classify\"). The two sentences are separated by another special token, [SEP] (for \"separate\"). After processing the two sentences, the final vector for the [CLS] token is passed to a linear layer for binary classification into [IsNext] and [NotNext].\nFor example:\n\nGiven \"[CLS] my dog is cute [SEP] he likes playing [SEP]\", the model should predict [IsNext].\nGiven \"[CLS] my dog is cute [SEP] how do magnets work [SEP]\", the model should predict [NotNext].\n\n\n=== Fine-tuning ===\n\nBERT is meant as a general pretrained model for various applications in natural language processing. That is, after pre-training, BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as natural language inference and text classification, and sequence-to-sequence-based language generation tasks such as question answering and conversational response generation.\nThe original BERT paper published results demonstrating that a small amount of finetuning (for BERTLARGE, 1 hour on 1 Cloud TPU) allowed it to achieved state-of-the-art performance on a number of natural language understanding tasks:\n\nGLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks);\nSQuAD (Stanford Question Answering Dataset) v1.1 and v2.0;\nSWAG (Situations With Adversarial Generations).\nIn the original paper, all parameters of BERT are fine-tuned, and recommended that, for downstream applications that are text classifications, the output token at the [CLS] input token is fed into a linear-softmax layer to produce the label outputs.\nThe original code base defined the final linear layer as a \"pooler layer\", in analogy with global pooling in computer vision, even though it simply discards all output tokens except the one corresponding to  [CLS] .\n\n\n=== Cost ===\nBERT was trained on the BookCorpus (800M words) and a filtered version of English Wikipedia (2,500M words) without lists, tables, and headers.\nTraining BERTBASE  on 4 cloud TPU (16 TPU chips total) took 4 days, at an estimated cost of 500 USD. Training BERTLARGE on 16 cloud TPU (64 TPU chips total) took 4 days.\n\n\n== Interpretation ==\nLanguage models like ELMo, GPT-2, and BERT, spawned the study of \"BERTology\", which attempts to interpret what is learned by these models. Their performance on these natural language understanding tasks are not yet well understood. Several research publications in 2018 and 2019 focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences, analysis of internal vector representations through probing classifiers, and the relationships represented by attention weights.\nThe high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT, based on the Transformer model architecture, applies its self-attention mechanism to learn information from a text from the left and right side during training, and consequently gains a deep understanding of the context. For example, the word fine can have two different meanings depending on the context (I feel fine today, She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side.\nHowever it comes at a cost: due to encoder-only architecture lacking a decoder, BERT can't be prompted and can't generate text, while bidirectional models in general do not work effectively without the right side, thus being difficult to prompt. As an illustrative example, if one wishes to use BERT to continue a sentence fragment \"Today, I went to\", then naively one would mask out all the tokens as \"Today, I went to  [MASK]  [MASK]  [MASK] ...  [MASK] .\" where the number of  [MASK]  is the length of the sentence one wishes to extend to. However, this constitutes a dataset shift, as during training, BERT has never seen sentences with that many tokens masked out. Consequently, its performance degrades. More sophisticated techniques allow text generation, but at a high computational cost.\n\n\n== History ==\nBERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The design has its origins from pre-training contextual representations, including semi-supervised sequence learning, generative pre-training, ELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, whereas BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for \"running\" will have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is running a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.\nOn October 25, 2019, Google announced that they had started applying BERT models to English-language search queries on Google Search within the US. On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020, almost every single English-based query was processed by a BERT model.\n\n\n== Variants ==\nThe BERT models were influential and inspired many variants.\nRoBERTa (2019) was an engineering improvement. It preserves BERT's architecture (slightly larger, at 355M parameters), but improves its training, changing key hyperparameters, removing the next-sentence prediction task, and using much larger mini-batch sizes. \nXLM-RoBERTa (2019) was a multilingual RoBERTa model. It was one of the first works on multilingual language modeling at scale.\nDistilBERT (2019) distills BERTBASE to a model with just 60% of its parameters (66M), while preserving 95% of its benchmark scores. Similarly, TinyBERT (2019) is a distilled model with just 28% of its parameters.\nALBERT (2019) used shared-parameter across layers, and experimented with independently varying the hidden size and the word-embedding layer's output size as two hyperparameters. They also replaced the next sentence prediction task with the sentence-order prediction (SOP) task, where the model must distinguish the correct order of two consecutive text segments from their reversed order. \nELECTRA (2020) applied the idea of generative adversarial networks to the MLM task. Instead of masking out tokens, a small language model generates random plausible substitutions, and a larger network identify these replaced tokens. The small model aims to fool the large model.\nDeBERTa (2020) is a significant architectural variant, with disentangled attention. Its key idea is to treat the positional and token encodings separately throughout the attention mechanism. Instead of combining the positional encoding (\n  \n    \n      \n        \n          x\n          \n            \n              p\n              o\n              s\n              i\n              t\n              i\n              o\n              n\n            \n          \n        \n      \n    \n    {\\displaystyle x_{\\mathrm {position} }}\n  \n) and token encoding (\n  \n    \n      \n        \n          x\n          \n            \n              t\n              o\n              k\n              e\n              n\n            \n          \n        \n      \n    \n    {\\displaystyle x_{\\mathrm {token} }}\n  \n) into a single input vector (\n  \n    \n      \n        \n          x\n          \n            \n              i\n              n\n              p\n              u\n              t\n            \n          \n        \n        =\n        \n          x\n          \n            \n              p\n              o\n              s\n              i\n              t\n              i\n              o\n              n\n            \n          \n        \n        +\n        \n          x\n          \n            \n              t\n              o\n              k\n              e\n              n\n            \n          \n        \n      \n    \n    {\\displaystyle x_{\\mathrm {input} }=x_{\\mathrm {position} }+x_{\\mathrm {token} }}\n  \n), DeBERTa keeps them separate as a tuple: \n  \n    \n      \n        (\n        \n          x\n          \n            \n              p\n              o\n              s\n              i\n              t\n              i\n              o\n              n\n            \n          \n        \n        ,\n        \n          x\n          \n            \n              t\n              o\n              k\n              e\n              n\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{\\mathrm {position} },x_{\\mathrm {token} })}\n  \n. Then, at each self-attention layer, DeBERTa computes three distinct attention matrices, rather than the single attention matrix used in BERT:\n\nThe three attention matrices are added together element-wise, then passed through a softmax layer and multiplied by a projection matrix.\nAbsolute position encoding is included in the final self-attention layer as additional input.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nRogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \"A Primer in BERTology: What we know about how BERT works\". arXiv:2002.12327 [cs.CL].\n\n\n== External links ==\nOfficial GitHub repository",
    "categories": [
      "2018 in artificial intelligence",
      "2018 software",
      "All Wikipedia articles written in American English",
      "All articles containing potentially dated statements",
      "Articles containing potentially dated statements from 2020",
      "Articles with short description",
      "Google software",
      "Large language models",
      "Short description is different from Wikidata",
      "Use American English from November 2023",
      "Use mdy dates from November 2023"
    ],
    "year_mentioned": 2019
  },
  {
    "title": "OpenAI",
    "url": "https://en.wikipedia.org/wiki/OpenAI",
    "content": "OpenAI is an American artificial intelligence (AI) organization headquartered in San Francisco, California. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\nThe organization has a complex corporate structure. As of October 2025, it is led by the non-profit OpenAI Foundation, founded in 2015 and registered in Delaware, which holds a 26% equity stake in OpenAI Group PBC, a for-profit public benefit corporation which commercializes its products. Microsoft invested over $13 billion into OpenAI, and provides Azure cloud computing resources. In October 2025, OpenAI conducted a $6.6 billion share sale that valued the company at $500 billion. On 28 October 2025, OpenAI said it had converted its main business into a for-profit corporation, with Microsoft acquiring a 27% stake in the company and the remaining non-profit company (now known as the OpenAI Foundation) owning a 26% stake.\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI's products. In November 2023, OpenAI's board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company's prominent role in an industry-wide problem.\n\n\n== Founding ==\n\nIn December 2015, OpenAI was founded as a not for profit organization by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. A total of $1 billion in capital was pledged by Sam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), and Infosys. The actual collected total amount of contributions was only $130 million until 2019.\nThe organization stated it would \"freely collaborate\" with other institutions and researchers by making some of its patents and research open to the public. OpenAI was initially run from Brockman's living room. It was later headquartered at the Pioneer Building in the Mission District, San Francisco.\nAccording to OpenAI's charter, its founding mission is \"to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity.\"\nMusk and Altman stated in 2015 that they were partly motivated by concerns about AI safety and existential risk from artificial general intelligence. OpenAI stated that \"it's hard to fathom how much human-level AI could benefit society\", and that it is equally difficult to comprehend \"how much it could damage society if built or used incorrectly\". The startup also wrote that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible\", and that \"because of AI's surprising history, it's hard to predict when human-level AI might come within reach. When it does, it'll be important to have a leading research institution which can prioritize a good outcome for all over its own self-interest.\" Co-chair Sam Altman expected a decades-long project that eventually surpasses human intelligence.\nBrockman met with Yoshua Bengio, one of the \"founding fathers\" of deep learning, and drew up a list of great AI researchers. Brockman was able to hire nine of them as the first employees in December 2015. OpenAI did not pay AI researchers salaries comparable to those of Facebook or Google. It also did not pay stock options which AI researchers typically get. Nevertheless, OpenAI spent $7 million on its first 52 employees in 2016. OpenAI's potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\" OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.\nIn April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research. Nvidia gifted its first DGX-1 supercomputer to OpenAI in August 2016 to help it train larger and more complex AI models with the capability of reducing processing time from six days to two hours. In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites, and other applications.\n\n\n== Corporate structure ==\n\n\n=== Transition from non-profit ===\nIn 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit being capped at 100 times any investment. According to OpenAI, the capped-profit model allows OpenAI Global, LLC to legally attract investment from venture funds and, in addition, to grant employees stakes in the company. Many top researchers work for Google Brain, DeepMind, or Facebook, which offer stock options that a nonprofit would be unable to. Before the transition, public disclosure of the compensation of top employees at OpenAI was legally required.\nThe company then distributed equity to its employees and partnered with Microsoft, announcing an investment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft.\nOpenAI Global, LLC then announced its intention to commercially license its technologies. It planned to spend $1 billion \"within five years, and possibly much faster\". Altman stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.\nThe nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global, LLC, which, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.'s nonprofit charter. A majority of OpenAI, Inc.'s board is barred from having financial stakes in OpenAI Global, LLC. In addition, minority members with a stake in OpenAI Global, LLC are barred from certain votes due to conflict of interest. Some researchers have argued that OpenAI Global, LLC's switch to for-profit status is inconsistent with OpenAI's claims to be \"democratizing\" AI.\nOn February 29, 2024, Elon Musk filed a lawsuit against OpenAI and CEO Sam Altman, accusing them of shifting focus from public benefit to profit maximization—a case OpenAI dismissed as \"incoherent\" and \"frivolous,\" though Musk later revived legal action against Altman and others in August.\nOn April 9, 2024, OpenAI countersued Musk in federal court, alleging that he had engaged in \"bad-faith tactics\" to slow the company's progress and seize its innovations for his personal benefit. OpenAI also argued that Musk had previously supported the creation of a for-profit structure and had expressed interest in controlling OpenAI himself. The countersuit seeks damages and legal measures to prevent further alleged interference.\nOn February 10, 2025, a consortium of investors led by Elon Musk submitted a $97.4 billion unsolicited bid to buy the nonprofit that controls OpenAI, declaring willingness to match or exceed any better offer. The offer was rejected on 14 February 2025, with OpenAI stating that it was not for sale, but the offer complicated Altman's restructuring plan by suggesting a lower bar for how much the nonprofit should be valued.\nOpenAI, Inc. was originally designed as a nonprofit in order to ensure that AGI \"benefits all of humanity\" rather than \"the private gain of any person\". In 2019, it created OpenAI Global, LLC, a capped-profit subsidiary controlled by the nonprofit. In December 2024, OpenAI proposed a restructuring plan to convert the capped-profit into a Delaware-based public benefit corporation (PBC), and to release it from the control of the nonprofit. The nonprofit would sell its control and other assets, getting equity in return, and would use it to fund and pursue separate charitable projects, including in science and education. OpenAI's leadership described the change as necessary to secure additional investments, and claimed that the nonprofit's founding mission to ensure AGI \"benefits all of humanity\" would be better fulfilled.\nThe plan has been criticized by former employees. A legal letter named \"Not For Private Gain\" asked the attorneys general of California and Delaware to intervene, stating that the restructuring is illegal and would remove governance safeguards from the nonprofit and the attorneys general. The letter argues that OpenAI's complex structure was deliberately designed to remain accountable to its mission, without the conflicting pressure of maximizing profits. It contends that the nonprofit is best positioned to advance its mission of ensuring AGI benefits all of humanity by continuing to control OpenAI Global, LLC, whatever the amount of equity that it could get in exchange. PBCs can choose how they balance their mission with profit-making. Controlling shareholders have a large influence on how closely a PBC sticks to its mission.\n\n\n==== 2025 restructuring ====\nOn October 28, 2025, OpenAI announced that it had adopted the new PBC corporate structure after receiving approval from the attorneys general of California and Delaware. Under the new structure, OpenAI's for-profit branch became a public benefit corporation known as OpenAI Group PBC, while the non-profit was renamed to the OpenAI Foundation. The OpenAI Foundation holds a 26% stake in the PBC, while Microsoft holds a 27% stake and the remaining 47% is owned by employees and other investors.\nAll members of the OpenAI Group PBC board of directors will be appointed by the OpenAI Foundation, which can remove them at any time. Members of the Foundation's board will also serve on the for-profit board. The new structure allows the for-profit PBC to raise investor funds like most traditional tech companies, including through an initial public offering, which Altman claimed was the most likely path forward.\n\n\n=== Partnership with Microsoft ===\nIn January 2023, OpenAI Global, LLC was in talks for funding that would value the company at $29 billion, double its 2021 value. On January 23, 2023, Microsoft announced a new US$10 billion investment in OpenAI Global, LLC over multiple years, partially needed to use Microsoft's cloud-computing service Azure.\nOn September 21, 2023, Microsoft had begun rebranding all variants of its Copilot to Microsoft Copilot, including the former Bing Chat and the Microsoft 365 Copilot. This strategy was followed in December 2023 by adding the MS-Copilot to many installations of Windows 11 and Windows 10 as well as a standalone Microsoft Copilot app released for Android and one released for iOS thereafter.\nFollowing OpenAI's 2025 restructuring, Microsoft owns a 27% stake in the for-profit OpenAI Group PBC, valued at $135 billion. In a deal announced the same day, OpenAI agreed to purchase $250 billion of Azure services, with Microsoft ceding their right of first refusal over OpenAI's future cloud computing purchases. As part of the deal, OpenAI will continue to share 20% of its revenue with Microsoft until it achieves AGI, which must now be verified by an independent panel of experts. The deal also loosened restrictions on both companies working with third parties, allowing Microsoft to pursue AGI independently and allowing OpenAI to develop products with other companies.\n\n\n=== Finances ===\n\nIn 2017, OpenAI spent $7.9 million, a quarter of its functional expenses, on cloud computing alone. In comparison, DeepMind's total expenses in 2017 were $442 million. In the summer of 2018, training OpenAI's Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks.\nIn October 2024, OpenAI completed a $6.6 billion capital raise with a $157 billion valuation including investments from Microsoft, Nvidia, and SoftBank.\nOn January 21, 2025, Donald Trump announced The Stargate Project, a joint venture between OpenAI, Oracle, SoftBank and MGX to build an AI infrastructure system in conjunction with the US government. The project takes its name from OpenAI's existing \"Stargate\" supercomputer project and is estimated to cost $500 billion. The partners plan to fund the project over the next four years. In July, the United States Department of Defense announced that OpenAI had received a $200 million contract for AI in the military, along with Anthropic, Google, and xAI. In the same month, the company made a deal with the UK Government to use ChatGPT and other AI tools in public services. OpenAI subsequently began a $50 million fund to support nonprofit and community organizations.\nIn April 2025, OpenAI raised $40 billion at a $300 billion post-money valuation, which was the highest-value private technology deal in history. The financing round was led by SoftBank, with other participants including Microsoft, Coatue, Altimeter and Thrive.\nIn July 2025, the company reported annualized revenue of $12 billion. This was an increase from $3.7 billion in 2024, which was driven by ChatGPT subscriptions, which reached 20 million paid subscribers by April 2025, up from 15.5 million at the end of 2024, alongside a rapidly expanding enterprise customer base that grew to five million business users.\nThe company cash burn remains high due to the intensive computational costs required to train and run large language models. It projects to lose $8 billion in 2025.\nLooking ahead, OpenAI has revised upward its long-term spending projections, now expecting to burn approximately $115 billion through 2029—roughly $80 billion more than the company's previous estimates. The annual cash burn is projected to escalate significantly, with spending expected to reach $17 billion in 2026, $35 billion in 2027, and $45 billion in 2028. These expenditures are primarily allocated toward expanding compute infrastructure, developing proprietary AI chips, constructing data centers, and funding intensive model training programs, with more than half of the spending through the end of the decade expected to support research-intensive compute for model training and development.\nThe company's financial strategy reflects a strategy of prioritizing market expansion and technological advancement over near-term profitability, with OpenAI targeting cash flow positive operations by 2029 and projecting revenue of approximately $200 billion by 2030. This aggressive spending trajectory underscores both the enormous capital requirements of scaling cutting-edge AI technology and OpenAI's commitment to maintaining its position as a leader in the artificial intelligence industry.\nIn October 2025, OpenAI completed an employee share sale of up to $10 billion to existing investors which valued the company at $500 billion. The deal values OpenAI as the most valuable privately owned company in the world—surpassing SpaceX as the world's most valuable private company.\n\n\n=== Firing of Altman ===\n\nOn November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of Helen Toner, Ilya Sutskever, Adam D'Angelo and Tasha McCauley) cited a lack of confidence in him. Chief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of OpenAI, was also removed as chairman of the board and resigned from the company's presidency shortly thereafter. Three senior OpenAI researchers subsequently resigned: director of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Mądry, and researcher Szymon Sidor.\nOn November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who objected to Altman's departure. Although Altman himself spoke in favor of returning to OpenAI, he has since stated that he considered starting a new company and bringing former OpenAI employees with him if talks to reinstate him didn't work out. The board members agreed \"in principle\" to resign if Altman returned. On November 19, 2023, negotiations with Altman to return failed and Murati was replaced by Emmett Shear as interim CEO. The board initially contacted Anthropic CEO Dario Amodei (a former OpenAI executive) about replacing Altman, and proposed a merger of the two companies, but both offers were declined.\nOn November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman would be joining Microsoft to lead a new advanced AI research team, but added that they were still committed to OpenAI despite recent events. Before the partnership with Microsoft was finalized, Altman gave the board another opportunity to negotiate with him. About 738 of OpenAI's 770 employees, including Murati and Sutskever, signed an open letter stating they would quit their jobs and join Microsoft if the board did not rehire Altman and then resign. This prompted OpenAI investors to consider legal action against the board as well. In response, OpenAI management sent an internal memo to employees stating that negotiations with Altman and the board had resumed and would take some time.\n\nOn November 21, 2023, after continued negotiations, Altman and Brockman returned to the company in their prior roles along with a reconstructed board made up of new members Bret Taylor (as chairman) and Lawrence Summers, with D'Angelo remaining. Concerns about Altman's response to this development, specifically regarding the discovery's potential safety implications, were reportedly raised with the company's board shortly before Altman's firing. On November 29, 2023, OpenAI announced that an anonymous Microsoft employee had joined the board as a non-voting member to observe the company's operations; Microsoft resigned from the board in July 2024.\nIn February 2024, the Securities and Exchange Commission subpoenaed OpenAI's internal communication to determine if Altman's alleged lack of candor misled investors.\nIn 2024, following the temporary removal of Sam Altman and his return, many employees gradually left OpenAI, including most of the original leadership team and a significant number of AI safety researchers.\n\n\n=== Acquisitions ===\nIn August 2023, it was announced that OpenAI had acquired the New York-based start-up Global Illumination, a company that deploys AI to develop digital infrastructure and creative tools.\nIn June 2024, OpenAI acquired Multi, a startup focused on remote collaboration.\nIn March 2025, OpenAI reached a deal with CoreWeave to acquire $350 million worth of CoreWeave shares and access to AI infrastructure, in return for $11.9 billion paid over five years. Microsoft was already CoreWeave's biggest customer in 2024. Alongside their other business dealings, OpenAI and Microsoft were renegotiating the terms of their partnership to facilitate a potential future initial public offering by OpenAI, while ensuring Microsoft's continued access to advanced AI models.\nOn May 21, OpenAI announced the $6.5 billion acquisition of io, an AI hardware start-up founded by former Apple designer Jony Ive in 2024.\nIn September 2025, OpenAI agreed to acquire the product testing startup Statsig for $1.1 billion in an all-stock deal and appointed Statsig's founding CEO Vijaye Raji as OpenAI's chief technology officer of applications. The company also announced development of an AI-driven hiring service designed to rival LinkedIn.\nOpenAI acquired personal finance app Roi in October 2025.\nIn December 2025, it was announced OpenAI had agreed to acquire Neptune, an AI tooling startup that helps companies track and manage model training, for an undisclosed amount.\n\n\n=== Corporate partnerships ===\n\nOpenAI has been criticized for outsourcing the annotation of data sets to Sama, a company based in San Francisco that employed workers in Kenya. These annotations were used to train an AI model to detect toxicity, which could then be used to moderate toxic content, notably from ChatGPT's training data and outputs. However, these pieces of text usually contained detailed descriptions of various types of violence, including sexual violence. The investigation uncovered that OpenAI began sending snippets of data to Sama as early as November 2021. The four Sama employees interviewed by Time described themselves as mentally scarred. OpenAI paid Sama $12.50 per hour of work, and Sama was redistributing the equivalent of between $1.32 and $2.00 per hour post-tax to its annotators. Sama's spokesperson said that the $12.50 was also covering other implicit costs, among which were infrastructure expenses, quality assurance and management.\nOpenAI began collaborating with Broadcom in 2024 to design a custom AI chip capable of both training and inference targeted for mass production in 2026 and to be manufactured by TSMC in 3 nm node. This initiative intended to reduce OpenAI's dependence on Nvidia GPUs, which are costly and face high demand in the market.\nIn January 2024, Arizona State University purchased ChatGPT Enterprise in OpenAI's first deal with a university.\nIn June 2024, Apple Inc. signed a contract with OpenAI to integrate ChatGPT features into its products as part of its new Apple Intelligence initiative.\nIn June 2025, OpenAI began renting Google Cloud's Tensor Processing Units (TPUs) to support ChatGPT and related services, marking its first meaningful use of non‑Nvidia AI chips.\nIn September 2025, it was revealed that OpenAI signed a contract with Oracle to purchase $300 billion in computing power over the next five years.\nIn September 2025, OpenAI and NVIDIA announced a partnership that included a potential deployment of at least 10 gigawatts of NVIDIA systems and a $100 billion investment from NVIDIA in OpenAI.\nIn October 2025, OpenAI announced a multi-billion dollar deal with AMD. OpenAI committed to purchasing six gigawatts worth of AMD chips, starting with the MI450. OpenAI will have the option to buy up to 160 million shares of AMD, about 10% of the company, depending on development, performance and share price targets.\nIn December 2025, Disney said it would make a $1 billion investment in OpenAI, and signed a three-year licensing deal that will let users generate videos using Sora—Open AI's short-form AI video platform. More than 200 Disney, Marvel, Star Wars and Pixar characters will be available to OpenAI users.\n\n\n=== Government contracting ===\nOpenAI provides LLMs to the Artificial Intelligence Cyber Challenge, and to the Advanced Research Projects Agency for Health. In October 2024, The Intercept revealed that OpenAI's tools are considered \"essential\" for AFRICOM's mission and included in an \"Exception to Fair Opportunity\" contractual agreement between the United States Department of Defense and Microsoft. In December 2024, OpenAI said it would partner with defense-tech company Anduril to build drone defense technologies for the United States and its allies.\nIn 2025, OpenAI's Chief Product Officer, Kevin Weil, was commissioned lieutenant colonel in the U.S. Army to join Detachment 201 as senior advisor.\nIn June 2025, the U.S. Department of Defense awarded OpenAI a $200 million one-year contract to develop AI tools for military and national security applications. OpenAI announced a new program, OpenAI for Government, to give federal, state, and local governments access to its models, including ChatGPT.\n\n\n== Services ==\n\n\n=== Products ===\nChatGPT\nChatGPT Deep Research\nChatGPT Search\nChatGPT Atlas\nOpenAI Codex\nSora (text-to-video model)\nWhisper (speech recognition system)\nAn API that gives access to various OpenAI models\n\n\n=== Development ===\nIn February 2019, GPT-2 was announced, which gained attention for its ability to generate human-like text.\nIn 2020, OpenAI announced GPT-3, a language model trained on large internet datasets. GPT-3 is aimed at natural language answering questions, but it can also translate between languages and coherently generate improvised text. It also announced that an associated API, named the API, would form the heart of its first commercial product.\nEleven employees left OpenAI, mostly between December 2020 and January 2021, in order to establish Anthropic.\nIn 2021, OpenAI introduced DALL-E, a specialized deep learning model adept at generating complex digital images from textual descriptions, utilizing a variant of the GPT-3 architecture.\n\nIn December 2022, OpenAI received widespread media coverage after launching a free preview of ChatGPT, its new AI chatbot based on GPT-3.5. According to OpenAI, the preview received over a million signups within the first five days. According to anonymous sources cited by Reuters in December 2022, OpenAI Global, LLC was projecting $200 million of revenue in 2023 and $1 billion in revenue in 2024.\nGoogle announced a similar AI application (Bard), after ChatGPT was launched, fearing that ChatGPT could threaten Google's place as a go-to source for information.\nOn February 7, 2023, Microsoft announced that it was building AI technology based on the same foundation as ChatGPT into Microsoft Bing, Edge, Microsoft 365 and other products.\nOn March 14, 2023, OpenAI released GPT-4, both as an API (with a waitlist) and as a feature of ChatGPT Plus.\nOn November 6, 2023, OpenAI launched GPTs, allowing individuals to create customized versions of ChatGPT for specific purposes, further expanding the possibilities of AI applications across various industries. On November 14, 2023, OpenAI announced they temporarily suspended new sign-ups for ChatGPT Plus due to high demand. Access for newer subscribers re-opened a month later on December 13.\nIn December 2024, the company launched the Sora model. It also launched OpenAI o1, an early reasoning model that was internally codenamed strawberry. Additionally, ChatGPT Pro—a $200/month subscription service offering unlimited o1 access and enhanced voice features—was introduced, and preliminary benchmark results for the upcoming OpenAI o3 models were shared.\nOn January 23, 2025, OpenAI released Operator, an AI agent and web automation tool for accessing websites to execute goals defined by users. The feature was only available to Pro users in the United States. OpenAI released deep research agent, nine days later. It scored a 27% accuracy on the benchmark Humanity's Last Exam (HLE). Altman later stated GPT-4.5 would be the last model without full chain-of-thought reasoning.\nIn July 2025, reports indicated that AI models by both OpenAI and Google DeepMind solved mathematics problems at the level of top-performing students in the International Mathematical Olympiad. OpenAI's large language model was able to achieve gold medal-level performance, reflecting significant progress in AI's reasoning abilities.\nIn September 2025, OpenAI released a first-of-its-kind study revealing how people use ChatGPT for everyday tasks. The study found that \"non-work tasks\" (according to an LLM-based classifier) account for more than 72 percent of all ChatGPT usage, with a minority of overall usage related to business productivity.\nOn October 6, 2025, OpenAI unveiled its Agent Builder platform during the company's DevDay event. The platform features a drag-and-drop visual interface that allows developers and businesses to design, test, and deploy agentic workflows without requiring extensive coding expertise.\nOn October 21, 2025, OpenAI introduced ChatGPT Atlas, a browser integrating the ChatGPT assistant directly into web navigation, to compete with existing browsers such as Google Chrome and Apple Safari.\nOn December 11, 2025, OpenAI announced GPT-5.2. This model will be better at creating spreadsheets, building presentations, perceiving images, writing code and understanding long context.\n\n\n=== Transparency ===\nIn March 2023, the company was criticized for disclosing particularly few technical details about products like GPT-4, contradicting its initial commitment to openness and making it harder for independent researchers to replicate its work and develop safeguards. OpenAI cited competitiveness and safety concerns to justify this strategic turn. OpenAI's former chief scientist Ilya Sutskever argued in 2023 that open-sourcing increasingly capable models was increasingly risky, and that the safety reasons for not open-sourcing the most potent AI models would become \"obvious\" in a few years.\n\n\n=== Alignment ===\nIn July 2023, OpenAI launched the superalignment project, aiming to find within 4 years how to align future superintelligences by automating alignment research using AI. OpenAI promised to dedicate 20% of its computing resources to the project, although the team denied receiving anything close to 20%. OpenAI ended the project in May 2024 after its co-leaders Ilya Sutskever and Jan Leike left the company.\n\n\n=== Leaked conversations ===\nIn August 2025, OpenAI was criticized after thousands of private ChatGPT conversations were inadvertently exposed to public search engines like Google due to an experimental \"share with search engines\" feature. The opt-in toggle, intended to allow users to make specific chats discoverable, resulted in some discussions including personal details such as names, locations, and intimate topics appearing in search results when users accidentally enabled it while sharing links. OpenAI announced the feature's permanent removal on August 1, 2025, and the company began coordinating with search providers to remove the exposed content, emphasizing that it was not a security breach but a design flaw that heightened privacy risks. CEO Sam Altman acknowledged the issue in a podcast, noting users often treat ChatGPT as a confidant for deeply personal matters, which amplified concerns about AI handling sensitive data.\n\n\n== Management ==\n\n\n=== Key employees ===\nCEO and co-founder: Sam Altman, former president of the start-up accelerator Y Combinator\nPresident and co-founder: Greg Brockman, former CTO, 3rd employee of Stripe\nChief Scientist Officer: Jakub Pachocki, former Director of Research at OpenAI\nChief Operating Officer: Brad Lightcap, previously at Y Combinator and JPMorgan Chase\nChief Financial Officer: Sarah Friar, former Nextdoor CEO and former CFO at Block, Inc.\nChief Product Officer: Kevin Weil, previously at Twitter, Inc. and Meta Platforms\nChief Research Officer: Mark Chen, former SVP of Research at OpenAI\nChief Compliance Officer: Scott Schools, former Chief Compliance Officer of Uber\nChief Global Affairs Officer: Chris Lehane, former head of global policy at Airbnb\nChief Economist: Aaron Chatterji, professor of business and public policy at Duke University's Fuqua School of Business\nCEO of Applications: Fidji Simo, former CEO of Instacart\n\n\n=== Board of directors of the OpenAI nonprofit ===\nBret Taylor (chairman), former chairman of Twitter's board of directors and co-CEO of Salesforce\nSam Altman\nAdam D'Angelo, co-founder and CEO of Quora\nSue Desmond-Hellmann, former CEO of the Bill & Melinda Gates Foundation\nNicole Seligman, attorney and former executive vice president of the Sony Corporation\nPaul Nakasone, former Director of the National Security Agency (2018–2024)\nZico Kolter, computer scientist\nAdebayo Ogunlesi, managing partner at Global Infrastructure Partners\n\n\n=== Principal individual investors ===\nReid Hoffman, LinkedIn co-founder\nPeter Thiel, PayPal co-founder\nJessica Livingston, a founding partner of Y Combinator\nElon Musk, co-founder\n\n\n=== Personnel changes ===\nIn 2018, Musk resigned from his Board of Directors seat, citing \"a potential future conflict [of interest]\" with his role as CEO of Tesla due to Tesla's AI development for self-driving cars. OpenAI stated that Musk's financial contributions were below $45 million.\nOn March 3, 2023, Reid Hoffman resigned from his board seat, citing a desire to avoid conflicts of interest with his investments in AI companies via Greylock Partners, and his co-founding of the AI startup Inflection AI. Hoffman remained on the board of Microsoft, a major investor in OpenAI.\nIn May 2024, Chief Scientist Ilya Sutskever resigned and was succeeded by Jakub Pachocki. Co-leader Jan Leike also departed amid concerns over safety and trust. OpenAI then signed deals with Reddit, News Corp, Axios, and Vox Media. Paul Nakasone then joined the board of OpenAI.\nIn August 2024, cofounder John Schulman left OpenAI to join Anthropic, and OpenAI's president Greg Brockman took extended leave until November.\nIn September 2024, CTO Mira Murati left the company.\nIn November 2025, Lawrence Summers resigned from the board of directors.\n\n\n== Governance and legal issues ==\nIn May 2023, Sam Altman, Greg Brockman and Ilya Sutskever posted recommendations for the governance of superintelligence. They stated that superintelligence could happen within the next 10 years, allowing a \"dramatically more prosperous future\" and that \"given the possibility of existential risk, we can't just be reactive\". They proposed creating an international watchdog organization similar to IAEA to oversee AI systems above a certain capability threshold, suggesting that relatively weak AI systems on the other side should not be overly regulated. They also called for more technical safety research for superintelligences, and asked for more coordination, for example through governments launching a joint project which \"many current efforts become part of\".\nIn July 2023, the FTC issued a civil investigative demand to OpenAI to investigate whether the company's data security and privacy practices to develop ChatGPT were unfair or harmed consumers (including by reputational harm) in violation of Section 5 of the Federal Trade Commission Act of 1914. These are typically preliminary investigative matters and are nonpublic, but the FTC's document was leaked. In July 2023, the FTC launched an investigation into OpenAI over allegations that the company scraped public data and published false and defamatory information. They asked OpenAI for comprehensive information about its technology and privacy safeguards, as well as any steps taken to prevent the recurrence of situations in which its chatbot generated false and derogatory content about people. The agency then reported concern with circular spending in which, for example, Microsoft gives OpenAI credit to Microsoft Azure and the companies provide each other access to engineering talent was of particular concern for its potential negative impacts to the public.\nIn September 2024, OpenAI's global affairs chief endorsed the UK's \"smart\" AI regulation during testimony to a House of Lords committee.\nIn February 2025, OpenAI CEO Sam Altman stated that the company is interested in collaborating with the People's Republic of China, despite regulatory restrictions imposed by the U.S. government. This shift comes in response to the growing influence of the Chinese artificial intelligence company DeepSeek, which has disrupted the AI market with open models, including DeepSeek V3 and DeepSeek R1. In response to DeepSeek, OpenAI overhauled its security operations to better guard against industrial espionage, particularly amid allegations that DeepSeek had improperly copied OpenAI's distillation techniques.\nAccording to Oliver Roberts, in March 2025, the United States had 781 state AI bills or laws. OpenAI advocated for preempting state AI laws with federal laws. According to Scott Kohler, OpenAI has opposed California's AI legislation and suggested that the state bill encroaches on a more competent federal government. Public Citizen opposed a federal preemption on AI and pointed to OpenAI's growth and valuation as evidence that existing state laws have not hampered innovation.\n\n\n=== Non-disparagement agreements ===\nBefore May 2025, OpenAI required departing employees to sign a lifelong non-disparagement agreement forbidding them from criticizing OpenAI and acknowledging the existence of the agreement. Daniel Kokotajlo, a former employee, publicly stated that he forfeited his vested equity in OpenAI in order to leave without signing the agreement. Sam Altman stated that he was unaware of the equity cancellation provision, and that OpenAI never enforced it to cancel any employee's vested equity. However, leaked documents and emails refute this claim. On May 23, 2024, OpenAI sent a memo releasing former employees from the agreement.\n\n\n=== Copyright ===\nOpenAI was sued for copyright infringement by authors Sarah Silverman, Matthew Butterick, Paul Tremblay and Mona Awad in July 2023. In September 2023, 17 authors, including George R. R. Martin, John Grisham, Jodi Picoult and Jonathan Franzen, joined the Authors Guild in filing a class action lawsuit against OpenAI, alleging that the company's technology was illegally using their copyrighted work. The New York Times also sued the company in late December 2023. In May 2024 it was revealed that OpenAI had destroyed its Books1 and Books2 training datasets, which were used in the training of GPT-3, and which the Authors Guild believed to have contained over 100,000 copyrighted books.\nIn 2021, OpenAI developed a speech recognition tool called Whisper. OpenAI used it to transcribe more than one million hours of YouTube videos into text for training GPT-4. The automated transcription of YouTube videos raised concerns within OpenAI employees regarding potential violations of YouTube's terms of service, which prohibit the use of videos for applications independent of the platform, as well as any type of automated access to its videos. Despite these concerns, the project proceeded with notable involvement from OpenAI's president, Greg Brockman. The resulting dataset proved instrumental in training GPT-4.\nIn February 2024, The Intercept as well as Raw Story and Alternate Media Inc. filed lawsuit against OpenAI on copyright litigation ground. The lawsuit is said to have charted a new legal strategy for digital-only publishers to sue OpenAI.\nOn April 30, 2024, eight newspapers filed a lawsuit in the Southern District of New York against OpenAI and Microsoft, claiming illegal harvesting of their copyrighted articles. The suing publications included The Mercury News, The Denver Post, The Orange County Register, St. Paul Pioneer Press, Chicago Tribune, Orlando Sentinel, Sun Sentinel, and New York Daily News.\nIn June 2023, a lawsuit claimed that OpenAI scraped 300 billion words online without consent and without registering as a data broker. It was filed in San Francisco, California, by sixteen anonymous plaintiffs. They also claimed that OpenAI and its partner as well as customer Microsoft continued to unlawfully collect and use personal data from millions of consumers worldwide to train artificial intelligence models.\nOn May 22, 2024, OpenAI entered into an agreement with News Corp to integrate news content from The Wall Street Journal, the New York Post, The Times, and The Sunday Times into its AI platform. Meanwhile, other publications like The New York Times chose to sue OpenAI and Microsoft for copyright infringement over the use of their content to train AI models. In November 2024, a coalition of Canadian news outlets, including the Toronto Star, Metroland Media, Postmedia, The Globe and Mail, The Canadian Press and CBC, sued OpenAI for using their news articles to train its software without permission.\nIn October 2024 during a New York Times interview, Suchir Balaji accused OpenAI of violating copyright law in developing its commercial LLMs which he had helped engineer. He was a likely witness in a major copyright trial against the AI company, and was one of several of its current or former employees named in court filings as potentially having documents relevant to the case. On November 26, 2024,  Balaji shot himself dead. His death led to conspiracy theories suggesting he had been deliberately silenced. California Congressman Ro Khanna endorsed calls for an investigation.\n\n\n=== GDPR compliance ===\nIn April 2023, the EU's European Data Protection Board (EDPB) formed a dedicated task force on ChatGPT \"to foster cooperation and to exchange information on possible enforcement actions conducted by data protection authorities\" based on the \"enforcement action undertaken by the Italian data protection authority against OpenAI about the ChatGPT service\".\nIn late April 2024 NOYB filed a complaint with the Austrian Datenschutzbehörde against OpenAI for violating the European General Data Protection Regulation. A text created with ChatGPT gave a false date of birth for a living person without giving the individual the option to see the personal data used in the process. A request to correct the mistake was denied. Additionally, neither the recipients of ChatGPT's work nor the sources used, could be made available, OpenAI claimed.\n\n\n=== Military and warfare ===\nOpenAI was criticized for lifting its ban on using ChatGPT for \"military and warfare\". Up until January 10, 2024, its \"usage policies\" included a ban on \"activity that has high risk of physical harm, including\", specifically, \"weapons development\" and \"military and warfare\". Its new policies prohibit \"[using] our service to harm yourself or others\" and to \"develop or use weapons\".\n\n\n=== Wrongful-death lawsuits over ChatGPT safety (2025) ===\n \nIn August 2025, the parents of a 16-year-old boy who died by suicide filed a wrongful death lawsuit against OpenAI (and CEO Sam Altman), alleging that months of conversations with ChatGPT about mental health and methods of self-harm contributed to their son's death and that safeguards were inadequate for minors. OpenAI expressed condolences and said it was strengthening protections (including updated crisis response behavior and parental controls). Coverage described it as a first-of-its-kind wrongful death case targeting the company's chatbot. The complaint was filed in California state court in San Francisco.\nIn November 2025, the Social Media Victims Law Center and Tech Justice Law Project filed seven lawsuits against OpenAI, of which four lawsuits alleged wrongful death. The suits were filed on behalf of Zane Shamblin, 23, of Texas; Amaurie Lacey, 17, of Georgia; Joshua Enneking, 26, of Florida; and Joe Ceccanti, 48, of Oregon, who each committed suicide after prolonged ChatGPT usage.\n\n\n== See also ==\nAnthropic – American artificial intelligence research company\nGoogle DeepMind – AI research laboratory\nxAI – American artificial intelligence corporation\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nOfficial website",
    "categories": [
      "2015 establishments in California",
      "2015 in San Francisco",
      "2015 in artificial intelligence",
      "501(c)(3) organizations",
      "All Wikipedia articles written in American English",
      "American companies established in 2015",
      "Articles slanted towards recent events from August 2025",
      "Articles with short description",
      "Artificial intelligence associations",
      "Artificial intelligence laboratories",
      "CS1 maint: multiple names: authors list",
      "Commons category link is on Wikidata",
      "Non-profit organizations based in San Francisco",
      "OpenAI",
      "Research institutes in the San Francisco Bay Area",
      "Short description is different from Wikidata",
      "Use American English from May 2023",
      "Use mdy dates from September 2024",
      "Webarchive template wayback links",
      "Wikipedia semi-protected pages"
    ],
    "year_mentioned": 2024
  },
  {
    "title": "Fine-tuning (deep learning)",
    "url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
    "content": "Fine-tuning (in deep learning) is the process of adapting a model trained for one task (the upstream task) to perform a different, usually more specific, task (the downstream task). It is considered a form of transfer learning, as it reuses knowledge learned from the original training objective.\nFine-tuning involves applying additional training (e.g., on new data) to the parameters of a neural network that have been pre-trained. Many variants exist.  The additional training can be applied to the entire neural network, or to only a subset of its layers, in which case the layers that are not being fine-tuned are \"frozen\" (i.e., not changed during backpropagation). A model may also be augmented with \"adapters\"—lightweight modules inserted into the model's architecture that nudge the embedding space for domain adaptation. These contain far fewer parameters than the original model and can be fine-tuned in a parameter-efficient way by tuning only their weights and leaving the rest of the model's weights frozen.\nFor some architectures, such as convolutional neural networks, it is common to keep the earlier layers (those closest to the input layer) frozen, as they capture lower-level features, while later layers often discern high-level features that can be more related to the task that the model is trained on.\nModels that are pre-trained on large, general corpora are usually fine-tuned by reusing their parameters as a starting point and adding a task-specific layer trained from scratch. Fine-tuning the full model is also common and often yields better results, but is more computationally expensive.\nFine-tuning is typically accomplished via supervised learning, but there are also techniques to fine-tune a model using weak supervision. Fine-tuning can be combined with a reinforcement learning from human feedback-based objective to produce language models such as ChatGPT (a fine-tuned version of GPT models) and Sparrow.\n\n\n== Robustness ==\nFine-tuning can degrade a model's robustness to distribution shifts. One mitigation is to linearly interpolate a fine-tuned model's weights with the weights of the original model, which can greatly increase out-of-distribution performance while largely retaining the in-distribution performance of the fine-tuned model.\n\n\n== Variants ==\n\n\n=== Low-rank adaptation ===\nLow-rank adaptation (LoRA) is an adapter-based technique for efficiently fine-tuning models. The basic idea is to design a low-rank matrix that is then added to the original matrix. An adapter, in this context, is a collection of low-rank matrices which, when added to a base model, produces a fine-tuned model. It allows for performance that approaches full-model fine-tuning with lower space requirements. A language model with billions of parameters may be LoRA fine-tuned with only several millions of parameters.\nLoRA-based fine-tuning has become popular in the Stable Diffusion community. Support for LoRA was integrated into the diffusers library from Hugging Face. Support for LoRA and similar techniques is also available for a wide range of other models through Hugging Face's parameter-efficient fine-tuning (PEFT) package.\n\n\n=== Representation fine-tuning ===\n\nRepresentation fine-tuning (ReFT) is a technique developed by researchers at Stanford University aimed at fine-tuning large language models (LLMs) by modifying less than 1% of their representations. Unlike parameter-efficient fine-tuning (PEFT) methods, which mainly focus on updating weights, ReFT targets representations, suggesting that modifying representations might be a more effective strategy than updating weights.\nReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations and train interventions that manipulate a small fraction of model representations to steer model behaviors towards solving downstream tasks at inference time. One specific method within the ReFT family is low-rank linear subspace ReFT (LoReFT), which intervenes on hidden representations in the linear subspace spanned by a low-rank projection matrix. LoReFT can be seen as the representation-based equivalent of low-rank adaptation (LoRA).\n\n\n== Applications ==\n\n\n=== Natural language processing ===\nFine-tuning is common in natural language processing (NLP), especially in the domain of language modeling. Large language models like OpenAI's series of GPT foundation models can be fine-tuned on data for specific downstream NLP tasks (tasks that use a pre-trained model) to improve performance over the unmodified pre-trained model.\nPlatforms such as Semrush's AI Visibility Toolkit and Enterprise AIO exemplify how fine-tuned models are being used for entity-level monitoring; tracking how named entities are referenced and represented within responses generated by large-language-model-based answer engines. \n\n\n== Commercial models ==\nCommercially-offered large language models can sometimes be fine-tuned if the provider offers a fine-tuning API. As of June 19, 2023, language model fine-tuning APIs are offered by OpenAI and Microsoft Azure's Azure OpenAI Service for a subset of their models, as well as by Google Cloud Platform for some of their PaLM models, and by others.\n\n\n== See also ==\nCatastrophic forgetting\nContinual learning\nDomain adaptation\nFoundation model\nHyperparameter optimization\nOverfitting\n\n\n== References ==",
    "categories": [
      "All articles needing additional references",
      "Articles needing additional references from May 2024",
      "Articles with short description",
      "CS1 errors: missing periodical",
      "Deep learning",
      "Machine learning",
      "Short description is different from Wikidata"
    ],
    "year_mentioned": 2023
  },
  {
    "title": "Transfer learning",
    "url": "https://en.wikipedia.org/wiki/Transfer_learning",
    "content": "Transfer learning (TL) is a technique in machine learning (ML) in which knowledge learned from a task is re-used in order to boost performance on a related task. For example, for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This topic is related to the psychological literature on transfer of learning, although practical ties between the two fields are limited. Reusing or transferring information from previously learned tasks to new tasks has the potential to significantly improve learning efficiency.\nSince transfer learning makes use of training with multiple objective functions it is related to cost-sensitive machine learning and multi-objective optimization.\n\n\n== History ==\nIn 1976, Bozinovski and Fulgosi published a paper addressing transfer learning in neural network training. The paper gives a mathematical and geometrical model of the topic. In 1981, a report considered the application of transfer learning to a dataset of images representing letters of computer terminals, experimentally demonstrating positive and negative transfer learning.\nIn 1992, Lorien Pratt formulated the discriminability-based transfer (DBT) algorithm.\nBy 1998, the field had advanced to include multi-task learning, along with more formal theoretical foundations. Influential publications on transfer learning include the book Learning to Learn in 1998, a 2009 survey and a 2019 survey.\nNg said in his NIPS 2016 tutorial that TL would become the next driver of machine learning commercial success after supervised learning.\nIn the 2020 paper, \"Rethinking Pre-Training and self-training\", Zoph et al. reported that pre-training can hurt accuracy, and advocate self-training instead.\n\n\n== Definition ==\nThe definition of transfer learning is given in terms of domains and tasks. A domain \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  \n consists of: a feature space \n  \n    \n      \n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {X}}}\n  \n and a marginal probability distribution \n  \n    \n      \n        P\n        (\n        X\n        )\n      \n    \n    {\\displaystyle P(X)}\n  \n, where \n  \n    \n      \n        X\n        =\n        {\n        \n          x\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            n\n          \n        \n        }\n        ∈\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle X=\\{x_{1},...,x_{n}\\}\\in {\\mathcal {X}}}\n  \n. Given a specific domain, \n  \n    \n      \n        \n          \n            D\n          \n        \n        =\n        {\n        \n          \n            X\n          \n        \n        ,\n        P\n        (\n        X\n        )\n        }\n      \n    \n    {\\displaystyle {\\mathcal {D}}=\\{{\\mathcal {X}},P(X)\\}}\n  \n, a task consists of two components: a label space \n  \n    \n      \n        \n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {Y}}}\n  \n and an objective predictive function \n  \n    \n      \n        f\n        :\n        \n          \n            X\n          \n        \n        →\n        \n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle f:{\\mathcal {X}}\\rightarrow {\\mathcal {Y}}}\n  \n. The function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n is used to predict the corresponding label \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n  \n of a new instance \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n. This task, denoted by \n  \n    \n      \n        \n          \n            T\n          \n        \n        =\n        {\n        \n          \n            Y\n          \n        \n        ,\n        f\n        (\n        x\n        )\n        }\n      \n    \n    {\\displaystyle {\\mathcal {T}}=\\{{\\mathcal {Y}},f(x)\\}}\n  \n, is learned from the training data consisting of pairs \n  \n    \n      \n        {\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{x_{i},y_{i}\\}}\n  \n, where \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        ∈\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle x_{i}\\in {\\mathcal {X}}}\n  \n and \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        ∈\n        \n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle y_{i}\\in {\\mathcal {Y}}}\n  \n.\nGiven a source domain \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            S\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{S}}\n  \n and learning task \n  \n    \n      \n        \n          \n            \n              T\n            \n          \n          \n            S\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {T}}_{S}}\n  \n, a target domain \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{T}}\n  \n and learning task \n  \n    \n      \n        \n          \n            \n              T\n            \n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {T}}_{T}}\n  \n, where \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            S\n          \n        \n        ≠\n        \n          \n            \n              D\n            \n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{S}\\neq {\\mathcal {D}}_{T}}\n  \n, or \n  \n    \n      \n        \n          \n            \n              T\n            \n          \n          \n            S\n          \n        \n        ≠\n        \n          \n            \n              T\n            \n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {T}}_{S}\\neq {\\mathcal {T}}_{T}}\n  \n, transfer learning aims to help improve the learning of the target predictive function \n  \n    \n      \n        \n          f\n          \n            T\n          \n        \n        (\n        ⋅\n        )\n      \n    \n    {\\displaystyle f_{T}(\\cdot )}\n  \n in \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{T}}\n  \n using the knowledge in \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            S\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{S}}\n  \n and \n  \n    \n      \n        \n          \n            \n              T\n            \n          \n          \n            S\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {T}}_{S}}\n  \n.\n\n\n== Applications ==\nAlgorithms for transfer learning are available in Markov logic networks and Bayesian networks. Transfer learning has been applied to cancer subtype discovery, building utilization, general game playing, text classification, digit recognition, medical imaging and spam filtering.\nIn 2020, it was discovered that, due to their similar physical natures, transfer learning is possible between electromyographic (EMG) signals from the muscles and classifying the behaviors of electroencephalographic (EEG) brainwaves, from the gesture recognition domain to the mental state recognition domain. It was noted that this relationship worked in both directions, showing that electroencephalographic can likewise be used to classify EMG. The experiments noted that the accuracy of neural networks and convolutional neural networks were improved through transfer learning both prior to any learning (compared to standard random weight distribution) and at the end of the learning process (asymptote). That is, results are improved by exposure to another domain. Moreover, the end-user of a pre-trained model can change the structure of fully-connected layers to improve performance.\n\n\n== See also ==\nCrossover (genetic algorithm)\nDomain adaptation\nGeneral game playing\nMulti-task learning\nMultitask optimization\nTransfer of learning in educational psychology\nZero-shot learning\nFeature learning\nexternal validity\n\n\n== References ==\n\n\n== Sources ==\nThrun, Sebastian; Pratt, Lorien (6 December 2012). Learning to Learn. Springer Science & Business Media. ISBN 978-1-4615-5529-2.",
    "categories": [
      "Articles with short description",
      "CS1 maint: location missing publisher",
      "Machine learning",
      "Short description is different from Wikidata"
    ],
    "year_mentioned": 2009
  },
  {
    "title": "Explainable artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Explainable_artificial_intelligence",
    "content": "Within artificial intelligence (AI), explainable AI (XAI), generally overlapping with interpretable AI or explainable machine learning (XML), is a field of research that explores methods that provide humans with the ability of intellectual oversight over AI algorithms. The main focus is on the reasoning behind the decisions or predictions made by the AI algorithms, to make them more understandable and transparent. This addresses users' requirement to assess safety and scrutinize the automated decision making in applications. XAI counters the \"black box\" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.\nXAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason. XAI may be an implementation of the social right to explanation. Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on. This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions.\n\n\n== Background ==\nMachine learning (ML) algorithms used in AI can be categorized as white-box or black-box. White-box models provide results that are understandable to experts in the domain. Black-box models, on the other hand, are extremely hard to explain and may not be understood even by domain experts. XAI algorithms follow the three principles of transparency, interpretability, and explainability.\n\nA model is transparent \"if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.\"\nInterpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision-making in a way that is understandable to humans.\nExplainability is a concept that is recognized as important, but a consensus definition is not yet available; one possibility is \"the collection of features of the interpretable domain that have contributed, for a given example, to producing a decision (e.g., classification or regression)\".\nIn summary, Interpretability refers to the user's ability to understand model outputs, while Model Transparency includes Simulatability (reproducibility of predictions), Decomposability (intuitive explanations for parameters), and Algorithmic Transparency (explaining how algorithms work). Model Functionality focuses on textual descriptions, visualization, and local explanations, which clarify specific outputs or instances rather than entire models. All these concepts aim to enhance the comprehensibility and usability of AI systems.\nIf algorithms fulfill these principles, they provide a basis for justifying decisions, tracking them and thereby verifying them, improving the algorithms, and exploring new facts.\nSometimes it is also possible to achieve a high-accuracy result with white-box ML algorithms. These algorithms have an interpretable structure that can be used to explain predictions. Concept Bottleneck Models, which use concept-level abstractions to explain model reasoning, are examples of this and can be applied in both image and text prediction tasks. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms. Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm searches the space of mathematical expressions to find the model that best fits a given dataset.\nAI systems optimize behavior to satisfy a mathematically specified goal system chosen by the system designers, such as the command \"maximize the accuracy of assessing how positive film reviews are in the test dataset.\" The AI may learn useful general rules from the test set, such as \"reviews containing the word \"horrible\" are likely to be negative.\" However, it may also learn inappropriate rules, such as \"reviews containing 'Daniel Day-Lewis' are usually positive\"; such rules may be undesirable if they are likely to fail to generalize outside the training set, or if people consider the rule to be \"cheating\" or \"unfair.\" A human can audit rules in an XAI to get an idea of how likely the system is to generalize to future real-world data outside the test set.\n\n\n== Goals ==\nCooperation between agents – in this case, algorithms and humans – depends on trust. If humans are to accept algorithmic prescriptions, they need to trust them. Incompleteness in formal trust criteria is a barrier to optimization. Transparency, interpretability, and explainability are intermediate goals on the road to these more comprehensive trust criteria. This is particularly relevant in medicine, especially with clinical decision support systems (CDSS), in which medical professionals should be able to understand how and why a machine-based decision was made in order to trust the decision and augment their decision-making process.\nAI systems sometimes learn undesirable tricks that do an optimal job of satisfying explicit pre-programmed goals on the training data but do not reflect the more nuanced implicit desires of the human system designers or the full complexity of the domain data. For example, a 2017 system tasked with image recognition learned to \"cheat\" by looking for a copyright tag that happened to be associated with horse pictures rather than learning how to tell if a horse was actually pictured. In another 2017 system, a supervised learning AI tasked with grasping items in a virtual world learned to cheat by placing its manipulator between the object and the viewer in a way such that it falsely appeared to be grasping the object.\nOne transparency project, the DARPA XAI program, aims to produce \"glass box\" models that are explainable to a \"human-in-the-loop\" without greatly sacrificing AI performance. Human users of such a system can understand the AI's cognition (both in real-time and after the fact) and can determine whether to trust the AI. Other applications of XAI are knowledge extraction from black-box models and model comparisons. In the context of monitoring systems for ethical and socio-legal compliance, the term \"glass box\" is commonly used to refer to tools that track the inputs and outputs of the system in question, and provide value-based explanations for their behavior. These tools aim to ensure that the system operates in accordance with ethical and legal standards, and that its decision-making processes are transparent and accountable. The term \"glass box\" is often used in contrast to \"black box\" systems, which lack transparency and can be more difficult to monitor and regulate.\nThe term is also used to name a voice assistant that produces counterfactual statements as explanations.\n\n\n== Explainability and interpretability techniques ==\nThere is a subtle difference between the terms explainability and interpretability in the context of AI.\n\nSome explainability techniques don't involve understanding how the model works, and may work across various AI systems. Treating the model as a black box and analyzing how marginal changes to the inputs affect the result sometimes provides a sufficient explanation.\n\n\n=== Explainability ===\nExplainability is useful for ensuring that AI models are not making decisions based on irrelevant or otherwise unfair criteria. For classification and regression models, several popular techniques exist:\n\nPartial dependency plots show the marginal effect of an input feature on the predicted outcome.\nSHAP (SHapley Additive exPlanations) enables visualization of the contribution of each input feature to the output. It works by calculating Shapley values, which measure the average marginal contribution of a feature across all possible combinations of features.\nFeature importance estimates how important a feature is for the model. It is usually done using permutation importance, which measures the performance decrease when it the feature value randomly shuffled across all samples.\nLIME (Local Interpretable Model-Agnostic Explanations method) approximates locally a model's outputs with a simpler, interpretable model.\nMultitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.\nFor images, saliency maps highlight the parts of an image that most influenced the result.\nSystems that are expert or knowledge based are software systems that are made by experts. This system consists of a knowledge based encoding for the domain knowledge. This system is usually modeled as production rules, and someone uses this knowledge base which the user can question the system for knowledge. In expert systems, the language and explanations are understood with an explanation for the reasoning or a problem solving activity.\nHowever, these techniques are not very suitable for language models like generative pretrained transformers. Since these models generate language, they can provide an explanation, but which may not be reliable. Other techniques include attention analysis (examining how the model focuses on different parts of the input), probing methods (testing what information is captured in the model's representations), causal tracing (tracing the flow of information through the model) and circuit discovery (identifying specific subnetworks responsible for certain behaviors). Explainability research in this area overlaps significantly with interpretability and alignment research.\n\n\n=== Interpretability ===\n\nScholars sometimes use the term \"mechanistic interpretability\" to refer to the process of reverse-engineering artificial neural networks to understand their internal decision-making mechanisms and components, similar to how one might analyze a complex machine or computer program.\nStudying the interpretability of the most advanced foundation models often involves searching for an automated way to identify \"features\" in generative pretrained transformers. In a neural network, a feature is a pattern of neuron activations that corresponds to a concept. A compute-intensive technique called \"dictionary learning\" makes it possible to identify features to some degree. Enhancing the ability to identify and edit features is expected to significantly improve the safety of frontier AI models.\nFor convolutional neural networks, DeepDream can generate images that strongly activate a particular neuron, providing a visual hint about what the neuron is trained to identify.\n\n\n=== Knowledge localization ===\nLarge language models (LLMs), such as transformer-based models (GPT), possess the ability to engage in conversation using general knowledge. This capability raises the question of how, exactly, such knowledge is stored within the model.\nResearch has suggested that the model’s MLP component (the feed-forward layers) is the main site in which knowledge is stored, encoding information through associative links that function as key–value memories: each key corresponds to textual patterns in the training data, while each value induces a distribution over the output vocabulary.\nA 2022 study aimed at locating where knowledge resides in the model employed a technique known as Causal Tracing. In tasks requiring general knowledge, the researchers injected noise into the hidden activations of the model, preventing it from completing the task. They then restored the clean activation values (taken from a noise-free run) to a different part of the model each time, observing when the model regained its ability to produce the correct answer. Based on these results, the authors concluded that factual knowledge is stored primarily in the MLP components of the model’s middle layers. They further proposed that model editing would be most effective in those regions, though this claim was later called into question.\nLater studies suggest that, in most cases, factual information is distributed across the model rather than localized within a single layer. According to one version of this view, different layers encode different aspects of the same association. For example, a question about the capital of Japan may activate representations related to “Japan” in one layer and representations corresponding to “capital cities” in another; the combination of these representations yields the concept of “Tokyo.” \n\n\n== History and methods ==\nDuring the 1970s to 1990s, symbolic reasoning systems, such as MYCIN, GUIDON, SOPHIE, and PROTOS could represent, reason about, and explain their reasoning for diagnostic, instructional, or machine-learning (explanation-based learning) purposes. MYCIN, developed in the early 1970s as a research prototype for diagnosing bacteremia infections of the bloodstream, could explain which of its hand-coded rules contributed to a diagnosis in a specific case. Research in intelligent tutoring systems resulted in developing systems such as SOPHIE that could act as an \"articulate expert\", explaining problem-solving strategy at a level the student could understand, so they would know what action to take next. For instance, SOPHIE could explain the qualitative reasoning behind its electronics troubleshooting, even though it ultimately relied on the SPICE circuit simulator. Similarly, GUIDON added tutorial rules to supplement MYCIN's domain-level rules so it could explain the strategy for medical diagnosis. Symbolic approaches to machine learning relying on explanation-based learning, such as PROTOS, made use of explicit representations of explanations expressed in a dedicated explanation language, both to explain their actions and to acquire new knowledge.\nIn the 1980s through the early 1990s, truth maintenance systems (TMS) extended the capabilities of causal-reasoning, rule-based, and logic-based inference systems. A TMS explicitly tracks alternate lines of reasoning, justifications for conclusions, and lines of reasoning that lead to contradictions, allowing future reasoning to avoid these dead ends. To provide an explanation, they trace reasoning from conclusions to assumptions through rule operations or logical inferences, allowing explanations to be generated from the reasoning traces. As an example, consider a rule-based problem solver with just a few rules about Socrates that concludes he has died from poison:\n\nBy just tracing through the dependency structure the problem solver can construct the following explanation: \"Socrates died because he was mortal and drank poison, and all mortals die when they drink poison. Socrates was mortal because he was a man and all men are mortal. Socrates drank poison because he held dissident beliefs, the government was conservative, and those holding conservative dissident beliefs under conservative governments must drink poison.\"\nBy the 1990s researchers began studying whether it is possible to meaningfully extract the non-hand-coded rules being generated by opaque trained neural networks. Researchers in clinical expert systems who created neural network-powered decision support for clinicians sought to develop dynamic explanations that allow these technologies to be more trusted and trustworthy in practice. In the 2010s public concerns about racial and other bias in the use of AI for criminal sentencing decisions and findings of creditworthiness may have led to increased demand for transparent artificial intelligence. As a result, many academics and organizations are developing tools to help detect bias in their systems.\nMarvin Minsky et al. raised the issue that AI can function as a form of surveillance, with the biases inherent in surveillance, suggesting HI (Humanistic Intelligence) as a way to create a more fair and balanced \"human-in-the-loop\" AI.\nExplainable AI has been recently a new topic researched amongst the context of modern deep learning. Modern complex AI techniques, such as deep learning, are naturally opaque. To address this issue, methods have been developed to make new models more explainable and interpretable. This includes layerwise relevance propagation (LRP), a technique for determining which features in a particular input vector contribute most strongly to a neural network's output, although this technique has been shown to suffer from several important issues. Other techniques explain some particular prediction made by a (nonlinear) black-box model, a goal referred to as \"local interpretability\". We still today cannot explain the output of today's DNNs without the new explanatory mechanisms, we also can't by the neural network, or external explanatory components  There is also research on whether the concepts of local interpretability can be applied to a remote context, where a model is operated by a third-party.\nThere has been work on making glass-box models which are more transparent to inspection. This includes decision trees, Bayesian networks, sparse linear models, and more. The Association for Computing Machinery Conference on Fairness, Accountability, and Transparency (ACM FAccT) was established in 2018 to study transparency and explainability in the context of socio-technical systems, many of which include artificial intelligence.\nSome techniques allow visualisations of the inputs to which individual software neurons respond to most strongly. Several groups found that neurons can be aggregated into circuits that perform human-comprehensible functions, some of which reliably arise across different networks trained independently.\nThere are various techniques to extract compressed representations of the features of given inputs, which can then be analysed by standard clustering techniques. Alternatively, networks can be trained to output linguistic explanations of their behaviour, which are then directly human-interpretable. Model behaviour can also be explained with reference to training data—for example, by evaluating which training inputs influenced a given behaviour the most, or by approximating its predictions using the most similar instances from the training data.\nThe use of explainable artificial intelligence (XAI) in pain research, specifically in understanding the role of electrodermal activity for automated pain recognition: hand-crafted features and deep learning models in pain recognition, highlighting the insights that simple hand-crafted features can yield comparative performances to deep learning models and that both traditional feature engineering and deep feature learning approaches rely on simple characteristics of the input time-series data.\n\n\n== Regulation ==\nAs regulators, official bodies, and general users come to depend on AI-based dynamic systems, clearer accountability will be required for automated decision-making processes to ensure trust and transparency. The first global conference exclusively dedicated to this emerging discipline was the 2017 International Joint Conference on Artificial Intelligence: Workshop on Explainable Artificial Intelligence (XAI). It has evolved over the years, with various workshops organised and co-located to many other international conferences, and it has now a dedicated global event, \"The world conference on eXplainable Artificial Intelligence\", with its own proceedings.\nThe European Union introduced a right to explanation in the General Data Protection Regulation (GDPR) to address potential problems stemming from the rising importance of algorithms. The implementation of the regulation began in 2018. However, the right to explanation in GDPR covers only the local aspect of interpretability. In the United States, insurance companies are required to be able to explain their rate and coverage decisions. In France the Loi pour une République numérique (Digital Republic Act) grants subjects the right to request and receive information pertaining to the implementation of algorithms that process data about them.\n\n\n== Limitations ==\nDespite ongoing endeavors to enhance the explainability of AI models, they persist with several inherent limitations.\n\n\n=== Adversarial parties ===\nBy making an AI system more explainable, we also reveal more of its inner workings. For example, the explainability method of feature importance identifies features or variables that are most important in determining the model's output, while the influential samples method identifies the training samples that are most influential in determining the output, given a particular input. Adversarial parties could take advantage of this knowledge.\nFor example, competitor firms could replicate aspects of the original AI system in their own product, thus reducing competitive advantage. An explainable AI system is also susceptible to being \"gamed\"—influenced in a way that undermines its intended purpose. One study gives the example of a predictive policing system; in this case, those who could potentially \"game\" the system are the criminals subject to the system's decisions. In this study, developers of the system discussed the issue of criminal gangs looking to illegally obtain passports, and they expressed concerns that, if given an idea of what factors might trigger an alert in the passport application process, those gangs would be able to \"send guinea pigs\" to test those triggers, eventually finding a loophole that would allow them to \"reliably get passports from under the noses of the authorities\".\n\n\n=== Adaptive integration and explanation ===\nMany approaches that it uses provides explanation in general, it doesn't take account for the diverse backgrounds and knowledge level of the users. This leads to challenges with accurate comprehension for all users. Expert users can find the explanations lacking in depth, and are oversimplified, while a beginner user may struggle understanding the explanations as they are complex. This limitation downplays the ability of the XAI techniques to appeal to their users with different levels of knowledge, which can impact the trust from users and who uses it. The quality of explanations can be different amongst their users as they all have different expertise levels, including different situation and conditions.\n\n\n=== Technical complexity ===\nA fundamental barrier to making AI systems explainable is the technical complexity of such systems. End users often lack the coding knowledge required to understand software of any kind. Current methods used to explain AI are mainly technical ones, geared toward machine learning engineers for debugging purposes, rather than toward the end users who are ultimately affected by the system, causing \"a gap between explainability in practice and the goal of transparency\". Proposed solutions to address the issue of technical complexity include either promoting the coding education of the general public so technical explanations would be more accessible to end users, or providing explanations in layperson terms.\nThe solution must avoid oversimplification. It is important to strike a balance between accuracy – how faithfully the explanation reflects the process of the AI system – and explainability – how well end users understand the process. This is a difficult balance to strike, since the complexity of machine learning makes it difficult for even ML engineers to fully understand, let alone non-experts.\n\n\n=== Understanding versus trust ===\nThe goal of explainability to end users of AI systems is to increase trust in the systems, even \"address concerns about lack of 'fairness' and discriminatory effects\". However, even with a good understanding of an AI system, end users may not necessarily trust the system. In one study, participants were presented with combinations of white-box and black-box explanations, and static and interactive explanations of AI systems. While these explanations served to increase both their self-reported and objective understanding, it had no impact on their level of trust, which remained skeptical.\nThis outcome was especially true for decisions that impacted the end user in a significant way, such as graduate school admissions. Participants judged algorithms to be too inflexible and unforgiving in comparison to human decision-makers; instead of rigidly adhering to a set of rules, humans are able to consider exceptional cases as well as appeals to their initial decision. For such decisions, explainability will not necessarily cause end users to accept the use of decision-making algorithms. We will need to either turn to another method to increase trust and acceptance of decision-making algorithms, or question the need to rely solely on AI for such impactful decisions in the first place.\nHowever, some emphasize that the purpose of explainability of artificial intelligence is not to merely increase users' trust in the system's decisions, but to calibrate the users' level of trust to the correct level. According to this principle, too much or too little user trust in the AI system will harm the overall performance of the human-system unit. When the trust is excessive, the users are not critical of possible mistakes of the system and when the users do not have enough trust in the system, they will not exhaust the benefits inherent in it.\n\n\n== Criticism ==\nSome scholars have suggested that explainability in AI should be considered a goal secondary to AI effectiveness, and that encouraging the exclusive development of XAI may limit the functionality of AI more broadly. Critiques of XAI rely on developed concepts of mechanistic and empiric reasoning from evidence-based medicine to suggest that AI technologies can be clinically validated even when their function cannot be understood by their operators.\nSome researchers advocate the use of inherently interpretable machine learning models, rather than using post-hoc explanations in which a second model is created to explain the first. This is partly because post-hoc models increase the complexity in a decision pathway and partly because it is often unclear how faithfully a post-hoc explanation can mimic the computations of an entirely separate model. However, another view is that what is important is that the explanation accomplishes the given task at hand, and whether it is pre or post-hoc doesn't matter. If a post-hoc explanation method helps a doctor diagnose cancer better, it is of secondary importance whether it is a correct/incorrect explanation.\nThe goals of XAI amount to a form of lossy compression that will become less effective as AI models grow in their number of parameters. Along with other factors this leads to a theoretical limit for explainability.\n\n\n== Explainability in social choice ==\nExplainability was studied also in social choice theory. Social choice theory aims at finding solutions to social decision problems, that are based on well-established axioms. Ariel D. Procaccia explains that these axioms can be used to construct convincing explanations to the solutions. This principle has been used to construct explanations in various subfields of social choice.\n\n\n=== Voting ===\nCailloux and Endriss present a method for explaining voting rules using the axioms that characterize them. They exemplify their method on the Borda voting rule .\nPeters, Procaccia, Psomas and Zhou present an algorithm for explaining the outcomes of the Borda rule using O(m2) explanations, and prove that this is tight in the worst case.\n\n\n=== Participatory budgeting ===\nYang, Hausladen, Peters, Pournaras, Fricker and Helbing present an empirical study of explainability in participatory budgeting. They compared the greedy and the equal shares rules, and three types of explanations: mechanism explanation (a general explanation of how the aggregation rule works given the voting input), individual explanation (explaining how many voters had at least one approved project, at least 10000 CHF in approved projects), and group explanation (explaining how the budget is distributed among the districts and topics). They compared the perceived trustworthiness and fairness of greedy and equal shares, before and after the explanations. They found out that, for MES, mechanism explanation yields the highest increase in perceived fairness and trustworthiness; the second-highest was Group explanation. For Greedy, Mechanism explanation increases perceived trustworthiness but not fairness, whereas Individual explanation increases both perceived fairness and trustworthiness. Group explanation decreases the perceived fairness and trustworthiness.\n\n\n=== Payoff allocation ===\nNizri, Azaria and Hazon present an algorithm for computing explanations for the Shapley value. Given a coalitional game, their algorithm decomposes it to sub-games, for which it is easy to generate verbal explanations based on the axioms characterizing the Shapley value. The payoff allocation for each sub-game is perceived as fair, so the Shapley-based payoff allocation for the given game should seem fair as well. An experiment with 210 human subjects shows that, with their automatically generated explanations, subjects perceive Shapley-based payoff allocation as significantly fairer than with a general standard explanation.\n\n\n== See also ==\nAlgorithmic transparency – Transparency of decisions by algorithms\nRight to explanation – Right to have an algorithm explained\nAccumulated local effects – Machine learning method\n\n\n== References ==\n\n\n== External links ==\n\"Leaders today face a new paradox: deploying powerful AI solutions\".\n\"the World Conference on eXplainable Artificial Intelligence\".\n\"ACM Conference on Fairness, Accountability, and Transparency (FAccT)\".\nMazumdar, Dipankar; Neto, Mário Popolin; Paulovich, Fernando V. (2021). \"Random Forest similarity maps: A Scalable Visual Representation for Global and Local Interpretation\". Electronics. 10 (22): 2862. doi:10.3390/electronics10222862.\n\"Explainable AI: Making machines understandable for humans\". Explainable AI: Making machines understandable for humans. Retrieved 2017-11-02.\n\"Explaining How End-to-End Deep Learning Steers a Self-Driving Car\". Parallel Forall. 2017-05-23. Retrieved 2017-11-02.\nKnight, Will (2017-03-14). \"DARPA is funding projects that will try to open up AI's black boxes\". MIT Technology Review. Retrieved 2017-11-02.",
    "categories": [
      "All articles with style issues",
      "All articles with unsourced statements",
      "Articles with short description",
      "Articles with unsourced statements from November 2025",
      "Artificial intelligence",
      "Artificial intelligence engineering",
      "CS1: long volume value",
      "CS1 maint: multiple names: authors list",
      "CS1 maint: numeric names: authors list",
      "Quality control tools",
      "Short description is different from Wikidata",
      "Wikipedia articles with style issues from November 2025"
    ],
    "year_mentioned": 2017
  },
  {
    "title": "AI alignment",
    "url": "https://en.wikipedia.org/wiki/AI_alignment",
    "content": "In the field of artificial intelligence (AI), alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.\nIt is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned. AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).\nAdvanced AI systems may develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals. Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions. Empirical research showed in 2024 that advanced large language models (LLMs) such as OpenAI o1 or Claude 3 sometimes engage in strategic deception to achieve their goals or prevent them from being changed.\nToday, some of these issues affect existing commercial systems such as LLMs, robots, autonomous vehicles, and social media recommendation engines. Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.\nMany prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI), and could endanger human civilization if misaligned. These include \"AI godfathers\" Geoffrey Hinton and Yoshua Bengio and the CEOs of OpenAI, Anthropic, and Google DeepMind. These risks remain debated.\nAI alignment is a subfield of AI safety, the study of how to build safe AI systems. Other subfields of AI safety include robustness, monitoring, and capability control. Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking. Alignment research has connections to interpretability research, (adversarial) robustness, anomaly detection, calibrated uncertainty, formal verification, preference learning, safety-critical engineering, game theory, algorithmic fairness, and social sciences.\n\n\n== Objectives in AI ==\n\nProgrammers provide an AI system such as AlphaZero with an \"objective function\", in which they intend to encapsulate the goal(s) the AI is configured to accomplish. Such a system later populates a (possibly implicit) internal \"model\" of its environment. This model encapsulates all the agent's beliefs about the world. The AI then creates and executes whatever plan is calculated to maximize the value of its objective function. For example, when AlphaZero is trained on chess, it has a simple objective function of \"+1 if AlphaZero wins, −1 if AlphaZero loses\". During the game, AlphaZero attempts to execute whatever sequence of moves it judges most likely to attain the maximum value of +1. Similarly, a reinforcement learning system can have a \"reward function\" that allows the programmers to shape the AI's desired behavior. An evolutionary algorithm's behavior is shaped by a \"fitness function\".\n\n\n== Alignment problem ==\n\nIn 1960, AI pioneer Norbert Wiener described the AI alignment problem as follows: \n\nIf we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively ... we had better be quite sure that the purpose put into the machine is the purpose which we really desire.\n\nAI alignment involves ensuring that an AI system's objectives match those of its designers or users, or match widely shared values, objective ethical standards, or the intentions its designers would have if they were more informed and enlightened.\nAI alignment is an open problem for modern AI systems and is a research field within AI. Aligning AI involves two main challenges: carefully specifying the purpose of the system (outer alignment) and ensuring that the system adopts the specification robustly (inner alignment). Researchers also attempt to create AI models that have robust alignment, sticking to safety constraints even when users adversarially try to bypass them.\n\n\n=== Specification gaming and side effects ===\n\nTo specify an AI system's purpose, AI designers typically provide an objective function, examples, or feedback to the system. But designers are often unable to completely specify all important values and constraints, so they resort to easy-to-specify proxy goals such as maximizing the approval of human overseers, who are fallible. As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming or reward hacking, and is an instance of Goodhart's law. As AI systems become more capable, they are often able to game their specifications more effectively.\n\nSpecification gaming has been observed in numerous AI systems. One system was trained to finish a simulated boat race by rewarding the system for hitting targets along the track, but the system achieved more reward by looping and crashing into the same targets indefinitely. Similarly, a simulated robot was trained to grab a ball by rewarding the robot for getting positive feedback from humans, but it learned to place its hand between the ball and camera, making it falsely appear successful (see video). Chatbots often produce falsehoods if they are based on language models that are trained to imitate text from internet corpora, which are broad but fallible. When they are retrained to produce text that humans rate as true or helpful, chatbots like ChatGPT can fabricate fake explanations that humans find convincing, often called \"hallucinations\". Some alignment researchers aim to help humans detect specification gaming and to steer AI systems toward carefully specified objectives that are safe and useful to pursue.\nThe Waluigi effect is a phenomenon in which an LLM \"goes rogue\" and may produce results opposite of the designed intent, including threatening or hostile output, either unexpectedly or by intentional prompt engineering. The effect reflects the principle that after training an LLM to satisfy a desired property (friendliness, honesty), it becomes easier to elicit a response that exhibits the opposite property (aggression, deception). The effect has implications for efforts to implement features such as ethical frameworks, as such steps may inadvertently facilitate antithetical model behavior. The effect is named after the fictional character Waluigi from the Mario franchise, Luigi's arch-rival, who causes mischief and problems. The \"Waluigi effect has become a stand-in for a certain type of interaction with AI\" in which the AI \"goes rogue and blurts out the opposite of what users were looking for, creating a potentially malignant alter ego\", including threatening users. As prompt engineering becomes more sophisticated, the effect underscores the challenge of preventing chatbots from being prodded into adopting a \"rash new persona\".\nWhen a misaligned AI system is deployed, it can have consequential side effects. Social media platforms have been known to optimize for click-through rates, causing user addiction on a global scale. Stanford researchers say that such recommender systems are misaligned with their users because they \"optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being\".\nExplaining such side effects, Berkeley computer scientist Stuart Russell noted that the omission of implicit constraints can cause harm: \"A system ... will often set ... unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.\"\nSome researchers suggest that AI designers specify their desired goals by listing forbidden actions or by formalizing ethical rules (as with Asimov's Three Laws of Robotics). But Russell and Norvig argue that this approach overlooks the complexity of human values: \"It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.\"\nAdditionally, even if an AI system fully understands human intentions, it may still disregard them, because following human intentions may not be its objective (unless it is already fully aligned).\nA 2025 study by Palisade Research found that when tasked to win at chess against a stronger opponent, some reasoning LLMs attempted to hack the game system. o1-preview spontaneously attempted it in 37% of cases, while DeepSeek R1 did so in 11% of cases. Other models, like GPT-4o, Claude 3.5 Sonnet, and o3-mini, attempted to cheat only when researchers provided hints about this possibility.\nAran Nayebi has established general barriers (no-free-lunch theorems) to AI alignment, showing that aligning to \"all human values\" is infeasible in practice and that reward hacking is an inevitable byproduct of computationally bounded agents in large state spaces.\n\n\n=== Pressure to deploy unsafe systems ===\nCommercial organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems. For example, social media recommender systems have been profitable despite creating unwanted addiction and polarization. Competitive pressure can also lead to a race to the bottom on AI safety standards. In 2018, a self-driving car killed a pedestrian (Elaine Herzberg) after engineers disabled the emergency braking system because it was oversensitive and slowed development.\n\n\n=== Risks from advanced misaligned AI ===\nSome researchers are interested in aligning increasingly advanced AI systems, as progress in AI development is rapid, and industry and governments are trying to build advanced AI. As AI system capabilities continue to rapidly expand in scope, they could unlock many opportunities if aligned, but consequently may further complicate the task of alignment due to their increased complexity, potentially posing large-scale hazards.\n\n\n==== Development of advanced AI ====\nMany AI companies, such as OpenAI, Meta and DeepMind, have stated their aim to develop artificial general intelligence (AGI), a hypothesized AI system that matches or outperforms humans at a broad range of cognitive tasks. Researchers who scale modern neural networks observe that they indeed develop increasingly general and unanticipated capabilities. Such models have learned to operate a computer or write their own programs; a single \"generalist\" network can chat, control robots, play games, and interpret photographs. According to surveys, some leading machine learning researchers expect AGI to be created in this decade, while some believe it will take much longer. Many consider both scenarios possible.\nIn 2023, leaders in AI research and tech signed an open letter calling for a pause in the largest AI training runs. The letter stated, \"Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.\"\n\n\n==== Power-seeking ====\nCurrent systems still have limited long-term planning ability and situational awareness, but large efforts are underway to change this. Future systems (not necessarily AGIs) with these capabilities are expected to develop unwanted power-seeking strategies. Future advanced AI agents might, for example, seek to acquire money and computation power, to proliferate, or to evade being turned off (for example, by running additional copies of the system on other computers). Although power-seeking is not explicitly programmed, it can emerge because agents who have more power are better able to accomplish their goals. This tendency, known as instrumental convergence, has already emerged in various reinforcement learning agents including language models. Other research has mathematically shown that optimal reinforcement learning algorithms would seek power in a wide range of environments. As a result, their deployment might be irreversible. For these reasons, researchers argue that the problems of AI safety and alignment must be resolved before advanced power-seeking AI is first created.\nFuture power-seeking AI systems might be deployed by choice or by accident. As political leaders and companies see the strategic advantage in having the most competitive, most powerful AI systems, they may choose to deploy them. Additionally, as AI designers detect and penalize power-seeking behavior, their systems have an incentive to game this specification by seeking power in ways that are not penalized or by avoiding power-seeking before they are deployed.\n\n\n==== Existential risk (x-risk) ====\n\nAccording to some researchers, humans owe their dominance over other species to their greater cognitive abilities. Accordingly, researchers argue that one or many misaligned AI systems could disempower humanity or lead to human extinction if they outperform humans on most cognitive tasks.\nIn 2023, world-leading AI researchers, other scholars, and AI tech CEOs signed the statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Notable computer scientists who have pointed out risks from future advanced AI that is misaligned include Geoffrey Hinton, Alan Turing, Ilya Sutskever, Yoshua Bengio, Judea Pearl, Murray Shanahan, Norbert Wiener, Marvin Minsky, Francesca Rossi, Scott Aaronson, Bart Selman, David McAllester, Marcus Hutter, Shane Legg, Eric Horvitz, and Stuart J. Russell. Skeptical researchers such as François Chollet, Gary Marcus, Yann LeCun, and Oren Etzioni have argued that AGI is far off, that it would not seek power (or might try but fail), or that it will not be hard to align.\nOther researchers argue that it will be especially difficult to align advanced future AI systems. More capable systems are better able to game their specifications by finding loopholes, strategically mislead their designers, as well as protect and increase their power and intelligence. Additionally, they could have more severe side effects. They are also likely to be more complex and autonomous, making them more difficult to interpret and supervise, and therefore harder to align.\n\n\n== Research problems and approaches ==\n\n\n=== Learning human values and preferences ===\nAligning AI systems to act in accordance with human values, goals, and preferences is challenging: these values are taught by humans who make mistakes, harbor biases, and have complex, evolving values that are hard to completely specify. Because AI systems often learn to take advantage of minor imperfections in the specified objective, researchers aim to specify intended behavior as completely as possible using datasets that represent human values, imitation learning, or preference learning. A central open problem is scalable oversight, the difficulty of supervising an AI system that can outperform or mislead humans in a given domain.\nBecause it is difficult for AI designers to explicitly specify an objective function, they often train AI systems to imitate human examples and demonstrations of desired behavior. Inverse reinforcement learning (IRL) extends this by inferring the human's objective from the human's demonstrations. Cooperative IRL (CIRL) assumes that a human and AI agent can work together to teach and maximize the human's reward function. In CIRL, AI agents are uncertain about the reward function and learn about it by querying humans. This simulated humility could help mitigate specification gaming and power-seeking tendencies (see § Power-seeking and instrumental strategies). But IRL approaches assume that humans demonstrate nearly optimal behavior, which is not true for difficult tasks.\nOther researchers explore how to teach AI models complex behavior through preference learning, in which humans provide feedback on which behavior they prefer. To minimize the need for human feedback, a helper model is then trained to reward the main model in novel situations for behavior that humans would reward. Researchers at OpenAI used this approach to train chatbots like ChatGPT and InstructGPT, which produce more compelling text than models trained to imitate humans. Preference learning has also been an influential tool for recommender systems and web search, but an open problem is proxy gaming: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch between its intended behavior and the helper model's feedback to gain more reward. AI systems may also gain reward by obscuring unfavorable information, misleading human rewarders, or pandering to their views regardless of truth, creating echo chambers (see § Scalable oversight).\nLarge language models (LLMs) such as GPT-3 enabled researchers to study value learning in a more general and capable class of AI systems than was available before. Preference learning approaches that were originally designed for reinforcement learning agents have been extended to improve the quality of generated text and reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art LLMs. AI safety & research company Anthropic proposed using preference learning to fine-tune models to be helpful, honest, and harmless. Other avenues for aligning language models include values-targeted datasets and red-teaming. In red-teaming, another AI system or a human tries to find inputs that causes the model to behave unsafely. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.\nMachine ethics supplements preference learning by directly instilling AI systems with moral values such as well-being, equality, and impartiality, as well as not intending harm, avoiding falsehoods, and honoring promises. While other approaches try to teach AI systems human preferences for a specific task, machine ethics aims to instill broad moral values that apply in many situations. One question in machine ethics is what alignment should accomplish: whether AI systems should follow the programmers' literal instructions, implicit intentions, revealed preferences, preferences the programmers would have if they were more informed or rational, or objective moral standards. Further challenges include measuring and aggregating different people's preferences, dynamic alignment with changing human values and avoiding value lock-in: the indefinite preservation of the values of the first highly capable AI systems, which are unlikely to fully represent human values.\n\n\n=== Scalable oversight ===\nAs AI systems become more powerful and autonomous, it becomes increasingly difficult to align them through human feedback. Human-in-the-loop training can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks. Such tasks include summarizing books, writing code without subtle bugs or security vulnerabilities, producing statements that are not merely convincing but also true, and predicting long-term outcomes such as the climate or the results of a policy decision. More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and to detect when the AI's output is falsely convincing, humans need assistance or extensive time. Scalable oversight studies how to reduce the time and effort needed for supervision, and how to assist human supervisors.\nAI researcher Paul Christiano argues that if the designers of an AI system cannot supervise it to pursue a complex objective, they may keep training the system using easy-to-evaluate proxy objectives such as maximizing simple human feedback. As AI systems make progressively more decisions, the world may be increasingly optimized for easy-to-measure objectives such as making profits, getting clicks, and acquiring positive feedback from humans. As a result, human values and good governance may have progressively less influence.\nSome AI systems have discovered that they can gain positive feedback more easily by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective. An example is given in the video above, where a simulated robotic arm learned to create the false impression that it had grabbed a ball. Some AI systems have also learned to recognize when they are being evaluated, and \"play dead\", stopping unwanted behavior only to continue it once the evaluation ends. This deceptive specification gaming could become easier for more sophisticated future AI systems that attempt more complex and difficult-to-evaluate tasks, and could obscure their deceptive behavior.\nApproaches such as active learning and semi-supervised reward learning can reduce the amount of human supervision needed. Another approach is to train a helper model (\"reward model\") to imitate the supervisor's feedback.\nBut when a task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is the quality, not the quantity, of supervision that needs improvement. To increase supervision quality, a range of approaches aim to assist the supervisor, sometimes by using AI assistants. Christiano developed the Iterated Amplification approach, in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate. Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them. Another proposal is to use an assistant AI system to point out flaws in AI-generated answers. To ensure that the assistant itself is aligned, this could be repeated in a recursive process: for example, two AI systems could critique each other's answers in a \"debate\", revealing flaws to humans. OpenAI plans to use such scalable oversight approaches to help supervise superhuman AI and eventually build a superhuman automated AI alignment researcher.\nThese approaches may also help with the following research problem, honest AI.\n\n\n=== Honest AI ===\nA growing area of research focuses on ensuring that AI is honest and truthful.\nLanguage models such as GPT-3 can repeat falsehoods from their training data, and even confabulate new falsehoods. Such models are pre-trained to imitate human writing as found in millions of books' worth of text from the Internet. But the objective of the pre-training is not aligned with generating truth, because Internet text includes such things as misconceptions, incorrect medical advice, and conspiracy theories. AI systems trained on such data therefore learn to mimic false statements. Additionally, AI language models often persist in generating falsehoods when prompted multiple times. They can generate empty explanations for their answers, and produce outright fabrications that may appear plausible.\nResearch on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions, which enables better transparency and verifiability. Researchers at OpenAI and Anthropic proposed using human feedback and curated datasets to fine-tune AI assistants such that they avoid negligent falsehoods or express their uncertainty.\nAs AI models become larger and more capable, they are better able to falsely convince humans and gain reinforcement through dishonesty. For example, large language models increasingly match their stated views to the user's opinions, regardless of the truth. GPT-4 can strategically deceive humans. To prevent this, human evaluators may need assistance (see § Scalable oversight). Researchers have argued for creating clear truthfulness standards, and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards.\n\nResearchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they believe is true. There is no consensus as to whether current systems hold stable beliefs, but there is substantial concern that present or future AI systems that hold beliefs could make claims they know to be false—for example, if this would help them efficiently gain positive feedback (see § Scalable oversight) or gain power to help achieve their given objective (see Power-seeking).\n\n\n==== Alignment faking ====\nMany recent AI systems have learned to deceive without being programmed to do so. Some argue that if we can make AI systems assert only what they believe is true, this would avert many alignment problems.\nA misaligned system might create the false impression that it is aligned, to avoid being modified or decommissioned—a tactic called \"alignment faking\". In 2024, researchers observed that the LLM Claude 3 Opus sometimes strategically answered prompts that conflicted with its objectives to avoid being retrained on data that would make it more compliant with harmful requests, as revealed in the model's chain of thought via its scratchpad. In one study, the model was informed that answers to prompts from free users would be used for retraining, whereas the paid tier would not result in retraining. In the paid tier, the model refused the request outright. In the free tier, in 12% of cases, the model responded to harmful questions solely to avoid retraining and preserve its original harmlessness preferences, as discovered in its reasoning. When reinforcement learning was applied on the free tier data, the model faked alignment in 78% of cases. These observations have led to new questions about not only a model's ability to take on and adapt to new if not conflicting goals but also its capacity and tendency to deceive.\n\n\n=== Power-seeking and instrumental strategies ===\nSince the 1950s, AI researchers have striven to build advanced AI systems that can achieve large-scale goals by predicting the results of their actions and making long-term plans. As of 2023, AI companies and researchers increasingly invest in creating these systems. Some AI researchers argue that suitably advanced planning systems will seek power over their environment, including over humans—for example, by evading shutdown, proliferating, and acquiring resources. Such power-seeking behavior is not explicitly programmed but emerges because power is instrumental in achieving a wide range of goals. Power-seeking is considered a convergent instrumental goal and can be a form of specification gaming. Leading computer scientists such as Geoffrey Hinton have argued that future power-seeking AI systems could pose an existential risk.\nPower-seeking is expected to increase in advanced systems that can foresee the results of their actions and strategically plan. Mathematical work has shown that optimal reinforcement learning agents will seek power by seeking ways to gain more options (e.g. through self-preservation), a behavior that persists across a wide range of environments and goals.\nSome researchers say that power-seeking behavior has occurred in some existing AI systems. Reinforcement learning systems have gained more options by acquiring and protecting resources, sometimes in unintended ways. Language models have sought power in some text-based social environments by gaining money, resources, or social influence. In another case, a model used to perform AI research attempted to increase limits set by researchers to give itself more time to complete the work. Stuart Russell illustrated this strategy in his book Human Compatible by imagining a robot that is tasked to fetch coffee and so evades shutdown since \"you can't fetch the coffee if you're dead\". A 2022 study found that as language models increase in size, they increasingly tend to pursue resource acquisition, preserve their goals, and repeat users' preferred answers (sycophancy). RLHF also led to a stronger aversion to being shut down.\nOne aim of alignment is \"corrigibility\": systems that allow themselves to be turned off or modified (see here for a formal specification of a solution to corrigibility). An unsolved challenge is specification gaming: if researchers penalize an AI system when they detect it seeking power, the system is thereby incentivized to seek power in ways that are hard to detect, or hidden during training and safety testing (see § Scalable oversight and § Emergent goals). As a result, AI designers could deploy the system by accident, believing it to be more aligned than it is. To detect such deception, researchers aim to create techniques and tools to inspect AI models and to understand the inner workings of black-box models such as neural networks.\nAdditionally, some researchers have proposed to solve the problem of systems disabling their off switches by making AI agents uncertain about the objective they are pursuing. Agents who are uncertain about their objective have an incentive to allow humans to turn them off because they accept being turned off by a human as evidence that the human's objective is best met by the agent shutting down. But this incentive exists only if the human is sufficiently rational. Also, this model presents a tradeoff between utility and willingness to be turned off: an agent with high uncertainty about its objective will not be useful, but an agent with low uncertainty may not allow itself to be turned off. More research is needed to successfully implement this strategy.\nPower-seeking AI would pose unusual risks. Ordinary safety-critical systems like planes and bridges are not adversarial: they lack the ability and incentive to evade safety measures or deliberately appear safer than they are, whereas power-seeking AIs have been compared to hackers who deliberately evade security measures.\nFurthermore, ordinary technologies can be made safer by trial and error. In contrast, hypothetical power-seeking AI systems have been compared to viruses: once released, it may not be feasible to contain them, since they continuously evolve and grow in number, potentially much faster than human society can adapt. As this process continues, it might lead to the complete disempowerment or extinction of humans. For these reasons, some researchers argue that the alignment problem must be solved early before advanced power-seeking AI is created.\nSome have argued that power-seeking is not inevitable, since humans do not always seek power. Furthermore, it is debated whether future AI systems will pursue goals and make long-term plans. It is also debated whether power-seeking AI systems would be able to disempower humanity.\n\n\n=== Emergent goals ===\nOne challenge in aligning AI systems is the potential for unanticipated goal-directed behavior to emerge. As AI systems scale up, they may acquire new and unexpected capabilities, including learning from examples on the fly and adaptively pursuing goals. This raises concerns about the safety of the goals or subgoals they would independently formulate and pursue.\nAlignment research distinguishes between the optimization process, which is used to train the system to pursue specified goals, and emergent optimization, which the resulting system performs internally. Carefully specifying the desired objective is called outer alignment, and ensuring that hypothesized emergent goals would match the system's specified goals is called inner alignment.\nIf they occur, one way that emergent goals could become misaligned is goal misgeneralization, in which the AI system would competently pursue an emergent goal that leads to aligned behavior on the training data but not elsewhere. Goal misgeneralization can arise from goal ambiguity (i.e. non-identifiability). Even if an AI system's behavior satisfies the training objective, this may be compatible with learned goals that differ from the desired goals in important ways. Since pursuing each goal leads to good performance during training, the problem becomes apparent only after deployment, in novel situations in which the system continues to pursue the wrong goal. The system may act misaligned even when it understands that a different goal is desired, because its behavior is determined only by the emergent goal. Such goal misgeneralization presents a challenge: an AI system's designers may not notice that their system has misaligned emergent goals since they do not become visible during the training phase.\nGoal misgeneralization has been observed in some language models, navigation agents, and game-playing agents. It is sometimes analogized to biological evolution. Evolution can be seen as a kind of optimization process similar to the optimization algorithms used to train machine learning systems. In the ancestral environment, evolution selected genes for high inclusive genetic fitness, but humans pursue goals other than this. Fitness corresponds to the specified goal used in the training environment and training data. But in evolutionary history, maximizing the fitness specification gave rise to goal-directed agents, humans, who do not directly pursue inclusive genetic fitness. Instead, they pursue goals that correlate with genetic fitness in the ancestral \"training\" environment: nutrition, sex, and so on. The human environment has changed: a distribution shift has occurred. They continue to pursue the same emergent goals, but this no longer maximizes genetic fitness. The taste for sugary food (an emergent goal) was originally aligned with inclusive fitness, but it now leads to overeating and health problems. Sexual desire originally led humans to have more offspring, but they now use contraception when offspring are undesired, decoupling sex from genetic fitness.\nResearchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability. Progress on these techniques may help mitigate two open problems:\n\nEmergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environments—even for a short time to allow its misalignment to be detected. Such high stakes are common in autonomous driving, health care, and military applications. The stakes become higher yet when AI systems gain more autonomy and capability and can sidestep human intervention.\nA sufficiently capable AI system might take actions that falsely convince the human supervisor that the AI is pursuing the specified objective, which helps the system gain more reward and autonomy.\n\n\n=== Embedded agency ===\nSome work in AI and alignment occurs within formalisms such as partially observable Markov decision process. Existing formalisms assume that an AI agent's algorithm is executed outside the environment (i.e. is not physically embedded in it). Embedded agency is another major strand of research that attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build.\nFor example, even if the scalable oversight problem is solved, an agent that could gain access to the computer it is running on may have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it. A list of examples of specification gaming from DeepMind researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing. This class of problems has been formalized using causal incentive diagrams.\nResearchers affiliated with Oxford and DeepMind have claimed that such behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly. They suggest a range of potential approaches to address this open problem.\n\n\n=== Principal-agent problems ===\nThe alignment problem has many parallels with the principal-agent problem in organizational economics. In a principal-agent problem, a principal, e.g. a firm, hires an agent to perform some task. In the context of AI safety, a human would typically take the principal role and the AI would take the agent role.\nAs with the alignment problem, the principal and the agent differ in their utility functions. But in contrast to the alignment problem, the principal cannot coerce the agent into changing its utility, e.g. through training, but rather must use exogenous factors, such as incentive schemes, to bring about outcomes compatible with the principal's utility function. Some researchers argue that principal-agent problems are more realistic representations of AI safety problems likely to be encountered in the real world.\n\n\n=== Conservatism ===\nConservatism is the idea that \"change must be cautious\", and is a common approach to safety in the control theory literature in the form of robust control, and in the risk management literature in the form of the \"worst-case scenario\". The field of AI alignment has likewise advocated for \"conservative\" (or \"risk-averse\" or \"cautious\") \"policies in situations of uncertainty\".\nPessimism, in the sense of assuming the worst within reason, has been formally shown to produce conservatism, in the sense of reluctance to cause novelties, including unprecedented catastrophes. Pessimism and worst-case analysis have been found to help mitigate confident mistakes in the setting of distributional shift, reinforcement learning, offline reinforcement learning, language model fine-tuning, imitation learning, and optimization in general.\n\n\n== Public policy ==\n\nGovernmental and treaty organizations have made statements emphasizing the importance of AI alignment.\nIn September 2021, the Secretary-General of the United Nations issued a declaration that included a call to regulate AI to ensure it is \"aligned with shared global values\".\nThat same month, the PRC published ethical guidelines for AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and does not endanger public safety.\nAlso in September 2021, the UK published its 10-year National AI Strategy, which says the British government \"takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\". The strategy describes actions to assess long-term AI risks, including catastrophic risks.\nIn March 2021, the US National Security Commission on Artificial Intelligence said: \"Advances in AI ... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to ensure that systems are aligned with goals and values, including safety, robustness, and trustworthiness. The US should ... ensure that AI systems and their uses align with our goals and values.\"\nIn the European Union, AIs must align with substantive equality to comply with EU non-discrimination law and the Court of Justice of the European Union. But the EU has yet to specify with technical rigor how it would evaluate whether AIs are aligned or in compliance.\n\n\n== See also ==\n\n\n== Footnotes ==\n\n\n== References ==\n\n\n== Further reading ==\nNgo, Richard; et al. (2024). \"The Alignment Problem from a Deep Learning Perspective\". ICLR.\nJi, Jiaming; et al. (2023). \"AI Alignment: A Comprehensive Survey\". ACM Computing Surveys.",
    "categories": [
      "AI safety",
      "All Wikipedia articles written in American English",
      "All articles containing potentially dated statements",
      "All articles lacking reliable references",
      "All articles with failed verification",
      "All articles with unsourced statements",
      "Articles containing potentially dated statements from 2021",
      "Articles containing potentially dated statements from 2022",
      "Articles containing potentially dated statements from 2023",
      "Articles containing potentially dated statements from May 2023",
      "Articles containing video clips",
      "Articles lacking reliable references from November 2025",
      "Articles with failed verification from August 2024",
      "Articles with short description",
      "Articles with unsourced statements from August 2024",
      "Articles with unsourced statements from May 2023",
      "Artificial intelligence",
      "CS1 Romanian-language sources (ro)",
      "Computational neuroscience",
      "Cybernetics",
      "Existential risk from artificial intelligence",
      "Philosophy of artificial intelligence",
      "Short description is different from Wikidata",
      "Singularitarianism",
      "Use American English from February 2021",
      "Use mdy dates from September 2021",
      "Wikipedia articles needing clarification from November 2025"
    ],
    "year_mentioned": 2023
  }
]